{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "f2267886810f2f15ec828da6309447cc90fcc527593918026a87c890b0378843"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    @staticmethod\n",
    "    def sigma(x):\n",
    "        return 1 / (np.exp(-x) + 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigma_deriv(x):\n",
    "        sig = NeuralNetwork.sigma(x)\n",
    "        deriv = sig * (1 - sig)\n",
    "        return deriv\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        # softmax function ---------\n",
    "        t = np.exp(x - np.max(x))\n",
    "        return t / np.sum(t, axis=1, keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_activation_function(name):\n",
    "        if name == 'sigma':\n",
    "            return NeuralNetwork.sigma\n",
    "        elif name == 'linear':\n",
    "            return lambda x: x\n",
    "        elif name == 'softmax':\n",
    "            return NeuralNetwork.softmax\n",
    "        elif name == 'tanh':\n",
    "            return np.tanh\n",
    "        elif name == 'relu':\n",
    "            return lambda x: np.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_activation_derivative(name):\n",
    "        if name == 'sigma':\n",
    "            return NeuralNetwork.sigma_deriv\n",
    "        elif name == 'linear':\n",
    "            return lambda x: np.ones_like(x)\n",
    "        elif name == 'softmax':\n",
    "            # jacobian of softmax - unused -------\n",
    "            def softmax_deriv(x):\n",
    "                value = NeuralNetwork.softmax(x)\n",
    "                SM = value.reshape((-1, 1))\n",
    "                jac = np.diagflat(value) - np.dot(SM, SM.T)\n",
    "                return jac\n",
    "\n",
    "            return softmax_deriv\n",
    "        elif name == 'tanh':\n",
    "            return lambda x: 1 - np.tanh(x)**2\n",
    "        elif name == 'relu':\n",
    "            return lambda x: x > 0\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss_function(name):\n",
    "        if name == 'mse':\n",
    "            return lambda x_pred, x: np.linalg.norm(x_pred - x)\n",
    "        elif name == 'mae':\n",
    "            return lambda x_pred, x: np.sum(np.abs(x_pred - x))\n",
    "        elif name == 'crossentropy':\n",
    "            return lambda x_pred, x: -np.sum(x*np.log(x_pred))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss_derivative(name):\n",
    "        # currently unused\n",
    "        if name == 'mse':\n",
    "            return lambda x_pred, x: (x_pred - x)\n",
    "        if name == 'mae':\n",
    "            return lambda x_pred, x: np.sign(x_pred - x)\n",
    "        elif name == 'crossentropy':\n",
    "            return lambda x_pred, x: x*(-1/x_pred)\n",
    "\n",
    "    class Layer:\n",
    "        def __init__(self, input_width, layer_width, activation_function):\n",
    "            # self.weights = np.random.uniform(0, 1, (input_width + 1, layer_width))\n",
    "            self.weights = np.random.normal(0, 1, (input_width + 1, layer_width))\n",
    "            self.activation_function = activation_function\n",
    "\n",
    "        def predict(self, x):\n",
    "            activation_function = NeuralNetwork.get_activation_function(self.activation_function)\n",
    "            return activation_function(np.dot(x, self.weights))\n",
    "\n",
    "        def feedforward_step(self, x):\n",
    "            activation_function = NeuralNetwork.get_activation_function(self.activation_function)\n",
    "            activation = np.dot(x, self.weights)\n",
    "            response = activation_function(activation)\n",
    "            return response, activation\n",
    "\n",
    "    def __init__(self, input_width, output_width, activation_function='sigma', loss_function='mse', bias_exists=True, seed=None, verbose=True):\n",
    "        ###\n",
    "        # create a new nn object. activation_function specifies activation used on hidden layers\n",
    "        # loss_function affects loss printed to console\n",
    "        ###\n",
    "        self.input_width = input_width\n",
    "        self.output_width = output_width\n",
    "        self.layers = []\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = NeuralNetwork.get_loss_function(loss_function)\n",
    "        self.loss_derivative = NeuralNetwork.get_loss_derivative(loss_function)\n",
    "        self.bias_exists = bias_exists\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def add_layer(self, layer_width):\n",
    "        ###\n",
    "        # add a hidden layer with specified number of neurons\n",
    "        ###\n",
    "        if len(self.layers) == 0:\n",
    "            self.layers.append(NeuralNetwork.Layer(self.input_width, layer_width, self.activation_function))\n",
    "        else:\n",
    "            self.layers.append(\n",
    "                NeuralNetwork.Layer(self.layers[-1].weights.shape[1], layer_width, self.activation_function))\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        ###\n",
    "        # predict responses on new data\n",
    "        ###\n",
    "        values = np.copy(x)\n",
    "        for layer in self.layers:\n",
    "            values = np.hstack((values, np.ones((values.shape[0], 1)) if self.bias_exists else np.zeros((values.shape[0], 1))))\n",
    "            values = layer.predict(values)\n",
    "        return values\n",
    "\n",
    "    def create_output_layer(self, activation_function='linear'):\n",
    "        ###\n",
    "        # create output layer with specified activation function. Use after adding all hidden layers and before training\n",
    "        ###\n",
    "        if len(self.layers) == 0:\n",
    "            self.layers.append(NeuralNetwork.Layer(self.input_width, self.output_width, activation_function))\n",
    "        else:\n",
    "            self.layers.append(\n",
    "                NeuralNetwork.Layer(self.layers[-1].weights.shape[1], self.output_width, activation_function))\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        response = np.copy(x)\n",
    "        response = response.reshape(1, -1)\n",
    "        response = np.hstack((response, np.ones((response.shape[0], 1))))\n",
    "        response_s = [response]\n",
    "        activation_s = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            response, activation = self.layers[i].feedforward_step(response_s[i])\n",
    "            activation_s.append(activation)\n",
    "            response = response.reshape(1, -1)\n",
    "            response = np.hstack((response, np.ones((response.shape[0], 1))))\n",
    "            response_s.append(response)\n",
    "        response, activation = self.layers[-1].feedforward_step(response_s[-1])\n",
    "        activation_s.append(activation)\n",
    "        response = response.reshape(1, -1)\n",
    "        response_s.append(response)\n",
    "        return (response_s, activation_s)\n",
    "\n",
    "    def backpropagation(self, x, y, r_s, a_s):\n",
    "        e_s = [None] * len(self.layers)\n",
    "\n",
    "        ###\n",
    "        # derivative of loss function in respect to final layer weights\n",
    "        # assuming loss is crossentropy and output layer is softmax\n",
    "        # formula is the same as in the case of linear outputs and mse\n",
    "        ###\n",
    "        e_s[-1] = a_s[-1] - y\n",
    "\n",
    "        for i in reversed(range(1, len(e_s))):\n",
    "            unbiased_weights = self.layers[i].weights[0:(self.layers[i].weights.shape[0] - 1), :]\n",
    "            e_s[i-1] = NeuralNetwork.get_activation_derivative(self.layers[i-1].activation_function)(a_s[i-1])*(e_s[i].dot(unbiased_weights.T))\n",
    "        gradient = [r_s[j].T.dot(e_s[j]) for j in range(0, len(self.layers))]\n",
    "        return gradient\n",
    "\n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr=0.01, method='basic', method_param=0.0):\n",
    "        ###\n",
    "        # train network. method can be 'basic', 'momentum' or 'rmsprop'.\n",
    "        # method_param specifies lambda in momentum or beta in rmsprop\n",
    "        ###\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        errors = []\n",
    "        eps = 1e-8\n",
    "        momentum = [np.zeros(layer.weights.shape) for layer in self.layers]\n",
    "        for e in range(epochs):\n",
    "            permutation = np.random.permutation(y.shape[0])\n",
    "            x = x[permutation, :]\n",
    "            y = y[permutation, :]\n",
    "            i = 0\n",
    "            while i < y.shape[0]:\n",
    "                deltas = [np.zeros(layer.weights.shape) for layer in self.layers]\n",
    "                x_batch = x[i:i + batch_size, :]\n",
    "                y_batch = y[i:i + batch_size, :]\n",
    "                i = i + batch_size\n",
    "                for j in range(0, y_batch.shape[0]):\n",
    "                    r_s, a_s = self.feedforward(x_batch[j, :])\n",
    "                    gradient = self.backpropagation(x_batch[j, :], y_batch[j, :], r_s, a_s)\n",
    "                    for k in range(0, len(deltas)):\n",
    "                        deltas[k] = deltas[k] - gradient[k]\n",
    "\n",
    "                if method == 'momentum':\n",
    "                    momentum = [delta + method_param * mom for mom, delta in zip(momentum, deltas)]\n",
    "                elif method == 'rmsprop':\n",
    "                    momentum = [method_param * mom + (1 - method_param)*np.square(delta) for mom, delta in zip(momentum, deltas)]\n",
    "\n",
    "                for j in range(0, len(deltas)):\n",
    "                    if method == 'momentum':\n",
    "                        self.layers[j].weights = self.layers[j].weights + lr*momentum[j]\n",
    "                    elif method == 'rmsprop':\n",
    "                        self.layers[j].weights = self.layers[j].weights + lr * (deltas[j] / (np.sqrt(momentum[j]) + eps))\n",
    "                    else:\n",
    "                        self.layers[j].weights = self.layers[j].weights + lr*deltas[j]\n",
    "\n",
    "                error = self.loss_function(self.predict(x_batch), y_batch)\n",
    "                if self.verbose:\n",
    "                    print(\"loss on batch = {}\".format(error))\n",
    "                errors.append(error)\n",
    "        return errors\n",
    "\n",
    "\n",
    "# TESTS ------------------------------------------------------------------------\n",
    "# set path to dataset folders\n",
    "classification = \"projekt1/classification/\"\n",
    "regression = \"projekt1/regression/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 784)\n(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_X = train_X.reshape(-1, 784)\n",
    "test_X = test_X.reshape(-1, 784)\n",
    "X_mean = np.mean(train_X)\n",
    "X_std = np.std(train_X)\n",
    "train_X_std = (train_X - X_mean) / X_std\n",
    "test_X_std = (test_X - X_mean) / X_std\n",
    "train_X_scaled = train_X / 255\n",
    "test_X_scaled = test_X / 255\n",
    "train_X_centered = (train_X - 127.5) / 127.5\n",
    "test_X_centered = (test_X - 127.5) / 127.5\n",
    "mnist_encoder = OneHotEncoder(sparse=False)\n",
    "train_y_onehot = mnist_encoder.fit_transform(train_y.reshape(-1,1))\n",
    "test_y_onehot = mnist_encoder.transform(test_y.reshape(-1,1))\n",
    "print(train_X_std.shape)\n",
    "print(train_y_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "atch = 82.94672235393061\n",
      "loss on batch = 80.61156839035098\n",
      "loss on batch = 82.0136445924746\n",
      "loss on batch = 84.11083308240171\n",
      "loss on batch = 78.52638555693302\n",
      "loss on batch = 85.8700172513968\n",
      "loss on batch = 82.66693950619054\n",
      "loss on batch = 79.51682553587332\n",
      "loss on batch = 80.52111690601319\n",
      "loss on batch = 84.7038035075073\n",
      "loss on batch = 79.83482862846145\n",
      "loss on batch = 84.84730633634523\n",
      "loss on batch = 84.92410856661769\n",
      "loss on batch = 81.5982121772735\n",
      "loss on batch = 84.2766112865328\n",
      "loss on batch = 84.6043872768486\n",
      "loss on batch = 83.60725875096698\n",
      "loss on batch = 82.26106627187615\n",
      "loss on batch = 78.77493483928869\n",
      "loss on batch = 82.55426584972369\n",
      "loss on batch = 81.97973197296005\n",
      "loss on batch = 86.7323973815145\n",
      "loss on batch = 85.20265892508439\n",
      "loss on batch = 85.53229417612758\n",
      "loss on batch = 87.21514827176651\n",
      "loss on batch = 83.42346241308542\n",
      "loss on batch = 83.53202258975443\n",
      "loss on batch = 81.67262929289494\n",
      "loss on batch = 84.06150952188172\n",
      "loss on batch = 77.9339704824651\n",
      "loss on batch = 83.37760096279965\n",
      "loss on batch = 82.22154860829966\n",
      "loss on batch = 84.18680883916718\n",
      "loss on batch = 84.08860351581127\n",
      "loss on batch = 83.84405158382496\n",
      "loss on batch = 86.01323941123312\n",
      "loss on batch = 81.19114805466774\n",
      "loss on batch = 86.10538904760134\n",
      "loss on batch = 85.64509148634127\n",
      "loss on batch = 83.62504523988648\n",
      "loss on batch = 84.60610809811661\n",
      "loss on batch = 85.94798825708945\n",
      "loss on batch = 82.55707673867681\n",
      "loss on batch = 84.16616499296161\n",
      "loss on batch = 79.69328997211753\n",
      "loss on batch = 82.13614836045255\n",
      "loss on batch = 80.36016180287707\n",
      "loss on batch = 86.35503284713582\n",
      "loss on batch = 84.73999227051905\n",
      "loss on batch = 81.00822498655415\n",
      "loss on batch = 82.87081040544355\n",
      "loss on batch = 83.72863056640942\n",
      "loss on batch = 82.15676198743165\n",
      "loss on batch = 79.26188752710138\n",
      "loss on batch = 85.03018414175163\n",
      "loss on batch = 79.15041561617053\n",
      "loss on batch = 85.98452382762864\n",
      "loss on batch = 82.24398838382794\n",
      "loss on batch = 86.54477382200295\n",
      "loss on batch = 81.73021046770128\n",
      "loss on batch = 83.567993442823\n",
      "loss on batch = 85.47028714069098\n",
      "loss on batch = 87.91679273510198\n",
      "loss on batch = 79.14171013312267\n",
      "loss on batch = 79.69993130121989\n",
      "loss on batch = 82.0232644878944\n",
      "loss on batch = 83.33447678137108\n",
      "loss on batch = 83.8431041700006\n",
      "loss on batch = 81.88537457743456\n",
      "loss on batch = 78.52346659745757\n",
      "loss on batch = 79.16084365648541\n",
      "loss on batch = 82.77618981237208\n",
      "loss on batch = 87.3818262097285\n",
      "loss on batch = 78.15460033488833\n",
      "loss on batch = 82.22350064547666\n",
      "loss on batch = 80.05932030281787\n",
      "loss on batch = 81.96888601679386\n",
      "loss on batch = 81.49188768441309\n",
      "loss on batch = 82.83851773072246\n",
      "loss on batch = 84.27097855791735\n",
      "loss on batch = 82.12205244777735\n",
      "loss on batch = 81.05844980994091\n",
      "loss on batch = 85.82390835482107\n",
      "loss on batch = 84.05012678295533\n",
      "loss on batch = 83.8475647934827\n",
      "loss on batch = 81.07622378104736\n",
      "loss on batch = 83.07647200376258\n",
      "loss on batch = 80.76735438344485\n",
      "loss on batch = 83.69895400861319\n",
      "loss on batch = 82.83141560088765\n",
      "loss on batch = 82.8180900488639\n",
      "loss on batch = 80.03278968043077\n",
      "loss on batch = 86.58085800278121\n",
      "loss on batch = 84.53186358968871\n",
      "loss on batch = 83.77990015429592\n",
      "loss on batch = 80.96141041012837\n",
      "loss on batch = 82.35783384467517\n",
      "loss on batch = 80.19602637241377\n",
      "loss on batch = 82.9414233540096\n",
      "loss on batch = 79.42364123967361\n",
      "loss on batch = 83.9097559686654\n",
      "loss on batch = 81.97070309904126\n",
      "loss on batch = 82.77932947735977\n",
      "loss on batch = 83.54958042105224\n",
      "loss on batch = 78.91205595844198\n",
      "loss on batch = 78.0617996506927\n",
      "loss on batch = 82.20257372040561\n",
      "loss on batch = 79.97883737164798\n",
      "loss on batch = 86.91979016372808\n",
      "loss on batch = 80.60863190644305\n",
      "loss on batch = 79.57956149499609\n",
      "loss on batch = 82.62503452686664\n",
      "loss on batch = 78.12532867298039\n",
      "loss on batch = 85.99664005355567\n",
      "loss on batch = 79.69426409655593\n",
      "loss on batch = 80.4598655231686\n",
      "loss on batch = 76.017726182546\n",
      "loss on batch = 83.74524260924713\n",
      "loss on batch = 79.29371096135579\n",
      "loss on batch = 80.31367512336232\n",
      "loss on batch = 79.4568848017434\n",
      "loss on batch = 81.39773627317206\n",
      "loss on batch = 84.36541155793147\n",
      "loss on batch = 84.60277366866876\n",
      "loss on batch = 84.10353267974848\n",
      "loss on batch = 83.26641586066212\n",
      "loss on batch = 81.86004952875646\n",
      "loss on batch = 86.29497261837426\n",
      "loss on batch = 79.38570086526069\n",
      "loss on batch = 82.39561287148786\n",
      "loss on batch = 78.07671941768544\n",
      "loss on batch = 84.15979679567528\n",
      "loss on batch = 81.03556520966585\n",
      "loss on batch = 82.45244915253937\n",
      "loss on batch = 81.16792031455546\n",
      "loss on batch = 81.99881733454428\n",
      "loss on batch = 82.39700956166482\n",
      "loss on batch = 82.65149658737386\n",
      "loss on batch = 80.52435826202256\n",
      "loss on batch = 82.59860781733613\n",
      "loss on batch = 81.89854473438656\n",
      "loss on batch = 83.7283011138338\n",
      "loss on batch = 86.87858877924818\n",
      "loss on batch = 80.17704017274086\n",
      "loss on batch = 83.18338926638737\n",
      "loss on batch = 79.01574216630999\n",
      "loss on batch = 87.88147627720791\n",
      "loss on batch = 76.6462005709908\n",
      "loss on batch = 81.06581205095208\n",
      "loss on batch = 81.91913775393533\n",
      "loss on batch = 80.66372924853881\n",
      "loss on batch = 80.50302785484199\n",
      "loss on batch = 79.5740986263009\n",
      "loss on batch = 80.07822000872659\n",
      "loss on batch = 83.33884606951932\n",
      "loss on batch = 77.43763226924355\n",
      "loss on batch = 79.52088625328578\n",
      "loss on batch = 78.46562005896662\n",
      "loss on batch = 79.64666971961708\n",
      "loss on batch = 79.78049374943001\n",
      "loss on batch = 80.28308031053274\n",
      "loss on batch = 80.24733626002275\n",
      "loss on batch = 79.81549383163394\n",
      "loss on batch = 78.90446164462402\n",
      "loss on batch = 81.06221280866836\n",
      "loss on batch = 79.01470138105498\n",
      "loss on batch = 80.2116144807425\n",
      "loss on batch = 81.17086850314284\n",
      "loss on batch = 80.59958338343901\n",
      "loss on batch = 78.7746622762247\n",
      "loss on batch = 83.88333831068994\n",
      "loss on batch = 81.92164640925765\n",
      "loss on batch = 81.1407319538966\n",
      "loss on batch = 81.50235442656395\n",
      "loss on batch = 81.0580575510213\n",
      "loss on batch = 79.7348984858649\n",
      "loss on batch = 82.84705375463949\n",
      "loss on batch = 81.48182813964891\n",
      "loss on batch = 81.70108909793339\n",
      "loss on batch = 80.27840556691422\n",
      "loss on batch = 78.97051789848342\n",
      "loss on batch = 79.49695604284325\n",
      "loss on batch = 80.56224661593899\n",
      "loss on batch = 80.65241294534334\n",
      "loss on batch = 81.28089526984323\n",
      "loss on batch = 82.13740774650103\n",
      "loss on batch = 82.05982546698718\n",
      "loss on batch = 80.44698022121396\n",
      "loss on batch = 83.95786410663695\n",
      "loss on batch = 78.58216774237425\n",
      "loss on batch = 79.58825930868028\n",
      "loss on batch = 79.78508660079862\n",
      "loss on batch = 77.16941791720899\n",
      "loss on batch = 79.07893247997308\n",
      "loss on batch = 83.03338287925912\n",
      "loss on batch = 78.58568556100688\n",
      "loss on batch = 81.52410050577198\n",
      "loss on batch = 82.45916326399988\n",
      "loss on batch = 81.6599134876065\n",
      "loss on batch = 78.97152895760617\n",
      "loss on batch = 80.76389006667813\n",
      "loss on batch = 77.17783001682616\n",
      "loss on batch = 80.96139499524296\n",
      "loss on batch = 83.52619639579027\n",
      "loss on batch = 80.1028704738035\n",
      "loss on batch = 83.12651436606555\n",
      "loss on batch = 79.14725111980123\n",
      "loss on batch = 82.1503166484654\n",
      "loss on batch = 81.55245987052803\n",
      "loss on batch = 79.68403933341426\n",
      "loss on batch = 79.16175936516203\n",
      "loss on batch = 80.16486766880652\n",
      "loss on batch = 80.32953299421769\n",
      "loss on batch = 80.11220908354736\n",
      "loss on batch = 79.73470686696385\n",
      "loss on batch = 80.84600853221953\n",
      "loss on batch = 80.44151645830863\n",
      "loss on batch = 82.8399201570299\n",
      "loss on batch = 80.28603898384075\n",
      "loss on batch = 78.06904356316024\n",
      "loss on batch = 80.52765795119925\n",
      "loss on batch = 82.67019864190097\n",
      "loss on batch = 82.44694840980338\n",
      "loss on batch = 78.9685011842071\n",
      "loss on batch = 84.00097293894841\n",
      "loss on batch = 81.44944494100477\n",
      "loss on batch = 79.27471073333913\n",
      "loss on batch = 79.36921409382535\n",
      "loss on batch = 81.07390012761071\n",
      "loss on batch = 81.36621009145091\n",
      "loss on batch = 79.90613513283213\n",
      "loss on batch = 77.90905480581256\n",
      "loss on batch = 83.79693723792533\n",
      "loss on batch = 77.76121642541764\n",
      "loss on batch = 80.25790274912185\n",
      "loss on batch = 82.20411976720501\n",
      "loss on batch = 80.39172110025288\n",
      "loss on batch = 80.39348713336592\n",
      "loss on batch = 82.37764790635424\n",
      "loss on batch = 78.01758066105896\n",
      "loss on batch = 77.49739696643374\n",
      "loss on batch = 80.50129548248378\n",
      "loss on batch = 79.33454373865706\n",
      "loss on batch = 80.05177005445553\n",
      "loss on batch = 77.48979984424992\n",
      "loss on batch = 81.35427952154983\n",
      "loss on batch = 79.46780678283827\n",
      "loss on batch = 80.26144713488384\n",
      "loss on batch = 82.12264679243086\n",
      "loss on batch = 79.38271842297944\n",
      "loss on batch = 77.53096745562723\n",
      "loss on batch = 81.77300070796599\n",
      "loss on batch = 78.0584478700153\n",
      "loss on batch = 80.24441145760784\n",
      "loss on batch = 79.71033811663372\n",
      "loss on batch = 78.67485351706677\n",
      "loss on batch = 79.23391294681663\n",
      "loss on batch = 82.1589175645151\n",
      "loss on batch = 82.68599345924484\n",
      "loss on batch = 78.52861872991885\n",
      "loss on batch = 84.37285223847428\n",
      "loss on batch = 78.58637638756684\n",
      "loss on batch = 81.40629378338654\n",
      "loss on batch = 80.11305927776093\n",
      "loss on batch = 76.81831044816289\n",
      "loss on batch = 83.26245828044347\n",
      "loss on batch = 77.61952861247872\n",
      "loss on batch = 79.8534894473631\n",
      "loss on batch = 85.7717744164834\n",
      "loss on batch = 77.76247143660879\n",
      "loss on batch = 76.91162747023249\n",
      "loss on batch = 80.71220131554921\n",
      "loss on batch = 78.27969285485902\n",
      "loss on batch = 80.11971490935173\n",
      "loss on batch = 82.51858329888066\n",
      "loss on batch = 80.31933561516503\n",
      "loss on batch = 78.95231638033445\n",
      "loss on batch = 79.64336790776917\n",
      "loss on batch = 79.79868684899236\n",
      "loss on batch = 81.6804524186494\n",
      "loss on batch = 79.13067512326332\n",
      "loss on batch = 76.60940430867711\n",
      "loss on batch = 80.63918404516384\n",
      "loss on batch = 80.32296691251776\n",
      "loss on batch = 78.95749761603068\n",
      "loss on batch = 83.16632904180493\n",
      "loss on batch = 82.38714732733098\n",
      "loss on batch = 79.96316079321801\n",
      "loss on batch = 80.31348308493729\n",
      "loss on batch = 83.00685190548614\n",
      "loss on batch = 78.16413716741972\n",
      "loss on batch = 76.84052259002851\n",
      "loss on batch = 77.1575021059731\n",
      "loss on batch = 83.4032138954885\n",
      "loss on batch = 79.69750661433412\n",
      "loss on batch = 78.29946140000317\n",
      "loss on batch = 79.52882729936098\n",
      "loss on batch = 80.41129774638637\n",
      "loss on batch = 80.61338025651986\n",
      "loss on batch = 79.85824238834921\n",
      "loss on batch = 80.83359002740244\n",
      "loss on batch = 78.24070112057623\n",
      "loss on batch = 80.4329673255205\n",
      "loss on batch = 78.58771904876929\n",
      "loss on batch = 79.09058753561527\n",
      "loss on batch = 81.92981057749647\n",
      "loss on batch = 79.16948495331462\n",
      "loss on batch = 79.65972850103083\n",
      "loss on batch = 81.9589550014226\n",
      "loss on batch = 80.3431160459449\n",
      "loss on batch = 80.624465243559\n",
      "loss on batch = 82.41769312327769\n",
      "loss on batch = 80.60530366869786\n",
      "loss on batch = 78.85454215215324\n",
      "loss on batch = 78.83154013786881\n",
      "loss on batch = 83.47673948581343\n",
      "loss on batch = 78.10343145754587\n",
      "loss on batch = 81.9245526217602\n",
      "loss on batch = 80.06628176009812\n",
      "loss on batch = 78.87105697959139\n",
      "loss on batch = 78.20638470755102\n",
      "loss on batch = 79.36975416126407\n",
      "loss on batch = 83.17234340315895\n",
      "loss on batch = 78.50737489872762\n",
      "loss on batch = 83.24066326567285\n",
      "loss on batch = 79.79872417055307\n",
      "loss on batch = 81.55379230445939\n",
      "loss on batch = 77.15578359010163\n",
      "loss on batch = 77.29193546573677\n",
      "loss on batch = 79.30231271003679\n",
      "loss on batch = 79.80631173998458\n",
      "loss on batch = 76.12385191300092\n",
      "loss on batch = 79.71604022213894\n",
      "loss on batch = 77.72529406514866\n",
      "loss on batch = 75.83582877473786\n",
      "loss on batch = 79.44549956475808\n",
      "loss on batch = 75.09875815346928\n",
      "loss on batch = 82.02951715370591\n",
      "loss on batch = 80.9560789889216\n",
      "loss on batch = 78.87227477585384\n",
      "loss on batch = 79.4754974954019\n",
      "loss on batch = 79.75104674876289\n",
      "loss on batch = 77.01323453732775\n",
      "loss on batch = 79.33040596213894\n",
      "loss on batch = 79.54849316274806\n",
      "loss on batch = 80.86496029767477\n",
      "loss on batch = 77.83432924186866\n",
      "loss on batch = 81.48627017805181\n",
      "loss on batch = 77.0469809804917\n",
      "loss on batch = 81.1449736637324\n",
      "loss on batch = 78.43022159296089\n",
      "loss on batch = 79.97842048915875\n",
      "loss on batch = 79.13951866811\n",
      "loss on batch = 80.10163009909581\n",
      "loss on batch = 76.80129304928742\n",
      "loss on batch = 79.77036875660572\n",
      "loss on batch = 81.94015840254116\n",
      "loss on batch = 76.68871286326899\n",
      "loss on batch = 80.54191186601082\n",
      "loss on batch = 78.98744070124597\n",
      "loss on batch = 79.5772381164974\n",
      "loss on batch = 78.16774640405328\n",
      "loss on batch = 80.45728124228393\n",
      "loss on batch = 84.46629918228774\n",
      "loss on batch = 82.930654421766\n",
      "loss on batch = 79.13495702701542\n",
      "loss on batch = 80.58061725516062\n",
      "loss on batch = 77.42325828328651\n",
      "loss on batch = 82.70543595290738\n",
      "loss on batch = 75.52521475360223\n",
      "loss on batch = 77.8797751787771\n",
      "loss on batch = 78.99695574757028\n",
      "loss on batch = 83.65694433183404\n",
      "loss on batch = 76.31512395432466\n",
      "loss on batch = 79.11974791147779\n",
      "loss on batch = 78.96230615631555\n",
      "loss on batch = 82.20299193217127\n",
      "loss on batch = 79.15062360387132\n",
      "loss on batch = 82.02579695263815\n",
      "loss on batch = 80.44936399852446\n",
      "loss on batch = 81.937125917793\n",
      "loss on batch = 79.30703167489435\n",
      "loss on batch = 77.982960801058\n",
      "loss on batch = 76.78745449077255\n",
      "loss on batch = 78.80320581406922\n",
      "loss on batch = 79.04382809875717\n",
      "loss on batch = 79.22449545263436\n",
      "loss on batch = 79.57618755102706\n",
      "loss on batch = 81.11639429441153\n",
      "loss on batch = 78.74129848296316\n",
      "loss on batch = 77.8386953934427\n",
      "loss on batch = 80.35249855853863\n",
      "loss on batch = 76.82796162463818\n",
      "loss on batch = 80.57282816633081\n",
      "loss on batch = 75.85127648160991\n",
      "loss on batch = 79.53614920450904\n",
      "loss on batch = 75.1762623121297\n",
      "loss on batch = 79.59054852605568\n",
      "loss on batch = 81.18801154520784\n",
      "loss on batch = 79.0716214214251\n",
      "loss on batch = 82.62427820895772\n",
      "loss on batch = 75.52661764916101\n",
      "loss on batch = 80.41854361954378\n",
      "loss on batch = 80.23272171378\n",
      "loss on batch = 78.68176036751078\n",
      "loss on batch = 78.2742940949241\n",
      "loss on batch = 78.59881249302691\n",
      "loss on batch = 78.33536887000433\n",
      "loss on batch = 80.5869987291072\n",
      "loss on batch = 76.76046174237035\n",
      "loss on batch = 77.43490100381432\n",
      "loss on batch = 78.56594369081866\n",
      "loss on batch = 81.03315724338678\n",
      "loss on batch = 78.71191611990324\n",
      "loss on batch = 80.50151279926139\n",
      "loss on batch = 78.88880690993786\n",
      "loss on batch = 79.5603641311643\n",
      "loss on batch = 77.99302844253732\n",
      "loss on batch = 77.8363419269816\n",
      "loss on batch = 80.17489259558204\n",
      "loss on batch = 80.0972174652452\n",
      "loss on batch = 79.36072761260431\n",
      "loss on batch = 83.81474305442605\n",
      "loss on batch = 78.095320929752\n",
      "loss on batch = 81.09368676609937\n",
      "loss on batch = 78.94267617336214\n",
      "loss on batch = 79.35419277340617\n",
      "loss on batch = 84.04715469555634\n",
      "loss on batch = 80.24206715837946\n",
      "loss on batch = 78.57090706261968\n",
      "loss on batch = 79.26351066100572\n",
      "loss on batch = 79.31779827509453\n",
      "loss on batch = 79.53960410018337\n",
      "loss on batch = 78.90250963208035\n",
      "loss on batch = 76.84726416732303\n",
      "loss on batch = 76.96527720453483\n",
      "loss on batch = 76.57858390879065\n",
      "loss on batch = 79.13109979745713\n",
      "loss on batch = 77.82892124104364\n",
      "loss on batch = 79.09747876227597\n",
      "loss on batch = 80.92985869475831\n",
      "loss on batch = 79.03227760231263\n",
      "loss on batch = 76.38284591167665\n",
      "loss on batch = 80.63116102423021\n",
      "loss on batch = 81.20308222192445\n",
      "loss on batch = 83.5071382733586\n",
      "loss on batch = 79.96213600675169\n",
      "loss on batch = 79.06553722391808\n",
      "loss on batch = 79.23529996474778\n",
      "loss on batch = 77.99221589696049\n",
      "loss on batch = 78.77001035630894\n",
      "loss on batch = 80.19168258650929\n",
      "loss on batch = 78.95425399596554\n",
      "loss on batch = 80.25127275133099\n",
      "loss on batch = 78.48228932282626\n",
      "loss on batch = 77.79458488045867\n",
      "loss on batch = 77.97000988738176\n",
      "loss on batch = 77.78800438497359\n",
      "loss on batch = 78.67051096473257\n",
      "loss on batch = 77.21452637489375\n",
      "loss on batch = 77.08820689221511\n",
      "loss on batch = 78.58768216014548\n",
      "loss on batch = 75.69336752540362\n",
      "loss on batch = 80.1628549284714\n",
      "loss on batch = 80.6143338503728\n",
      "loss on batch = 82.83121882338853\n",
      "loss on batch = 80.55653991641654\n",
      "loss on batch = 83.09477764878767\n",
      "loss on batch = 78.2565664115497\n",
      "loss on batch = 80.02281603802967\n",
      "loss on batch = 77.5362697588846\n",
      "loss on batch = 78.20589496098509\n",
      "loss on batch = 76.0894252272593\n",
      "loss on batch = 79.21632947675062\n",
      "loss on batch = 79.69449433638951\n",
      "loss on batch = 79.35058734541917\n",
      "loss on batch = 78.85753215114025\n",
      "loss on batch = 78.60549707994642\n",
      "loss on batch = 77.78684121860111\n",
      "loss on batch = 77.80894023576766\n",
      "loss on batch = 74.55713263998341\n",
      "loss on batch = 79.979347613886\n",
      "loss on batch = 77.23142144168598\n",
      "loss on batch = 75.69149021181619\n",
      "loss on batch = 74.26691651163875\n",
      "loss on batch = 76.09356549811156\n",
      "loss on batch = 78.54938425118101\n",
      "loss on batch = 75.78514175603229\n",
      "loss on batch = 79.85918418012312\n",
      "loss on batch = 78.53503787134903\n",
      "loss on batch = 77.82409603722198\n",
      "loss on batch = 80.18008978562135\n",
      "loss on batch = 77.32351433498934\n",
      "loss on batch = 80.49929367773296\n",
      "loss on batch = 76.95921135644653\n",
      "loss on batch = 78.30303986491145\n",
      "loss on batch = 76.02834331564934\n",
      "loss on batch = 76.37740738938822\n",
      "loss on batch = 76.55252359438546\n",
      "loss on batch = 74.55953543798034\n",
      "loss on batch = 75.47128134579403\n",
      "loss on batch = 80.25331156774591\n",
      "loss on batch = 78.58162274339463\n",
      "loss on batch = 79.17567409796459\n",
      "loss on batch = 80.30384845627142\n",
      "loss on batch = 82.06380281608108\n",
      "loss on batch = 81.52201694288595\n",
      "loss on batch = 79.56555445264897\n",
      "loss on batch = 75.58796431198428\n",
      "loss on batch = 74.73237003010826\n",
      "loss on batch = 74.32464176098921\n",
      "loss on batch = 79.48015230192267\n",
      "loss on batch = 77.31193418018655\n",
      "loss on batch = 77.69717674794734\n",
      "loss on batch = 80.99124037416337\n",
      "loss on batch = 82.54463807849385\n",
      "loss on batch = 78.29461779755206\n",
      "loss on batch = 77.82255206691593\n",
      "loss on batch = 78.71689969494406\n",
      "loss on batch = 76.24902546320203\n",
      "loss on batch = 82.12207594389744\n",
      "loss on batch = 79.05284433052597\n",
      "loss on batch = 77.91832817075657\n",
      "loss on batch = 77.64689397651242\n",
      "loss on batch = 80.9412660267975\n",
      "loss on batch = 82.63095032455809\n",
      "loss on batch = 82.45595351289181\n",
      "loss on batch = 77.8331650275708\n",
      "loss on batch = 80.5012505563113\n",
      "loss on batch = 75.83241372302902\n",
      "loss on batch = 82.04886599284782\n",
      "loss on batch = 79.66634130312117\n",
      "loss on batch = 76.83950877727514\n",
      "loss on batch = 80.71516862966757\n",
      "loss on batch = 78.63886717818798\n",
      "loss on batch = 79.49682835328659\n",
      "loss on batch = 77.39849984344222\n",
      "loss on batch = 82.21444047393248\n",
      "loss on batch = 79.23932628962481\n",
      "loss on batch = 75.27573467555044\n",
      "loss on batch = 76.93713354848063\n",
      "loss on batch = 75.32377197442102\n",
      "loss on batch = 80.06338280946207\n",
      "loss on batch = 77.09859901244411\n",
      "loss on batch = 82.7197626110098\n",
      "loss on batch = 78.89403107197572\n",
      "loss on batch = 82.4982087390955\n",
      "loss on batch = 79.43691610810527\n",
      "loss on batch = 79.4533744601399\n",
      "loss on batch = 82.50706025364344\n",
      "loss on batch = 77.98433405471596\n",
      "loss on batch = 80.77677067713824\n",
      "loss on batch = 78.13251313707605\n",
      "loss on batch = 78.67810060991947\n",
      "loss on batch = 76.02009751416784\n",
      "loss on batch = 77.31858161033196\n",
      "loss on batch = 79.09782164390992\n",
      "loss on batch = 77.3698840909936\n",
      "loss on batch = 78.3210812017515\n",
      "loss on batch = 77.74470008253512\n",
      "loss on batch = 78.83934046010927\n",
      "loss on batch = 77.96167474149297\n",
      "loss on batch = 80.33159604651782\n",
      "loss on batch = 80.56143236307733\n",
      "loss on batch = 80.66546026435537\n",
      "loss on batch = 80.35796773855688\n",
      "loss on batch = 80.16803731179002\n",
      "loss on batch = 78.3873392110511\n",
      "loss on batch = 80.59938409989361\n",
      "loss on batch = 76.76392445052494\n",
      "loss on batch = 78.23701889188288\n",
      "loss on batch = 77.55540058596438\n",
      "loss on batch = 81.16048010957746\n",
      "loss on batch = 79.29262428902037\n",
      "loss on batch = 80.41322690034825\n",
      "loss on batch = 77.58145930833969\n",
      "loss on batch = 80.35427806842843\n",
      "loss on batch = 76.42162143032056\n",
      "loss on batch = 82.24710461924144\n",
      "loss on batch = 78.6644327125825\n",
      "loss on batch = 80.4386684165213\n",
      "loss on batch = 78.53499805863808\n",
      "loss on batch = 82.1148821355836\n",
      "loss on batch = 76.84576342894653\n",
      "loss on batch = 78.6387223396597\n",
      "loss on batch = 80.17914377968123\n",
      "loss on batch = 77.53756604264382\n",
      "loss on batch = 81.75851546627493\n",
      "loss on batch = 79.99977178993763\n",
      "loss on batch = 76.36837131972253\n",
      "loss on batch = 77.03820008040783\n",
      "Acc: 0.9006\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=True)\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(196)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(20)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=1, batch_size=50, lr=.01, method='rmsprop', method_param=0.2)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "on batch = 84.06219733443274\n",
      "loss on batch = 96.96634442356296\n",
      "loss on batch = 84.16391428158781\n",
      "loss on batch = 90.7073357904165\n",
      "loss on batch = 79.01846076060227\n",
      "loss on batch = 96.25606742005124\n",
      "loss on batch = 86.79872517278105\n",
      "loss on batch = 89.12842788418027\n",
      "loss on batch = 86.57321463506597\n",
      "loss on batch = 95.5901503626826\n",
      "loss on batch = 84.1234436267309\n",
      "loss on batch = 92.27592853302158\n",
      "loss on batch = 84.64246809252637\n",
      "loss on batch = 88.3407262563791\n",
      "loss on batch = 89.21267663064728\n",
      "loss on batch = 92.70430073977504\n",
      "loss on batch = 85.42069917346754\n",
      "loss on batch = 89.08860025774813\n",
      "loss on batch = 78.82168340817614\n",
      "loss on batch = 95.5623800156826\n",
      "loss on batch = 81.36197341367541\n",
      "loss on batch = 91.67074388666384\n",
      "loss on batch = 85.46362843208925\n",
      "loss on batch = 91.83574101768804\n",
      "loss on batch = 90.86351512314667\n",
      "loss on batch = 86.74469437970669\n",
      "loss on batch = 82.81455925461096\n",
      "loss on batch = 90.32497904119086\n",
      "loss on batch = 81.6854768629442\n",
      "loss on batch = 88.11497916761365\n",
      "loss on batch = 87.97940787026832\n",
      "loss on batch = 92.21865651968619\n",
      "loss on batch = 86.99596087909677\n",
      "loss on batch = 85.36460533391792\n",
      "loss on batch = 92.14380688005797\n",
      "loss on batch = 89.40336891015593\n",
      "loss on batch = 86.24342263545938\n",
      "loss on batch = 90.56325001310407\n",
      "loss on batch = 90.90266133796945\n",
      "loss on batch = 89.93521393082645\n",
      "loss on batch = 85.11969279542355\n",
      "loss on batch = 89.71505681424159\n",
      "loss on batch = 85.8365056890965\n",
      "loss on batch = 85.76540866073182\n",
      "loss on batch = 80.25186204473312\n",
      "loss on batch = 91.4987182974251\n",
      "loss on batch = 87.5866138668069\n",
      "loss on batch = 88.25322420339413\n",
      "loss on batch = 91.58782126532144\n",
      "loss on batch = 89.13069797882838\n",
      "loss on batch = 87.46163095936936\n",
      "loss on batch = 85.31704791864435\n",
      "loss on batch = 85.80604187380146\n",
      "loss on batch = 82.74282545392569\n",
      "loss on batch = 91.59644833132515\n",
      "loss on batch = 81.96883884714231\n",
      "loss on batch = 91.8219000772348\n",
      "loss on batch = 80.87564925554753\n",
      "loss on batch = 87.14865847438884\n",
      "loss on batch = 81.29224783791125\n",
      "loss on batch = 90.58521939296095\n",
      "loss on batch = 83.2722263820827\n",
      "loss on batch = 86.63610627985945\n",
      "loss on batch = 82.96149822529762\n",
      "loss on batch = 87.1560129294792\n",
      "loss on batch = 81.8375118796157\n",
      "loss on batch = 86.40019530473037\n",
      "loss on batch = 85.48477683340994\n",
      "loss on batch = 84.02595657928053\n",
      "loss on batch = 79.58944184973475\n",
      "loss on batch = 85.20488043314987\n",
      "loss on batch = 81.87050994935619\n",
      "loss on batch = 90.38929334750352\n",
      "loss on batch = 83.94444073636652\n",
      "loss on batch = 83.79068308027878\n",
      "loss on batch = 78.35099513299136\n",
      "loss on batch = 81.97342171338973\n",
      "loss on batch = 79.81420462162406\n",
      "loss on batch = 85.64453803533426\n",
      "loss on batch = 81.95611629129863\n",
      "loss on batch = 85.5962292713182\n",
      "loss on batch = 88.22766173511903\n",
      "loss on batch = 85.71726827316215\n",
      "loss on batch = 90.67353262126322\n",
      "loss on batch = 86.78419709521572\n",
      "loss on batch = 86.64012597122473\n",
      "loss on batch = 83.11943694807003\n",
      "loss on batch = 87.62043445439647\n",
      "loss on batch = 86.48504176479422\n",
      "loss on batch = 84.50961716210625\n",
      "loss on batch = 88.33666542523324\n",
      "loss on batch = 81.48589100915595\n",
      "loss on batch = 81.9019870161051\n",
      "loss on batch = 89.96998765616146\n",
      "loss on batch = 84.30543469937794\n",
      "loss on batch = 83.0903658943384\n",
      "loss on batch = 86.96031317556891\n",
      "loss on batch = 83.20183757963596\n",
      "loss on batch = 84.64092268224881\n",
      "loss on batch = 86.62087911640191\n",
      "loss on batch = 81.76473466960488\n",
      "loss on batch = 87.71238959189773\n",
      "loss on batch = 86.52023660817042\n",
      "loss on batch = 83.90702971039926\n",
      "loss on batch = 78.456382971231\n",
      "loss on batch = 87.55823868173071\n",
      "loss on batch = 83.82803015934135\n",
      "loss on batch = 81.8965694106584\n",
      "loss on batch = 88.63831841912395\n",
      "loss on batch = 81.20205252773091\n",
      "loss on batch = 81.71838804909339\n",
      "loss on batch = 88.19535851831495\n",
      "loss on batch = 82.65860840767346\n",
      "loss on batch = 87.61634182990949\n",
      "loss on batch = 83.59220030910947\n",
      "loss on batch = 82.11699122669035\n",
      "loss on batch = 80.80798319213329\n",
      "loss on batch = 85.84614721864936\n",
      "loss on batch = 80.41632136445695\n",
      "loss on batch = 85.5041612363793\n",
      "loss on batch = 79.11757445503846\n",
      "loss on batch = 84.39741863912045\n",
      "loss on batch = 86.85335257707445\n",
      "loss on batch = 90.17167795173023\n",
      "loss on batch = 86.59602043587265\n",
      "loss on batch = 82.95307922506689\n",
      "loss on batch = 82.219313756754\n",
      "loss on batch = 90.5959291027591\n",
      "loss on batch = 82.84500114109679\n",
      "loss on batch = 89.30141790471346\n",
      "loss on batch = 74.6010157036223\n",
      "loss on batch = 92.83955504507247\n",
      "loss on batch = 88.65148559506511\n",
      "loss on batch = 88.00798975122275\n",
      "loss on batch = 90.33935559810249\n",
      "loss on batch = 88.65072101723202\n",
      "loss on batch = 79.72298836531317\n",
      "loss on batch = 87.44208505903316\n",
      "loss on batch = 76.87951239707603\n",
      "loss on batch = 87.26872981692821\n",
      "loss on batch = 85.3808846025507\n",
      "loss on batch = 85.80219524261753\n",
      "loss on batch = 87.86980387823854\n",
      "loss on batch = 86.60472006072652\n",
      "loss on batch = 82.57319723824017\n",
      "loss on batch = 84.16819098004854\n",
      "loss on batch = 91.54378148704279\n",
      "loss on batch = 82.73357106563797\n",
      "loss on batch = 78.21319631801492\n",
      "loss on batch = 83.4197609643623\n",
      "loss on batch = 76.30086000484755\n",
      "loss on batch = 83.22590507269783\n",
      "loss on batch = 77.72485288957321\n",
      "loss on batch = 80.3868786391889\n",
      "loss on batch = 86.92002375116994\n",
      "loss on batch = 77.15080628042135\n",
      "loss on batch = 84.99820761655191\n",
      "loss on batch = 83.0648244028175\n",
      "loss on batch = 85.7225422701981\n",
      "loss on batch = 84.77124248141297\n",
      "loss on batch = 85.48241415032862\n",
      "loss on batch = 78.82835229231415\n",
      "loss on batch = 82.13481421051836\n",
      "loss on batch = 80.12726271643082\n",
      "loss on batch = 83.00386348563606\n",
      "loss on batch = 80.87839690848378\n",
      "loss on batch = 84.56236072079477\n",
      "loss on batch = 82.33866543637691\n",
      "loss on batch = 88.36934350384365\n",
      "loss on batch = 78.4033980040109\n",
      "loss on batch = 88.79588917501044\n",
      "loss on batch = 81.61291147137013\n",
      "loss on batch = 85.67835743121151\n",
      "loss on batch = 81.83935950322521\n",
      "loss on batch = 85.62437737748624\n",
      "loss on batch = 79.15276478961768\n",
      "loss on batch = 87.74179223152653\n",
      "loss on batch = 84.39134489012511\n",
      "loss on batch = 86.47570655743512\n",
      "loss on batch = 83.97885929106796\n",
      "loss on batch = 81.41262944784529\n",
      "loss on batch = 83.9378049648702\n",
      "loss on batch = 84.39631595795596\n",
      "loss on batch = 82.47089213667337\n",
      "loss on batch = 82.38264498300451\n",
      "loss on batch = 90.39129354278158\n",
      "loss on batch = 78.9601416082722\n",
      "loss on batch = 83.03067307501274\n",
      "loss on batch = 88.25214038073096\n",
      "loss on batch = 82.3101453186645\n",
      "loss on batch = 75.93674810967968\n",
      "loss on batch = 85.42471200304223\n",
      "loss on batch = 74.01769918466115\n",
      "loss on batch = 85.0396778691759\n",
      "loss on batch = 81.44917119194047\n",
      "loss on batch = 86.80682909576811\n",
      "loss on batch = 80.0598306731564\n",
      "loss on batch = 94.65119692062387\n",
      "loss on batch = 78.4177237055784\n",
      "loss on batch = 88.19824983541037\n",
      "loss on batch = 78.47602923452908\n",
      "loss on batch = 87.06684909907153\n",
      "loss on batch = 84.06096303642558\n",
      "loss on batch = 90.9417617785414\n",
      "loss on batch = 81.9132081384615\n",
      "loss on batch = 87.2732360966119\n",
      "loss on batch = 76.84072448137687\n",
      "loss on batch = 88.3482568153986\n",
      "loss on batch = 77.38216441466531\n",
      "loss on batch = 86.4892232262265\n",
      "loss on batch = 74.89669385016072\n",
      "loss on batch = 90.26591210766449\n",
      "loss on batch = 83.32825241660433\n",
      "loss on batch = 91.14932723391263\n",
      "loss on batch = 80.69746434698058\n",
      "loss on batch = 87.04571224815285\n",
      "loss on batch = 76.3614827985665\n",
      "loss on batch = 89.65432917659601\n",
      "loss on batch = 77.01987198997534\n",
      "loss on batch = 85.46789481873913\n",
      "loss on batch = 80.96773625813836\n",
      "loss on batch = 86.5525530040771\n",
      "loss on batch = 82.21144694942831\n",
      "loss on batch = 85.49757513175751\n",
      "loss on batch = 78.94733778878458\n",
      "loss on batch = 91.0742493416912\n",
      "loss on batch = 78.30339526191236\n",
      "loss on batch = 87.87215704548589\n",
      "loss on batch = 78.88452146348935\n",
      "loss on batch = 83.6885668347801\n",
      "loss on batch = 79.55501958598222\n",
      "loss on batch = 82.39096855511974\n",
      "loss on batch = 86.02151320534408\n",
      "loss on batch = 80.79229792550069\n",
      "loss on batch = 86.88785505304298\n",
      "loss on batch = 82.9104086210762\n",
      "loss on batch = 84.8950196995606\n",
      "loss on batch = 85.70271480289242\n",
      "loss on batch = 82.08088501455174\n",
      "loss on batch = 83.22012280507779\n",
      "loss on batch = 84.2339668669687\n",
      "loss on batch = 79.40387892349594\n",
      "loss on batch = 82.57449423426667\n",
      "loss on batch = 86.59786980551112\n",
      "loss on batch = 80.54230359807354\n",
      "loss on batch = 85.33565348051297\n",
      "loss on batch = 84.34975292694956\n",
      "loss on batch = 83.54369590713016\n",
      "loss on batch = 79.57119932138067\n",
      "loss on batch = 81.44782984932564\n",
      "loss on batch = 77.49280093412646\n",
      "loss on batch = 81.384597352834\n",
      "loss on batch = 84.46582737080581\n",
      "loss on batch = 77.1462409962467\n",
      "loss on batch = 82.78650131755299\n",
      "loss on batch = 82.97795630133768\n",
      "loss on batch = 83.97465208546606\n",
      "loss on batch = 76.727582999206\n",
      "loss on batch = 81.20047938190723\n",
      "loss on batch = 89.0370003711962\n",
      "loss on batch = 80.83660797986305\n",
      "loss on batch = 85.19655985749907\n",
      "loss on batch = 84.44551013104831\n",
      "loss on batch = 85.25329733695568\n",
      "loss on batch = 83.16440821538532\n",
      "loss on batch = 91.24727783778734\n",
      "loss on batch = 77.78719527955882\n",
      "loss on batch = 82.20961568996798\n",
      "loss on batch = 85.17322210659127\n",
      "loss on batch = 82.17684813411097\n",
      "loss on batch = 76.53915232302157\n",
      "loss on batch = 84.92905184950145\n",
      "loss on batch = 79.775500792502\n",
      "loss on batch = 81.67307583357345\n",
      "loss on batch = 83.73015472850815\n",
      "loss on batch = 86.03753433758547\n",
      "loss on batch = 76.12419718461967\n",
      "loss on batch = 83.86583001099393\n",
      "loss on batch = 79.81757436015822\n",
      "loss on batch = 88.06836644291855\n",
      "loss on batch = 77.69308147219972\n",
      "loss on batch = 84.8710596076827\n",
      "loss on batch = 78.42173666008759\n",
      "loss on batch = 82.36434202749979\n",
      "loss on batch = 79.2548586535557\n",
      "loss on batch = 87.94424608175399\n",
      "loss on batch = 82.47652800677474\n",
      "loss on batch = 86.59565587446448\n",
      "loss on batch = 80.09335694299233\n",
      "loss on batch = 90.20089362449914\n",
      "loss on batch = 78.98966544662076\n",
      "loss on batch = 87.14845761673725\n",
      "loss on batch = 78.82899297002783\n",
      "loss on batch = 85.19388359520848\n",
      "loss on batch = 84.02003004812491\n",
      "loss on batch = 83.45486834032675\n",
      "loss on batch = 81.11830583007571\n",
      "loss on batch = 88.62481602222815\n",
      "loss on batch = 86.43847868997949\n",
      "loss on batch = 80.71391310965956\n",
      "loss on batch = 83.4776374598121\n",
      "loss on batch = 83.57321588764418\n",
      "loss on batch = 80.16930327706046\n",
      "loss on batch = 86.03417529236884\n",
      "loss on batch = 82.41063948844771\n",
      "loss on batch = 82.91130696554242\n",
      "loss on batch = 79.46094417907807\n",
      "loss on batch = 81.90698140439324\n",
      "loss on batch = 81.47165400062401\n",
      "loss on batch = 86.21266445726312\n",
      "loss on batch = 76.72675909777442\n",
      "loss on batch = 91.31759752538818\n",
      "loss on batch = 80.80432037075877\n",
      "loss on batch = 90.56569830891267\n",
      "loss on batch = 77.28398757000835\n",
      "loss on batch = 90.42256665483441\n",
      "loss on batch = 77.78330175765443\n",
      "loss on batch = 86.07308430355775\n",
      "loss on batch = 74.42829904636655\n",
      "loss on batch = 86.75135721905049\n",
      "loss on batch = 75.37475287516409\n",
      "loss on batch = 86.59723228648464\n",
      "loss on batch = 81.71543575196132\n",
      "loss on batch = 80.5091271856117\n",
      "loss on batch = 76.19887451114862\n",
      "loss on batch = 85.13114934265943\n",
      "loss on batch = 75.68774915390671\n",
      "loss on batch = 85.09289562218444\n",
      "loss on batch = 79.76874415822925\n",
      "loss on batch = 84.62820194722008\n",
      "loss on batch = 81.46805582900473\n",
      "loss on batch = 81.13416774180789\n",
      "loss on batch = 79.41070055410637\n",
      "loss on batch = 81.25930101986194\n",
      "loss on batch = 79.4210266089072\n",
      "loss on batch = 82.32856209656136\n",
      "loss on batch = 76.13322184781985\n",
      "loss on batch = 84.40822945387004\n",
      "loss on batch = 77.8109001363508\n",
      "loss on batch = 80.98199483634588\n",
      "loss on batch = 83.6523209390287\n",
      "loss on batch = 84.54871849106794\n",
      "loss on batch = 78.51246891709094\n",
      "loss on batch = 89.48133783406317\n",
      "loss on batch = 78.67662979812673\n",
      "loss on batch = 85.55650132463308\n",
      "loss on batch = 73.33540405996976\n",
      "loss on batch = 87.55267681316505\n",
      "loss on batch = 81.20294106529306\n",
      "loss on batch = 80.87522959194013\n",
      "loss on batch = 76.89268824207649\n",
      "loss on batch = 82.88022245604292\n",
      "loss on batch = 75.84988713785752\n",
      "loss on batch = 88.19264940811186\n",
      "loss on batch = 77.42090503132687\n",
      "loss on batch = 84.4155041094977\n",
      "loss on batch = 76.18853114082893\n",
      "loss on batch = 83.98462813533092\n",
      "loss on batch = 80.97915976875478\n",
      "loss on batch = 82.01908481394412\n",
      "loss on batch = 73.91803787324295\n",
      "loss on batch = 84.93911959180194\n",
      "loss on batch = 80.88789626613826\n",
      "loss on batch = 86.50726627624758\n",
      "loss on batch = 84.43520300945045\n",
      "loss on batch = 79.88112021146777\n",
      "loss on batch = 80.40885228386577\n",
      "loss on batch = 82.42200371975204\n",
      "loss on batch = 78.40956867153693\n",
      "loss on batch = 86.2321522563349\n",
      "loss on batch = 84.78414112372235\n",
      "loss on batch = 87.13825325329111\n",
      "loss on batch = 86.53725767502951\n",
      "loss on batch = 83.36259117969394\n",
      "loss on batch = 78.62965418392278\n",
      "loss on batch = 80.82224905047826\n",
      "loss on batch = 87.43076686007544\n",
      "loss on batch = 84.38044073530114\n",
      "loss on batch = 79.66221965412684\n",
      "loss on batch = 81.71699276675085\n",
      "loss on batch = 80.69642541732964\n",
      "loss on batch = 82.82318269835133\n",
      "loss on batch = 85.17323526990118\n",
      "loss on batch = 81.77569081394061\n",
      "loss on batch = 78.97983609077117\n",
      "loss on batch = 78.63223324877018\n",
      "loss on batch = 77.8244498910739\n",
      "loss on batch = 80.59012497814919\n",
      "loss on batch = 80.4542724056142\n",
      "loss on batch = 85.0652485310109\n",
      "loss on batch = 77.255758479629\n",
      "loss on batch = 81.98160506990826\n",
      "loss on batch = 77.15138700525567\n",
      "loss on batch = 84.10611227965609\n",
      "loss on batch = 86.26915599328176\n",
      "loss on batch = 78.03040606494034\n",
      "loss on batch = 76.55430812011917\n",
      "loss on batch = 79.68375907685977\n",
      "loss on batch = 83.52056727718886\n",
      "loss on batch = 82.25533155845028\n",
      "loss on batch = 81.89215785089516\n",
      "loss on batch = 85.3928708244789\n",
      "loss on batch = 82.93435330748996\n",
      "loss on batch = 82.9389583911344\n",
      "loss on batch = 77.72041024527127\n",
      "loss on batch = 83.41980356685215\n",
      "loss on batch = 79.9579529203713\n",
      "loss on batch = 80.52953568005447\n",
      "loss on batch = 81.30668724579868\n",
      "loss on batch = 84.71333589177445\n",
      "loss on batch = 75.86816028893725\n",
      "loss on batch = 85.16954682900044\n",
      "loss on batch = 74.87868568291319\n",
      "loss on batch = 85.50691144780833\n",
      "loss on batch = 80.68450549198676\n",
      "loss on batch = 85.89957972574611\n",
      "loss on batch = 80.81665771287217\n",
      "loss on batch = 81.24700475684463\n",
      "loss on batch = 79.58090133418966\n",
      "loss on batch = 86.31983477143199\n",
      "loss on batch = 81.8475004342609\n",
      "loss on batch = 81.71804908353708\n",
      "loss on batch = 78.40505705567497\n",
      "loss on batch = 84.117317933913\n",
      "loss on batch = 77.57614055684449\n",
      "loss on batch = 91.80552855151134\n",
      "loss on batch = 81.01260887092118\n",
      "loss on batch = 86.62249220866188\n",
      "loss on batch = 81.01747608338985\n",
      "loss on batch = 85.15373567389429\n",
      "loss on batch = 84.39706016923773\n",
      "loss on batch = 84.78305238810174\n",
      "loss on batch = 77.75271534505147\n",
      "loss on batch = 80.6355549331638\n",
      "loss on batch = 74.0939101301107\n",
      "loss on batch = 79.69128512676477\n",
      "loss on batch = 73.02938045736765\n",
      "loss on batch = 84.57634930636289\n",
      "loss on batch = 80.71869158750505\n",
      "loss on batch = 83.32717728975608\n",
      "loss on batch = 81.93189786773806\n",
      "loss on batch = 78.60998413031824\n",
      "loss on batch = 73.90998930149402\n",
      "loss on batch = 82.61883772210555\n",
      "loss on batch = 79.51180596796706\n",
      "loss on batch = 84.94620961569677\n",
      "loss on batch = 83.10246762798975\n",
      "loss on batch = 79.8441738114197\n",
      "loss on batch = 81.92990656965821\n",
      "loss on batch = 74.50895797088013\n",
      "loss on batch = 85.9625365845318\n",
      "loss on batch = 83.32793667770807\n",
      "loss on batch = 81.51458652089863\n",
      "loss on batch = 83.38670972257536\n",
      "loss on batch = 82.3684455234401\n",
      "loss on batch = 79.12498230267563\n",
      "loss on batch = 78.53881938124863\n",
      "loss on batch = 78.7371218965971\n",
      "loss on batch = 79.94023944398877\n",
      "loss on batch = 78.41729516949474\n",
      "loss on batch = 77.36475402666221\n",
      "loss on batch = 81.80098130528246\n",
      "loss on batch = 80.27688087708833\n",
      "loss on batch = 82.46845729937584\n",
      "loss on batch = 88.78990956464737\n",
      "loss on batch = 80.56058080417901\n",
      "loss on batch = 79.88770860673372\n",
      "loss on batch = 84.926864423705\n",
      "loss on batch = 77.13185304151251\n",
      "loss on batch = 79.85579241218143\n",
      "loss on batch = 85.41888519593849\n",
      "loss on batch = 81.4956519859658\n",
      "loss on batch = 78.13977104946848\n",
      "loss on batch = 79.40356385877912\n",
      "loss on batch = 80.2902630774338\n",
      "loss on batch = 80.67515146161139\n",
      "loss on batch = 69.77088844473317\n",
      "loss on batch = 87.25786582051626\n",
      "loss on batch = 78.34121538107019\n",
      "loss on batch = 82.68888344146221\n",
      "loss on batch = 76.06067657337377\n",
      "loss on batch = 86.87729133215997\n",
      "loss on batch = 72.92170007959353\n",
      "loss on batch = 82.92595001368142\n",
      "loss on batch = 68.79141265362782\n",
      "loss on batch = 89.73973967116567\n",
      "loss on batch = 74.5050886995906\n",
      "loss on batch = 83.66844416885428\n",
      "loss on batch = 76.84228579707741\n",
      "loss on batch = 85.8978247277823\n",
      "loss on batch = 74.75886543422587\n",
      "loss on batch = 87.74378031975229\n",
      "loss on batch = 71.73081235854195\n",
      "loss on batch = 91.1907924261834\n",
      "loss on batch = 71.7494331756882\n",
      "loss on batch = 82.59036420575268\n",
      "loss on batch = 72.56643286079222\n",
      "loss on batch = 80.12547167742251\n",
      "loss on batch = 77.59015235713127\n",
      "loss on batch = 82.41881419345589\n",
      "loss on batch = 72.19985314442334\n",
      "loss on batch = 83.91204541458134\n",
      "loss on batch = 76.27057730925645\n",
      "loss on batch = 84.30427144411546\n",
      "loss on batch = 77.51514295152022\n",
      "loss on batch = 83.68103974568938\n",
      "loss on batch = 70.62293248400763\n",
      "loss on batch = 82.69738035597665\n",
      "loss on batch = 72.08183324754457\n",
      "loss on batch = 81.77831054190828\n",
      "loss on batch = 78.67493673362698\n",
      "loss on batch = 84.64506872231976\n",
      "loss on batch = 80.2982771639016\n",
      "loss on batch = 79.28842805643279\n",
      "loss on batch = 81.8014338612684\n",
      "loss on batch = 91.31504251551117\n",
      "loss on batch = 80.45500296525218\n",
      "loss on batch = 85.98574891675084\n",
      "loss on batch = 79.76722900713848\n",
      "loss on batch = 82.09596491378562\n",
      "loss on batch = 77.04957370703649\n",
      "loss on batch = 85.34657042488543\n",
      "loss on batch = 75.83520019390943\n",
      "loss on batch = 85.30586372031618\n",
      "loss on batch = 81.09069902788274\n",
      "loss on batch = 82.07978651724072\n",
      "loss on batch = 83.94111005849274\n",
      "loss on batch = 81.29445282336174\n",
      "loss on batch = 80.24699925656851\n",
      "loss on batch = 74.6172431225981\n",
      "loss on batch = 84.78132020905238\n",
      "loss on batch = 81.45597394666436\n",
      "loss on batch = 75.9273449827405\n",
      "loss on batch = 85.47797359434858\n",
      "loss on batch = 83.85554807662419\n",
      "loss on batch = 76.63479242288369\n",
      "loss on batch = 77.07378949559197\n",
      "loss on batch = 83.82298988711594\n",
      "loss on batch = 78.25306528129298\n",
      "loss on batch = 81.12437365024365\n",
      "loss on batch = 72.33882073297738\n",
      "loss on batch = 79.2521671058984\n",
      "loss on batch = 79.47720221545958\n",
      "loss on batch = 79.44961337217237\n",
      "loss on batch = 79.21246290359443\n",
      "loss on batch = 83.58198522543393\n",
      "loss on batch = 85.16589125738759\n",
      "loss on batch = 80.63818204714649\n",
      "loss on batch = 81.85665557514676\n",
      "loss on batch = 82.51119487320575\n",
      "loss on batch = 79.70306996897938\n",
      "loss on batch = 78.05241583670505\n",
      "loss on batch = 83.44139984582287\n",
      "loss on batch = 77.41063346856595\n",
      "loss on batch = 82.29260382735123\n",
      "loss on batch = 82.40670355651419\n",
      "loss on batch = 81.75448482678333\n",
      "loss on batch = 86.82112505073648\n",
      "loss on batch = 73.10545609540998\n",
      "loss on batch = 80.8336708896212\n",
      "loss on batch = 79.55813296310416\n",
      "loss on batch = 85.02706661446688\n",
      "loss on batch = 74.92445918013762\n",
      "loss on batch = 82.19602407577915\n",
      "loss on batch = 80.27131926535077\n",
      "loss on batch = 83.85505717079307\n",
      "loss on batch = 83.68817854560304\n",
      "loss on batch = 82.47577112252543\n",
      "loss on batch = 80.29981984533879\n",
      "loss on batch = 86.63308103844297\n",
      "loss on batch = 74.70713525745079\n",
      "loss on batch = 84.70792280782071\n",
      "loss on batch = 78.6463645031917\n",
      "loss on batch = 84.84651464014587\n",
      "loss on batch = 76.03691813197958\n",
      "loss on batch = 81.77477991635783\n",
      "loss on batch = 78.29858824322788\n",
      "loss on batch = 86.4238207619572\n",
      "loss on batch = 80.15161319467771\n",
      "loss on batch = 79.52842535241813\n",
      "loss on batch = 78.72241737984302\n",
      "loss on batch = 85.50877507719046\n",
      "loss on batch = 78.0291408188748\n",
      "loss on batch = 85.2076432102975\n",
      "loss on batch = 77.70945373567645\n",
      "loss on batch = 82.8962920839918\n",
      "loss on batch = 80.43297844023158\n",
      "loss on batch = 85.53261773108501\n",
      "loss on batch = 75.90838328248716\n",
      "loss on batch = 82.33834906095981\n",
      "loss on batch = 75.70200867962464\n",
      "Acc: 0.8831\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=True)\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(200)\n",
    "nn.add_layer(150)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(50)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=1, batch_size=50, lr=.01, method='rmsprop', method_param=0.2)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "80.1277811524182\n",
      "loss on batch = 193.04373605508303\n",
      "loss on batch = 188.1689680002682\n",
      "loss on batch = 185.85789388573812\n",
      "loss on batch = 172.42014163145132\n",
      "loss on batch = 185.28543234243995\n",
      "loss on batch = 180.98445915993972\n",
      "loss on batch = 189.85670771331863\n",
      "loss on batch = 177.04707061200924\n",
      "loss on batch = 189.992997349745\n",
      "loss on batch = 182.55791083350644\n",
      "loss on batch = 190.4439647354813\n",
      "loss on batch = 183.8342904044249\n",
      "loss on batch = 181.95383816469922\n",
      "loss on batch = 169.41992910247433\n",
      "loss on batch = 189.65554565544855\n",
      "loss on batch = 184.774647327815\n",
      "loss on batch = 182.47457220825632\n",
      "loss on batch = 175.6107130340464\n",
      "loss on batch = 184.4432598907973\n",
      "loss on batch = 175.5019768132971\n",
      "loss on batch = 184.1124680176759\n",
      "loss on batch = 183.81707430220678\n",
      "loss on batch = 179.81728990112106\n",
      "loss on batch = 178.06831501121331\n",
      "loss on batch = 188.02566015099012\n",
      "loss on batch = 183.33143216417974\n",
      "loss on batch = 186.20303731141945\n",
      "loss on batch = 180.63430715097246\n",
      "loss on batch = 182.52216690716148\n",
      "loss on batch = 174.7675757259674\n",
      "loss on batch = 182.82210724588754\n",
      "loss on batch = 188.00257542523948\n",
      "loss on batch = 190.9478257906061\n",
      "loss on batch = 185.51831195534604\n",
      "loss on batch = 177.9642788887689\n",
      "loss on batch = 179.58110030064518\n",
      "loss on batch = 184.70445640263847\n",
      "loss on batch = 170.87155576620967\n",
      "loss on batch = 171.96852696357854\n",
      "loss on batch = 190.30060252599796\n",
      "loss on batch = 172.97901900310904\n",
      "loss on batch = 190.00244032167313\n",
      "loss on batch = 180.311486865856\n",
      "loss on batch = 186.77467449869744\n",
      "loss on batch = 180.23543189855187\n",
      "loss on batch = 179.84105319571117\n",
      "loss on batch = 179.34692292411782\n",
      "loss on batch = 184.87832669372588\n",
      "loss on batch = 174.77730600267108\n",
      "loss on batch = 178.19834568663893\n",
      "loss on batch = 179.04090162297825\n",
      "loss on batch = 181.12241983355437\n",
      "loss on batch = 184.95799686577578\n",
      "loss on batch = 186.38686865953397\n",
      "loss on batch = 186.74226540037034\n",
      "loss on batch = 189.1641332575532\n",
      "loss on batch = 186.67601735104412\n",
      "loss on batch = 175.07339326569374\n",
      "loss on batch = 183.14014123185956\n",
      "loss on batch = 179.6147766705495\n",
      "loss on batch = 180.5501918642642\n",
      "loss on batch = 173.01961511323196\n",
      "loss on batch = 182.81180315904385\n",
      "loss on batch = 180.35055693008235\n",
      "loss on batch = 183.36969942324845\n",
      "loss on batch = 166.84400829096012\n",
      "loss on batch = 183.56790626359185\n",
      "loss on batch = 174.9931708779728\n",
      "loss on batch = 185.60186475669684\n",
      "loss on batch = 181.12357239216922\n",
      "loss on batch = 185.34144987963322\n",
      "loss on batch = 178.79993359520245\n",
      "loss on batch = 185.18984685432923\n",
      "loss on batch = 176.57564772869492\n",
      "loss on batch = 186.6028558546732\n",
      "loss on batch = 171.55213707325822\n",
      "loss on batch = 185.48965221962274\n",
      "loss on batch = 185.12901279779868\n",
      "loss on batch = 179.0368692785786\n",
      "loss on batch = 180.3455372340216\n",
      "loss on batch = 187.51725741093878\n",
      "loss on batch = 177.74148347123383\n",
      "loss on batch = 187.5484808707322\n",
      "loss on batch = 176.71112594171808\n",
      "loss on batch = 187.99891042288309\n",
      "loss on batch = 179.16737497719527\n",
      "loss on batch = 185.7768921664319\n",
      "loss on batch = 174.07789908482738\n",
      "loss on batch = 192.07116301345195\n",
      "loss on batch = 183.00171454917705\n",
      "loss on batch = 191.13490562799785\n",
      "loss on batch = 184.20394576602047\n",
      "loss on batch = 195.64127594103087\n",
      "loss on batch = 183.8736613438839\n",
      "loss on batch = 186.32309071870563\n",
      "loss on batch = 182.81869929944003\n",
      "loss on batch = 186.3091218273927\n",
      "loss on batch = 187.08278057296167\n",
      "loss on batch = 184.56260644402556\n",
      "loss on batch = 181.69923830670047\n",
      "loss on batch = 190.96413781378328\n",
      "loss on batch = 178.99617476889952\n",
      "loss on batch = 191.61408959473818\n",
      "loss on batch = 179.9676872511363\n",
      "loss on batch = 178.8255640621406\n",
      "loss on batch = 169.64949728620078\n",
      "loss on batch = 186.92804882196594\n",
      "loss on batch = 182.56286850008317\n",
      "loss on batch = 184.07944844535012\n",
      "loss on batch = 187.78134878690338\n",
      "loss on batch = 184.19101860716555\n",
      "loss on batch = 187.1941938856319\n",
      "loss on batch = 175.30141491532237\n",
      "loss on batch = 177.65054583598814\n",
      "loss on batch = 182.1146105778639\n",
      "loss on batch = 186.19314532379158\n",
      "loss on batch = 181.69260531652355\n",
      "loss on batch = 177.171747566498\n",
      "loss on batch = 189.86658946270717\n",
      "loss on batch = 178.1230814133475\n",
      "loss on batch = 176.7528741811241\n",
      "loss on batch = 185.1588403268407\n",
      "loss on batch = 185.3822651934296\n",
      "loss on batch = 178.06732785047674\n",
      "loss on batch = 178.63455392379865\n",
      "loss on batch = 177.50942815585205\n",
      "loss on batch = 184.78862754055825\n",
      "loss on batch = 175.97850892914636\n",
      "loss on batch = 184.0369286901629\n",
      "loss on batch = 187.25871195369123\n",
      "loss on batch = 183.31334537312455\n",
      "loss on batch = 169.5885637227849\n",
      "loss on batch = 190.49783786287443\n",
      "loss on batch = 175.40990275240222\n",
      "loss on batch = 189.31776619572372\n",
      "loss on batch = 172.7103282438765\n",
      "loss on batch = 187.3882692907825\n",
      "loss on batch = 170.72057286365313\n",
      "loss on batch = 182.69266708401528\n",
      "loss on batch = 178.46216826583014\n",
      "loss on batch = 172.10647223094767\n",
      "loss on batch = 176.5214225789341\n",
      "loss on batch = 179.80738896976783\n",
      "loss on batch = 181.98372576547965\n",
      "loss on batch = 172.15007487521217\n",
      "loss on batch = 177.70187323694807\n",
      "loss on batch = 187.38165349900498\n",
      "loss on batch = 174.17738742047553\n",
      "loss on batch = 181.9856338593823\n",
      "loss on batch = 183.34752852819165\n",
      "loss on batch = 177.02743955791777\n",
      "loss on batch = 188.4103081086568\n",
      "loss on batch = 167.7844512146446\n",
      "loss on batch = 186.37822548671824\n",
      "loss on batch = 182.15072785314223\n",
      "loss on batch = 181.54949169628904\n",
      "loss on batch = 171.6639776546957\n",
      "loss on batch = 184.289721779684\n",
      "loss on batch = 182.4180567284326\n",
      "loss on batch = 180.43360379978776\n",
      "loss on batch = 175.47640112693728\n",
      "loss on batch = 178.1379264762772\n",
      "loss on batch = 176.63499978982782\n",
      "loss on batch = 179.98978277515818\n",
      "loss on batch = 182.82360959026707\n",
      "loss on batch = 182.90119558265286\n",
      "loss on batch = 188.8915314224319\n",
      "loss on batch = 181.6680707026822\n",
      "loss on batch = 183.98363131102775\n",
      "loss on batch = 180.06635682570806\n",
      "loss on batch = 190.1484772754992\n",
      "loss on batch = 175.0968022518186\n",
      "loss on batch = 195.59554737004106\n",
      "loss on batch = 176.4887183296188\n",
      "loss on batch = 179.67547404169323\n",
      "loss on batch = 183.89575530773476\n",
      "loss on batch = 171.45818866901996\n",
      "loss on batch = 186.40022728967364\n",
      "loss on batch = 183.59270039876833\n",
      "loss on batch = 188.993701048675\n",
      "loss on batch = 184.73331132994184\n",
      "loss on batch = 189.68630090082695\n",
      "loss on batch = 182.76481964001928\n",
      "loss on batch = 180.705866669206\n",
      "loss on batch = 179.0050244355051\n",
      "loss on batch = 180.37650257716194\n",
      "loss on batch = 185.00489072720052\n",
      "loss on batch = 182.4846267442766\n",
      "loss on batch = 186.60295733132006\n",
      "loss on batch = 177.53695276992204\n",
      "loss on batch = 180.75431154946978\n",
      "loss on batch = 183.4554430740859\n",
      "loss on batch = 170.1756151518697\n",
      "loss on batch = 174.23106168262274\n",
      "loss on batch = 186.79154291755125\n",
      "loss on batch = 178.77573083183125\n",
      "loss on batch = 184.86099782689064\n",
      "loss on batch = 187.53942672731813\n",
      "loss on batch = 185.13093882127836\n",
      "loss on batch = 180.01738366041695\n",
      "loss on batch = 184.29986717493853\n",
      "loss on batch = 187.60780814937294\n",
      "loss on batch = 180.68488595466295\n",
      "loss on batch = 179.24375587756225\n",
      "loss on batch = 178.27392634894667\n",
      "loss on batch = 175.37762674122922\n",
      "loss on batch = 184.66891241173124\n",
      "loss on batch = 171.67335785331971\n",
      "loss on batch = 179.21385981675712\n",
      "loss on batch = 178.48803971054\n",
      "loss on batch = 182.3942261246391\n",
      "loss on batch = 182.99511497214667\n",
      "loss on batch = 181.69136976429093\n",
      "loss on batch = 190.15056742342333\n",
      "loss on batch = 182.28451028060448\n",
      "loss on batch = 183.0410953720496\n",
      "loss on batch = 180.93715148691211\n",
      "loss on batch = 176.45896015575647\n",
      "loss on batch = 187.66004644627083\n",
      "loss on batch = 182.60983323638374\n",
      "loss on batch = 180.52109160839603\n",
      "loss on batch = 187.0963239708542\n",
      "loss on batch = 187.73104289392518\n",
      "loss on batch = 182.27028579992927\n",
      "loss on batch = 182.22061146011907\n",
      "loss on batch = 180.68964749987472\n",
      "loss on batch = 184.4727111359199\n",
      "loss on batch = 173.96646615993396\n",
      "loss on batch = 182.5406699077118\n",
      "loss on batch = 172.08289874104406\n",
      "loss on batch = 188.61811480029127\n",
      "loss on batch = 180.78213837262336\n",
      "loss on batch = 176.10283480848707\n",
      "loss on batch = 178.40485327232125\n",
      "loss on batch = 174.33651609956786\n",
      "loss on batch = 175.2472921517388\n",
      "loss on batch = 177.81149677335566\n",
      "loss on batch = 187.56207340662098\n",
      "loss on batch = 175.8813274201587\n",
      "loss on batch = 189.55576526406907\n",
      "loss on batch = 182.66413555914818\n",
      "loss on batch = 178.73151943823262\n",
      "loss on batch = 180.72140605140095\n",
      "loss on batch = 189.36181998236987\n",
      "loss on batch = 173.0019577983985\n",
      "loss on batch = 183.57673935047924\n",
      "loss on batch = 181.5901508724931\n",
      "loss on batch = 176.17037385020433\n",
      "loss on batch = 177.74027626016675\n",
      "loss on batch = 171.39023488528764\n",
      "loss on batch = 175.05456339514177\n",
      "loss on batch = 183.662407580101\n",
      "loss on batch = 173.3740534483729\n",
      "loss on batch = 181.21288793330558\n",
      "loss on batch = 176.30163455889846\n",
      "loss on batch = 179.42542226256882\n",
      "loss on batch = 180.69645509983468\n",
      "loss on batch = 175.11294555955118\n",
      "loss on batch = 185.41728456840292\n",
      "loss on batch = 179.94399697731347\n",
      "loss on batch = 178.95556980771937\n",
      "loss on batch = 171.3591636303202\n",
      "loss on batch = 188.5812972815697\n",
      "loss on batch = 177.80208735036598\n",
      "loss on batch = 188.66323496936172\n",
      "loss on batch = 177.65986923006594\n",
      "loss on batch = 189.35687772848001\n",
      "loss on batch = 169.24662194180965\n",
      "loss on batch = 187.5116150492679\n",
      "loss on batch = 172.88100234606145\n",
      "loss on batch = 190.45947754543832\n",
      "loss on batch = 164.602176824911\n",
      "loss on batch = 195.94455380113897\n",
      "loss on batch = 178.7082798834989\n",
      "loss on batch = 188.23214101454255\n",
      "loss on batch = 164.4071940483168\n",
      "loss on batch = 191.33776778728378\n",
      "loss on batch = 172.16344519858282\n",
      "loss on batch = 182.65638951113522\n",
      "loss on batch = 170.72278959513739\n",
      "loss on batch = 191.2032445201801\n",
      "loss on batch = 167.4594942325566\n",
      "loss on batch = 189.23330895324835\n",
      "loss on batch = 172.95173476595875\n",
      "loss on batch = 177.7579106141771\n",
      "loss on batch = 169.26781756284348\n",
      "loss on batch = 187.22164851480616\n",
      "loss on batch = 174.98446806683387\n",
      "loss on batch = 183.26731663703356\n",
      "loss on batch = 172.0591452453025\n",
      "loss on batch = 182.07351349850424\n",
      "loss on batch = 165.5824155593693\n",
      "loss on batch = 189.7819287785236\n",
      "loss on batch = 186.694688995791\n",
      "loss on batch = 178.55688208757502\n",
      "loss on batch = 177.8090630637879\n",
      "loss on batch = 192.64236171290668\n",
      "loss on batch = 174.75698948196543\n",
      "loss on batch = 187.2769523284978\n",
      "loss on batch = 180.76132237416684\n",
      "loss on batch = 194.42902766646412\n",
      "loss on batch = 172.914930564622\n",
      "loss on batch = 190.31916465321916\n",
      "loss on batch = 178.07813537590312\n",
      "loss on batch = 181.95205621554902\n",
      "loss on batch = 162.24751245242317\n",
      "loss on batch = 183.52571445391797\n",
      "loss on batch = 165.2011985537542\n",
      "loss on batch = 186.60383942904042\n",
      "loss on batch = 166.42534196278746\n",
      "loss on batch = 185.35566086334197\n",
      "loss on batch = 172.8584949304843\n",
      "loss on batch = 186.58963016677848\n",
      "loss on batch = 171.48718416767258\n",
      "loss on batch = 192.634370973352\n",
      "loss on batch = 168.11948373353852\n",
      "loss on batch = 186.43204895378517\n",
      "loss on batch = 162.20999955878278\n",
      "loss on batch = 191.51438746185565\n",
      "loss on batch = 167.87940873440445\n",
      "loss on batch = 199.05906554321064\n",
      "loss on batch = 167.1894845108937\n",
      "loss on batch = 193.75965708848602\n",
      "loss on batch = 176.0876491580007\n",
      "loss on batch = 188.7249452432648\n",
      "loss on batch = 175.26790642917496\n",
      "loss on batch = 181.9461637384298\n",
      "loss on batch = 167.52024222974805\n",
      "loss on batch = 190.3762340604161\n",
      "loss on batch = 175.14928483185327\n",
      "loss on batch = 182.12165504062864\n",
      "loss on batch = 172.3453946583864\n",
      "loss on batch = 186.59955600893989\n",
      "loss on batch = 179.845777697137\n",
      "loss on batch = 185.25296202024867\n",
      "loss on batch = 174.07426320890704\n",
      "loss on batch = 186.89651702753264\n",
      "loss on batch = 169.28167567893053\n",
      "loss on batch = 185.96736820230305\n",
      "loss on batch = 169.08394347601222\n",
      "loss on batch = 187.68982227695506\n",
      "loss on batch = 171.05072010620987\n",
      "loss on batch = 190.6598999133471\n",
      "loss on batch = 172.8695830243668\n",
      "loss on batch = 175.35877367288737\n",
      "loss on batch = 173.0522358333319\n",
      "loss on batch = 184.91624533005017\n",
      "loss on batch = 170.50924877223636\n",
      "loss on batch = 184.30913848082636\n",
      "loss on batch = 168.75799796879872\n",
      "loss on batch = 179.81986837652838\n",
      "loss on batch = 166.00170860615395\n",
      "loss on batch = 182.28012587071044\n",
      "loss on batch = 173.26190246313774\n",
      "loss on batch = 184.33775073547702\n",
      "loss on batch = 168.13085109074729\n",
      "loss on batch = 183.7031465778828\n",
      "loss on batch = 183.50720151012843\n",
      "loss on batch = 182.87499877435417\n",
      "loss on batch = 167.9909265704065\n",
      "loss on batch = 190.16477855784876\n",
      "loss on batch = 174.5079031166169\n",
      "loss on batch = 190.14835203768322\n",
      "loss on batch = 172.08431790003584\n",
      "loss on batch = 177.57700796125428\n",
      "loss on batch = 178.14630316319545\n",
      "loss on batch = 186.4454624131791\n",
      "loss on batch = 172.19879787916346\n",
      "loss on batch = 183.57595084097522\n",
      "loss on batch = 176.14475082915448\n",
      "loss on batch = 181.56285783492183\n",
      "loss on batch = 168.81889815781574\n",
      "loss on batch = 190.52239493501747\n",
      "loss on batch = 187.41596722093965\n",
      "loss on batch = 185.127140678123\n",
      "loss on batch = 174.98468005467004\n",
      "loss on batch = 174.45349989505266\n",
      "loss on batch = 177.32886630708427\n",
      "loss on batch = 173.85222378093817\n",
      "loss on batch = 179.59233872976228\n",
      "loss on batch = 180.20621317908706\n",
      "loss on batch = 179.5479829395507\n",
      "loss on batch = 183.6066423514199\n",
      "loss on batch = 184.4740632351846\n",
      "loss on batch = 185.52723457514588\n",
      "loss on batch = 175.38314314610912\n",
      "loss on batch = 180.68446625601467\n",
      "loss on batch = 171.7098643607633\n",
      "loss on batch = 184.17478322805167\n",
      "loss on batch = 178.8025986218141\n",
      "loss on batch = 177.79171054196203\n",
      "loss on batch = 182.64607682425998\n",
      "loss on batch = 180.9283200146267\n",
      "loss on batch = 181.45043639558116\n",
      "loss on batch = 179.7900598604958\n",
      "loss on batch = 180.1127170060667\n",
      "loss on batch = 184.23036180957894\n",
      "loss on batch = 182.99877789404331\n",
      "loss on batch = 178.66477553912236\n",
      "loss on batch = 171.29283892865888\n",
      "loss on batch = 183.60473816807792\n",
      "loss on batch = 181.89135873621711\n",
      "loss on batch = 185.7427141174163\n",
      "loss on batch = 169.65636616375872\n",
      "loss on batch = 175.77305735955872\n",
      "loss on batch = 186.02788409105474\n",
      "loss on batch = 185.465258352913\n",
      "loss on batch = 176.56008412279743\n",
      "loss on batch = 188.46011523241734\n",
      "loss on batch = 181.7182390613043\n",
      "loss on batch = 174.80354179893607\n",
      "loss on batch = 179.43796529225736\n",
      "loss on batch = 178.52306903621525\n",
      "loss on batch = 182.35413511205087\n",
      "loss on batch = 178.17229958221702\n",
      "loss on batch = 181.67524871986404\n",
      "loss on batch = 171.20079406846895\n",
      "loss on batch = 174.3445733415852\n",
      "loss on batch = 183.25703570292745\n",
      "loss on batch = 190.2713719279928\n",
      "loss on batch = 179.16831256979006\n",
      "loss on batch = 184.26830553393586\n",
      "loss on batch = 183.69796538315634\n",
      "loss on batch = 176.61653369106816\n",
      "loss on batch = 173.5663954692053\n",
      "loss on batch = 187.2114354513967\n",
      "loss on batch = 179.63151717892853\n",
      "loss on batch = 176.25157046030733\n",
      "loss on batch = 176.19571241308392\n",
      "loss on batch = 178.6777328249788\n",
      "loss on batch = 180.11881385773248\n",
      "loss on batch = 176.63231111845107\n",
      "loss on batch = 183.99697884150254\n",
      "loss on batch = 179.66504664761598\n",
      "loss on batch = 181.71638936657746\n",
      "loss on batch = 166.13205794876563\n",
      "loss on batch = 177.2955659592651\n",
      "loss on batch = 171.86546713260213\n",
      "loss on batch = 175.07922365699847\n",
      "loss on batch = 175.60237751243994\n",
      "loss on batch = 174.8267633450087\n",
      "loss on batch = 183.8031046582103\n",
      "loss on batch = 189.64741430343395\n",
      "loss on batch = 170.721656841034\n",
      "loss on batch = 184.70035506133678\n",
      "loss on batch = 174.05702421586398\n",
      "loss on batch = 185.37780154168712\n",
      "loss on batch = 176.665490968448\n",
      "loss on batch = 182.0856746752558\n",
      "loss on batch = 177.85508419063297\n",
      "loss on batch = 176.3968571599691\n",
      "loss on batch = 177.83465385798044\n",
      "loss on batch = 179.40284034803835\n",
      "loss on batch = 174.63963026300578\n",
      "loss on batch = 190.24094023100193\n",
      "loss on batch = 167.44593321713694\n",
      "loss on batch = 172.79697186783767\n",
      "loss on batch = 180.7631883344163\n",
      "loss on batch = 189.62189565523215\n",
      "loss on batch = 168.98946783827444\n",
      "loss on batch = 188.6909826744713\n",
      "loss on batch = 176.6023734095321\n",
      "loss on batch = 178.96796615198502\n",
      "loss on batch = 180.2358890372426\n",
      "loss on batch = 184.34919961374206\n",
      "loss on batch = 178.4450282837886\n",
      "loss on batch = 184.42629031291472\n",
      "loss on batch = 176.13596150148052\n",
      "loss on batch = 177.0692771272452\n",
      "loss on batch = 174.2487313282244\n",
      "loss on batch = 183.51476722289303\n",
      "loss on batch = 175.8006948679543\n",
      "loss on batch = 175.96684279440834\n",
      "loss on batch = 186.32036155262853\n",
      "loss on batch = 180.59729358172143\n",
      "loss on batch = 167.54217234937175\n",
      "loss on batch = 176.886490492873\n",
      "loss on batch = 184.24190334320798\n",
      "loss on batch = 173.24688047540565\n",
      "loss on batch = 184.72277836255427\n",
      "loss on batch = 185.49178169539596\n",
      "loss on batch = 178.70232120275938\n",
      "loss on batch = 186.97219631899594\n",
      "loss on batch = 178.34900159452127\n",
      "loss on batch = 184.31020034246797\n",
      "loss on batch = 187.49315050195395\n",
      "loss on batch = 174.32988781587625\n",
      "loss on batch = 185.18449433854443\n",
      "loss on batch = 176.49477547509193\n",
      "loss on batch = 176.2799227534665\n",
      "loss on batch = 174.6989915160965\n",
      "loss on batch = 177.45655350455186\n",
      "loss on batch = 166.93151559095463\n",
      "loss on batch = 184.54952326301543\n",
      "loss on batch = 180.61003198614418\n",
      "loss on batch = 172.94298597888718\n",
      "loss on batch = 181.68865188777525\n",
      "loss on batch = 183.4342410345992\n",
      "loss on batch = 171.10847081989854\n",
      "loss on batch = 174.8303233042742\n",
      "loss on batch = 181.69481972698046\n",
      "loss on batch = 172.55484537242148\n",
      "loss on batch = 172.0812516171133\n",
      "loss on batch = 163.96357727303518\n",
      "loss on batch = 187.3942034517466\n",
      "loss on batch = 165.00621484662742\n",
      "loss on batch = 193.62150865931054\n",
      "loss on batch = 181.02273591377997\n",
      "loss on batch = 184.17907819154644\n",
      "loss on batch = 161.721959705901\n",
      "loss on batch = 183.1327058843583\n",
      "loss on batch = 176.39608237158484\n",
      "loss on batch = 184.46899620778433\n",
      "loss on batch = 170.6725478439194\n",
      "loss on batch = 174.57544604660285\n",
      "loss on batch = 177.10719887156776\n",
      "loss on batch = 174.87488683262058\n",
      "loss on batch = 172.77916727462187\n",
      "loss on batch = 175.5535014930163\n",
      "loss on batch = 171.15094176833247\n",
      "loss on batch = 170.31810249477326\n",
      "loss on batch = 187.4578040957365\n",
      "loss on batch = 176.8141775113376\n",
      "loss on batch = 177.36377842347716\n",
      "loss on batch = 186.302215231747\n",
      "loss on batch = 161.680590652594\n",
      "loss on batch = 184.318034727647\n",
      "loss on batch = 177.42615088959542\n",
      "loss on batch = 177.4233587797725\n",
      "loss on batch = 172.21249268521117\n",
      "loss on batch = 177.90397437818388\n",
      "loss on batch = 179.46571705740402\n",
      "loss on batch = 175.81368328658556\n",
      "loss on batch = 166.99633897076927\n",
      "loss on batch = 185.33766432569962\n",
      "loss on batch = 170.72787151188476\n",
      "loss on batch = 178.80262372416956\n",
      "loss on batch = 176.63388357828552\n",
      "loss on batch = 175.41734458381825\n",
      "loss on batch = 175.2170063317798\n",
      "loss on batch = 185.99777403323287\n",
      "loss on batch = 174.4238721056469\n",
      "loss on batch = 180.76453431126595\n",
      "loss on batch = 179.661334154749\n",
      "loss on batch = 185.8171391486187\n",
      "loss on batch = 174.52994018949275\n",
      "loss on batch = 182.45058321208774\n",
      "loss on batch = 170.8517852132714\n",
      "loss on batch = 172.64835968259737\n",
      "loss on batch = 170.98416976617247\n",
      "loss on batch = 178.65297553653417\n",
      "loss on batch = 171.67922346101807\n",
      "loss on batch = 167.23143593740537\n",
      "loss on batch = 183.79228898296947\n",
      "loss on batch = 184.41699276083884\n",
      "loss on batch = 172.0777675002685\n",
      "loss on batch = 181.6479805424387\n",
      "loss on batch = 163.87175571977866\n",
      "loss on batch = 185.34858415202712\n",
      "loss on batch = 166.4472555919233\n",
      "loss on batch = 179.2372304962724\n",
      "loss on batch = 179.95682493881122\n",
      "loss on batch = 170.9299097619963\n",
      "loss on batch = 178.54170465495952\n",
      "loss on batch = 180.11587940610565\n",
      "loss on batch = 176.5366821893831\n",
      "loss on batch = 176.0172376874493\n",
      "loss on batch = 184.4655034028361\n",
      "loss on batch = 167.71001234571168\n",
      "loss on batch = 175.35974464174345\n",
      "loss on batch = 178.28438352230734\n",
      "loss on batch = 179.56757534825152\n",
      "loss on batch = 182.51980415994302\n",
      "loss on batch = 174.41835474221622\n",
      "loss on batch = 165.95943194388033\n",
      "loss on batch = 173.0645296259749\n",
      "Acc: 0.7815\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=True)\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(196)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(20)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=2, batch_size=100, lr=.02, method='rmsprop', method_param=0.2)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ch = 77.84789612770992\n",
      "loss on batch = 85.15512132948348\n",
      "loss on batch = 79.5461053503787\n",
      "loss on batch = 84.44010488988687\n",
      "loss on batch = 73.11118513679574\n",
      "loss on batch = 84.9029293981136\n",
      "loss on batch = 83.75567481204729\n",
      "loss on batch = 85.27028895811557\n",
      "loss on batch = 86.32930130999317\n",
      "loss on batch = 86.48169382563461\n",
      "loss on batch = 79.25475851962298\n",
      "loss on batch = 78.72806372309518\n",
      "loss on batch = 82.37341834449103\n",
      "loss on batch = 84.72991350860319\n",
      "loss on batch = 82.54928508454701\n",
      "loss on batch = 88.35843220383828\n",
      "loss on batch = 80.38815121525408\n",
      "loss on batch = 82.78033762183239\n",
      "loss on batch = 78.36408614562383\n",
      "loss on batch = 81.87636656554412\n",
      "loss on batch = 78.61283851857425\n",
      "loss on batch = 87.07248550933666\n",
      "loss on batch = 82.86050658694035\n",
      "loss on batch = 89.14518634175089\n",
      "loss on batch = 84.48703742482161\n",
      "loss on batch = 87.6674297735445\n",
      "loss on batch = 83.06205120426893\n",
      "loss on batch = 76.77330840474951\n",
      "loss on batch = 86.53711616590363\n",
      "loss on batch = 79.32698712457113\n",
      "loss on batch = 86.11423948599985\n",
      "loss on batch = 80.87865284993077\n",
      "loss on batch = 83.07729758724713\n",
      "loss on batch = 85.226607692081\n",
      "loss on batch = 83.2645445193706\n",
      "loss on batch = 85.585487095549\n",
      "loss on batch = 79.53893890093696\n",
      "loss on batch = 80.81952284242294\n",
      "loss on batch = 84.35911035944892\n",
      "loss on batch = 81.83821673669544\n",
      "loss on batch = 84.19383405680028\n",
      "loss on batch = 81.21996109361672\n",
      "loss on batch = 86.84252305394995\n",
      "loss on batch = 82.12388707801749\n",
      "loss on batch = 83.43311753566806\n",
      "loss on batch = 80.12347903741542\n",
      "loss on batch = 84.32422205707749\n",
      "loss on batch = 81.34069019443703\n",
      "loss on batch = 82.68792453075734\n",
      "loss on batch = 86.28618580878526\n",
      "loss on batch = 82.52710419792993\n",
      "loss on batch = 86.9002554551125\n",
      "loss on batch = 85.07819866411683\n",
      "loss on batch = 82.46722775667592\n",
      "loss on batch = 82.16920496140195\n",
      "loss on batch = 82.76744336135445\n",
      "loss on batch = 85.12026013910375\n",
      "loss on batch = 81.52821395101313\n",
      "loss on batch = 81.43753005896941\n",
      "loss on batch = 85.09014165866222\n",
      "loss on batch = 79.42813927117504\n",
      "loss on batch = 82.10430800794526\n",
      "loss on batch = 86.50238960987926\n",
      "loss on batch = 89.85851065052384\n",
      "loss on batch = 80.30387607716145\n",
      "loss on batch = 78.7974754573946\n",
      "loss on batch = 85.26401317443549\n",
      "loss on batch = 82.62390803907145\n",
      "loss on batch = 84.15927449346178\n",
      "loss on batch = 78.86519904612378\n",
      "loss on batch = 80.76531258578099\n",
      "loss on batch = 80.33834501945353\n",
      "loss on batch = 86.53841574012704\n",
      "loss on batch = 82.53626890763007\n",
      "loss on batch = 86.09629668599086\n",
      "loss on batch = 79.71480465749491\n",
      "loss on batch = 82.63542742723277\n",
      "loss on batch = 83.29895201915178\n",
      "loss on batch = 80.82359812438823\n",
      "loss on batch = 80.94793343965713\n",
      "loss on batch = 80.16786149014362\n",
      "loss on batch = 77.21714495637538\n",
      "loss on batch = 83.07031185196976\n",
      "loss on batch = 87.02738039696357\n",
      "loss on batch = 85.91777417994153\n",
      "loss on batch = 82.36242275289624\n",
      "loss on batch = 85.05458932338496\n",
      "loss on batch = 80.47295093532192\n",
      "loss on batch = 87.27369579350085\n",
      "loss on batch = 80.59358493084503\n",
      "loss on batch = 79.59913035830525\n",
      "loss on batch = 82.97773039628157\n",
      "loss on batch = 82.77046769421945\n",
      "loss on batch = 82.50382767842035\n",
      "loss on batch = 85.05006455222352\n",
      "loss on batch = 81.05714439120231\n",
      "loss on batch = 79.43534756907054\n",
      "loss on batch = 77.46782960520383\n",
      "loss on batch = 84.12392445462294\n",
      "loss on batch = 78.76373536600296\n",
      "loss on batch = 85.71911626295875\n",
      "loss on batch = 83.31018063011976\n",
      "loss on batch = 82.48602898083581\n",
      "loss on batch = 82.2702612538212\n",
      "loss on batch = 83.37472945862295\n",
      "loss on batch = 78.83598533699723\n",
      "loss on batch = 78.03946497612779\n",
      "loss on batch = 81.70601983445098\n",
      "loss on batch = 83.25092957626191\n",
      "loss on batch = 80.63797676830157\n",
      "loss on batch = 76.88429244517786\n",
      "loss on batch = 81.5367137751047\n",
      "loss on batch = 79.90759763743766\n",
      "loss on batch = 83.55783343189314\n",
      "loss on batch = 83.94665441914992\n",
      "loss on batch = 77.1922249143011\n",
      "loss on batch = 78.56660176105282\n",
      "loss on batch = 82.34720544530603\n",
      "loss on batch = 82.61548024009261\n",
      "loss on batch = 76.34616293070708\n",
      "loss on batch = 78.76312366001358\n",
      "loss on batch = 84.18213046661288\n",
      "loss on batch = 82.29144935548943\n",
      "loss on batch = 76.07911673161885\n",
      "loss on batch = 85.62586170676892\n",
      "loss on batch = 81.48768517631422\n",
      "loss on batch = 75.42581549294626\n",
      "loss on batch = 83.48838831594814\n",
      "loss on batch = 79.51250624354593\n",
      "loss on batch = 82.23177213174505\n",
      "loss on batch = 77.54002493625416\n",
      "loss on batch = 86.39479834214022\n",
      "loss on batch = 84.41573464718417\n",
      "loss on batch = 86.45979157993978\n",
      "loss on batch = 81.1897590567035\n",
      "loss on batch = 80.86434434349856\n",
      "loss on batch = 83.89074785618305\n",
      "loss on batch = 86.37498132993389\n",
      "loss on batch = 78.53369033718754\n",
      "loss on batch = 84.30073087255097\n",
      "loss on batch = 78.38919941687273\n",
      "loss on batch = 86.12293062936749\n",
      "loss on batch = 81.4207721229761\n",
      "loss on batch = 83.02309387005226\n",
      "loss on batch = 82.7165788461219\n",
      "loss on batch = 82.14203260384664\n",
      "loss on batch = 82.4737568935777\n",
      "loss on batch = 81.60648526474381\n",
      "loss on batch = 78.17094260844495\n",
      "loss on batch = 86.28697068690752\n",
      "loss on batch = 80.7839326357371\n",
      "loss on batch = 82.12525510244163\n",
      "loss on batch = 83.68003546914632\n",
      "loss on batch = 85.48990092703467\n",
      "loss on batch = 84.26942174437825\n",
      "loss on batch = 77.88700062055543\n",
      "loss on batch = 82.84256944255814\n",
      "loss on batch = 81.33040732691393\n",
      "loss on batch = 76.13871554719711\n",
      "loss on batch = 83.2530872535175\n",
      "loss on batch = 83.19359471611185\n",
      "loss on batch = 82.71914394933273\n",
      "loss on batch = 75.1696395434983\n",
      "loss on batch = 86.90063157661001\n",
      "loss on batch = 77.72238368771288\n",
      "loss on batch = 79.47221983354643\n",
      "loss on batch = 79.35813760290533\n",
      "loss on batch = 82.65374917797851\n",
      "loss on batch = 79.97305132628438\n",
      "loss on batch = 89.16307149216931\n",
      "loss on batch = 80.949847216547\n",
      "loss on batch = 84.23313576463208\n",
      "loss on batch = 81.75785989337035\n",
      "loss on batch = 87.44154662378342\n",
      "loss on batch = 79.47326341374838\n",
      "loss on batch = 82.17327126189029\n",
      "loss on batch = 83.72407889245692\n",
      "loss on batch = 87.40469216798314\n",
      "loss on batch = 86.77314619640993\n",
      "loss on batch = 81.37536808445353\n",
      "loss on batch = 73.93207612253255\n",
      "loss on batch = 80.78554875858418\n",
      "loss on batch = 80.68645838502565\n",
      "loss on batch = 83.6222608823227\n",
      "loss on batch = 78.35574432443507\n",
      "loss on batch = 79.4626256677989\n",
      "loss on batch = 79.3476330789124\n",
      "loss on batch = 77.17275526474651\n",
      "loss on batch = 82.21287190101883\n",
      "loss on batch = 83.97624713967612\n",
      "loss on batch = 75.21359836768718\n",
      "loss on batch = 78.54638515948807\n",
      "loss on batch = 73.86527990908212\n",
      "loss on batch = 79.21595184241284\n",
      "loss on batch = 79.36909713812227\n",
      "loss on batch = 78.25383918326389\n",
      "loss on batch = 79.40063035749033\n",
      "loss on batch = 82.82067256429345\n",
      "loss on batch = 82.53056650466839\n",
      "loss on batch = 82.43462254567183\n",
      "loss on batch = 81.51484964385413\n",
      "loss on batch = 79.5439502221007\n",
      "loss on batch = 80.46803956061612\n",
      "loss on batch = 84.13137305338705\n",
      "loss on batch = 81.6466024086238\n",
      "loss on batch = 81.24992273327524\n",
      "loss on batch = 76.07663801902505\n",
      "loss on batch = 84.72451320010649\n",
      "loss on batch = 75.69654018721441\n",
      "loss on batch = 80.38161718070216\n",
      "loss on batch = 73.0801934200295\n",
      "loss on batch = 82.95444365689673\n",
      "loss on batch = 85.60447537646198\n",
      "loss on batch = 83.6679803560649\n",
      "loss on batch = 80.22448368712108\n",
      "loss on batch = 79.96351415112233\n",
      "loss on batch = 84.03452955575139\n",
      "loss on batch = 81.22852244079805\n",
      "loss on batch = 82.45119400061716\n",
      "loss on batch = 79.34266494504624\n",
      "loss on batch = 81.13439976083747\n",
      "loss on batch = 81.60192236459892\n",
      "loss on batch = 80.2631955213676\n",
      "loss on batch = 77.33095352154518\n",
      "loss on batch = 86.9126021015739\n",
      "loss on batch = 84.8409917519375\n",
      "loss on batch = 82.43670680286391\n",
      "loss on batch = 83.71578918400803\n",
      "loss on batch = 84.9216138321947\n",
      "loss on batch = 78.83831951535193\n",
      "loss on batch = 78.06767290166775\n",
      "loss on batch = 82.8525767447104\n",
      "loss on batch = 82.09801457699426\n",
      "loss on batch = 80.25332924964499\n",
      "loss on batch = 80.52538328171403\n",
      "loss on batch = 86.62992835308245\n",
      "loss on batch = 82.78314192437706\n",
      "loss on batch = 79.23395485859209\n",
      "loss on batch = 79.45832761734\n",
      "loss on batch = 78.89515158205126\n",
      "loss on batch = 80.40342435102237\n",
      "loss on batch = 84.16368983421522\n",
      "loss on batch = 80.28300850588752\n",
      "loss on batch = 82.28167987303523\n",
      "loss on batch = 81.03791630103689\n",
      "loss on batch = 79.21318256431604\n",
      "loss on batch = 85.11188932606308\n",
      "loss on batch = 83.809666117307\n",
      "loss on batch = 83.11225074553994\n",
      "loss on batch = 78.80446079700494\n",
      "loss on batch = 79.19855820196791\n",
      "loss on batch = 81.15506636543566\n",
      "loss on batch = 81.48483845898636\n",
      "loss on batch = 85.52048713254291\n",
      "loss on batch = 83.4240686436753\n",
      "loss on batch = 80.7692472610079\n",
      "loss on batch = 80.33627191557115\n",
      "loss on batch = 82.70539524578612\n",
      "loss on batch = 82.07406133332941\n",
      "loss on batch = 80.51667297985819\n",
      "loss on batch = 83.2378438445424\n",
      "loss on batch = 79.81277958641152\n",
      "loss on batch = 84.12308800263881\n",
      "loss on batch = 76.4358155135961\n",
      "loss on batch = 83.2697250843779\n",
      "loss on batch = 84.48142654988546\n",
      "loss on batch = 78.4838031717487\n",
      "loss on batch = 85.37102945185826\n",
      "loss on batch = 86.63113956636504\n",
      "loss on batch = 79.76083823128351\n",
      "loss on batch = 76.74161703059812\n",
      "loss on batch = 78.79616683483592\n",
      "loss on batch = 79.67742709557488\n",
      "loss on batch = 80.04674701623145\n",
      "loss on batch = 87.04973984939558\n",
      "loss on batch = 83.42658679399904\n",
      "loss on batch = 76.1581185265596\n",
      "loss on batch = 82.42704757127169\n",
      "loss on batch = 82.41649562788734\n",
      "loss on batch = 78.40146666583587\n",
      "loss on batch = 79.75276053656893\n",
      "loss on batch = 79.25382357961757\n",
      "loss on batch = 84.06641099337986\n",
      "loss on batch = 82.42054745176844\n",
      "loss on batch = 77.98500343239019\n",
      "loss on batch = 82.82871619846338\n",
      "loss on batch = 81.06027594676559\n",
      "loss on batch = 83.96291646123753\n",
      "loss on batch = 79.0615075783463\n",
      "loss on batch = 80.89638174948811\n",
      "loss on batch = 79.33142207394202\n",
      "loss on batch = 80.14529866922484\n",
      "loss on batch = 75.32444659503543\n",
      "loss on batch = 83.73531418244016\n",
      "loss on batch = 83.12884512025907\n",
      "loss on batch = 78.53491858592939\n",
      "loss on batch = 83.2558990769947\n",
      "loss on batch = 84.79380778958813\n",
      "loss on batch = 82.81077069606826\n",
      "loss on batch = 83.36386921530601\n",
      "loss on batch = 82.002147009002\n",
      "loss on batch = 80.86090144830098\n",
      "loss on batch = 81.07669420991152\n",
      "loss on batch = 79.14462531817512\n",
      "loss on batch = 74.29240934885192\n",
      "loss on batch = 84.2056520692567\n",
      "loss on batch = 79.86317168944979\n",
      "loss on batch = 77.41322859587069\n",
      "loss on batch = 80.92032942202587\n",
      "loss on batch = 78.55691517079848\n",
      "loss on batch = 79.83397668238017\n",
      "loss on batch = 85.4982696972688\n",
      "loss on batch = 83.57739377600569\n",
      "loss on batch = 79.91781390059441\n",
      "loss on batch = 81.20352243716832\n",
      "loss on batch = 84.97220963810767\n",
      "loss on batch = 82.45425016802693\n",
      "loss on batch = 81.16675995564344\n",
      "loss on batch = 77.33121326752189\n",
      "loss on batch = 82.69132326286874\n",
      "loss on batch = 74.71116499435533\n",
      "loss on batch = 74.98818014010968\n",
      "loss on batch = 85.27371303760901\n",
      "loss on batch = 78.1952562983983\n",
      "loss on batch = 82.32758603245118\n",
      "loss on batch = 80.82637934048702\n",
      "loss on batch = 83.94919520935106\n",
      "loss on batch = 78.70705689841151\n",
      "loss on batch = 79.45109839036287\n",
      "loss on batch = 78.3118114175237\n",
      "loss on batch = 76.17058409975917\n",
      "loss on batch = 72.03402612226878\n",
      "loss on batch = 83.10942317032847\n",
      "loss on batch = 81.69846642744498\n",
      "loss on batch = 77.53261866922941\n",
      "loss on batch = 80.34968164306977\n",
      "loss on batch = 80.17369335316849\n",
      "loss on batch = 79.8954147832498\n",
      "loss on batch = 90.71303240871126\n",
      "loss on batch = 79.78474505940562\n",
      "loss on batch = 84.3065464336864\n",
      "loss on batch = 84.13711352162915\n",
      "loss on batch = 81.83727285675681\n",
      "loss on batch = 80.9256930848187\n",
      "loss on batch = 82.55803841659326\n",
      "loss on batch = 80.10637216832616\n",
      "loss on batch = 81.2524677917009\n",
      "loss on batch = 80.51148960711396\n",
      "loss on batch = 80.1427506959663\n",
      "loss on batch = 78.67459709039385\n",
      "loss on batch = 81.0100797902368\n",
      "loss on batch = 81.61240815627923\n",
      "loss on batch = 79.60199632895139\n",
      "loss on batch = 79.49116643572663\n",
      "loss on batch = 79.29887931598864\n",
      "loss on batch = 82.38343733250989\n",
      "loss on batch = 87.8592011506162\n",
      "loss on batch = 83.36703787304901\n",
      "loss on batch = 86.86728081768709\n",
      "loss on batch = 80.6130856834253\n",
      "loss on batch = 83.70978377669365\n",
      "loss on batch = 83.83441208078268\n",
      "loss on batch = 83.82491559772687\n",
      "loss on batch = 82.68831049632986\n",
      "loss on batch = 82.20328001324398\n",
      "loss on batch = 82.74115173994346\n",
      "loss on batch = 82.75807937031844\n",
      "loss on batch = 76.69770188144452\n",
      "loss on batch = 80.74675856200477\n",
      "loss on batch = 83.30038102961772\n",
      "loss on batch = 80.5369808321984\n",
      "loss on batch = 75.20750834315146\n",
      "loss on batch = 81.69077909629891\n",
      "loss on batch = 76.53419589071368\n",
      "loss on batch = 79.5005393812314\n",
      "loss on batch = 83.55310707571866\n",
      "loss on batch = 82.05776172125589\n",
      "loss on batch = 81.39688295345965\n",
      "loss on batch = 82.4450302503765\n",
      "loss on batch = 81.26249113446187\n",
      "loss on batch = 82.33931717161718\n",
      "loss on batch = 82.14276251079919\n",
      "loss on batch = 79.18446742496616\n",
      "loss on batch = 78.66897912694236\n",
      "loss on batch = 77.4021596609133\n",
      "loss on batch = 81.03873378632105\n",
      "loss on batch = 83.1609958311036\n",
      "loss on batch = 80.47637935334434\n",
      "loss on batch = 81.28703284487919\n",
      "loss on batch = 78.645285271294\n",
      "loss on batch = 73.65687700672663\n",
      "loss on batch = 81.37487855457974\n",
      "loss on batch = 77.30869846491645\n",
      "loss on batch = 83.01875645379238\n",
      "loss on batch = 79.5780888931273\n",
      "loss on batch = 79.89514020713828\n",
      "loss on batch = 80.11513438078917\n",
      "loss on batch = 81.48676437563573\n",
      "loss on batch = 84.62605181675924\n",
      "loss on batch = 81.26841689172375\n",
      "loss on batch = 75.75635234912826\n",
      "loss on batch = 78.71594187540356\n",
      "loss on batch = 82.21886726099548\n",
      "loss on batch = 76.67202418049536\n",
      "loss on batch = 85.54687833987455\n",
      "loss on batch = 80.30095583142983\n",
      "loss on batch = 80.33511790568468\n",
      "loss on batch = 79.21444305484616\n",
      "loss on batch = 85.34430191359138\n",
      "loss on batch = 75.91232556265764\n",
      "loss on batch = 78.3247058748106\n",
      "loss on batch = 82.65793647181766\n",
      "loss on batch = 81.03593347642351\n",
      "loss on batch = 85.70960177963482\n",
      "loss on batch = 81.26050497321106\n",
      "loss on batch = 81.43539759375591\n",
      "loss on batch = 74.39960986913982\n",
      "loss on batch = 81.68500867049315\n",
      "loss on batch = 80.26492896944583\n",
      "loss on batch = 79.58000674823657\n",
      "loss on batch = 77.60407357241976\n",
      "loss on batch = 86.25672254628616\n",
      "loss on batch = 78.94168040160815\n",
      "loss on batch = 80.54861710015317\n",
      "loss on batch = 86.7218874705977\n",
      "loss on batch = 80.85325181937321\n",
      "loss on batch = 72.28139967907569\n",
      "loss on batch = 80.6944543471239\n",
      "loss on batch = 81.75073205587083\n",
      "loss on batch = 76.05420347958596\n",
      "loss on batch = 84.01311737484639\n",
      "loss on batch = 84.57501439214352\n",
      "loss on batch = 79.65839335868228\n",
      "loss on batch = 78.70421070961699\n",
      "loss on batch = 78.5697726060144\n",
      "loss on batch = 77.41072856025369\n",
      "loss on batch = 80.47059275008843\n",
      "loss on batch = 80.54877055562018\n",
      "loss on batch = 84.73783989628492\n",
      "loss on batch = 80.32557109703475\n",
      "loss on batch = 85.09022220609833\n",
      "loss on batch = 80.49465525597694\n",
      "loss on batch = 76.63389903534205\n",
      "loss on batch = 82.4067204010419\n",
      "loss on batch = 80.45254827042714\n",
      "loss on batch = 87.14715953154092\n",
      "loss on batch = 73.72923083464619\n",
      "loss on batch = 81.24303642231563\n",
      "loss on batch = 82.05590886734556\n",
      "loss on batch = 79.43357540377752\n",
      "loss on batch = 79.73422150119333\n",
      "loss on batch = 82.96273504103164\n",
      "loss on batch = 83.49711356949419\n",
      "loss on batch = 78.21504405345252\n",
      "loss on batch = 85.04166486849762\n",
      "loss on batch = 79.03012101809232\n",
      "loss on batch = 75.43831597738028\n",
      "loss on batch = 79.72500270851751\n",
      "loss on batch = 76.85752895023965\n",
      "loss on batch = 76.58812449662226\n",
      "loss on batch = 77.92250934870756\n",
      "loss on batch = 83.77633916744531\n",
      "loss on batch = 80.53356797702034\n",
      "loss on batch = 82.49151818914325\n",
      "loss on batch = 81.71927146616903\n",
      "loss on batch = 84.78894591193733\n",
      "loss on batch = 79.65071529101523\n",
      "loss on batch = 80.74001294185966\n",
      "loss on batch = 76.49292744818113\n",
      "loss on batch = 81.88612852488806\n",
      "loss on batch = 78.2948993133399\n",
      "loss on batch = 80.14460267508554\n",
      "loss on batch = 78.97960713510389\n",
      "loss on batch = 84.91683681990884\n",
      "loss on batch = 82.02288812544802\n",
      "loss on batch = 85.00629164060899\n",
      "loss on batch = 81.65819751000078\n",
      "loss on batch = 80.86851901367524\n",
      "loss on batch = 81.70768110436293\n",
      "loss on batch = 81.45461976652325\n",
      "loss on batch = 79.10708685048384\n",
      "loss on batch = 85.54235984189785\n",
      "loss on batch = 78.36909389527924\n",
      "loss on batch = 80.54590552517237\n",
      "loss on batch = 72.57981784950357\n",
      "loss on batch = 79.0822887690284\n",
      "loss on batch = 80.05555276714418\n",
      "loss on batch = 78.2210601350011\n",
      "loss on batch = 79.81021827466701\n",
      "loss on batch = 79.536703324686\n",
      "loss on batch = 77.92230954990104\n",
      "loss on batch = 82.16622072080861\n",
      "loss on batch = 79.23777945291023\n",
      "loss on batch = 84.8092237891459\n",
      "loss on batch = 76.86671860092454\n",
      "loss on batch = 82.43827125317621\n",
      "loss on batch = 76.11966403609864\n",
      "loss on batch = 77.0838161737311\n",
      "loss on batch = 77.55739280078078\n",
      "loss on batch = 79.07206241935032\n",
      "loss on batch = 74.00976531068821\n",
      "loss on batch = 83.94316703765273\n",
      "loss on batch = 81.86676709103783\n",
      "loss on batch = 80.94278998380545\n",
      "loss on batch = 79.91763824557135\n",
      "loss on batch = 84.18759848700171\n",
      "loss on batch = 74.26189951713329\n",
      "loss on batch = 83.05322711510686\n",
      "loss on batch = 81.5507033466942\n",
      "loss on batch = 75.11187637474906\n",
      "loss on batch = 82.44114374522664\n",
      "loss on batch = 77.10084372415261\n",
      "loss on batch = 81.60379068941467\n",
      "loss on batch = 78.32706489726836\n",
      "loss on batch = 81.08845593994236\n",
      "loss on batch = 84.12707384880477\n",
      "loss on batch = 80.70806443085618\n",
      "loss on batch = 84.60275303843997\n",
      "loss on batch = 74.79642423857135\n",
      "loss on batch = 79.9899661317352\n",
      "loss on batch = 78.61361583334146\n",
      "loss on batch = 82.8802723361828\n",
      "loss on batch = 78.67722289929885\n",
      "loss on batch = 78.79902847607396\n",
      "loss on batch = 80.71609925299403\n",
      "loss on batch = 83.44657199758309\n",
      "loss on batch = 84.70018359396025\n",
      "loss on batch = 83.71111319813564\n",
      "loss on batch = 80.12477752775663\n",
      "loss on batch = 78.42645012261723\n",
      "loss on batch = 82.89040093298958\n",
      "loss on batch = 79.13282824751772\n",
      "loss on batch = 81.37859677494289\n",
      "loss on batch = 80.7194952397905\n",
      "loss on batch = 84.64310266905575\n",
      "loss on batch = 78.43837397997439\n",
      "loss on batch = 78.15978322411385\n",
      "loss on batch = 83.309353987369\n",
      "loss on batch = 80.40757033565413\n",
      "loss on batch = 80.06955314629579\n",
      "loss on batch = 79.19910725363206\n",
      "loss on batch = 85.56606498604378\n",
      "loss on batch = 77.6516632544512\n",
      "loss on batch = 83.74066987364267\n",
      "loss on batch = 78.88339730865968\n",
      "loss on batch = 80.22960200871046\n",
      "loss on batch = 81.78677953430085\n",
      "loss on batch = 85.55963939400478\n",
      "loss on batch = 79.02151924501483\n",
      "loss on batch = 88.47561252684686\n",
      "loss on batch = 81.63817756220192\n",
      "loss on batch = 82.7311719952323\n",
      "loss on batch = 80.46861236480294\n",
      "loss on batch = 77.08255830077988\n",
      "loss on batch = 77.28469733326594\n",
      "loss on batch = 82.55440939642543\n",
      "loss on batch = 76.43613005265941\n",
      "loss on batch = 80.3530635005017\n",
      "loss on batch = 75.47140818110364\n",
      "loss on batch = 82.19135801081306\n",
      "loss on batch = 80.91059736954728\n",
      "loss on batch = 80.49121511243186\n",
      "loss on batch = 76.71662592711233\n",
      "loss on batch = 85.58760441527019\n",
      "loss on batch = 77.12130474062464\n",
      "loss on batch = 86.00157317066991\n",
      "loss on batch = 80.65761185006176\n",
      "loss on batch = 77.43359590631528\n",
      "loss on batch = 80.78815672023522\n",
      "loss on batch = 80.99768914257119\n",
      "loss on batch = 77.24216447239206\n",
      "loss on batch = 76.53486275979174\n",
      "loss on batch = 80.22741035714017\n",
      "loss on batch = 85.78438217471052\n",
      "loss on batch = 78.4837197002044\n",
      "loss on batch = 79.8185941684246\n",
      "loss on batch = 77.53358286819773\n",
      "loss on batch = 84.08406575829737\n",
      "loss on batch = 82.4107949516839\n",
      "loss on batch = 79.97485227349377\n",
      "loss on batch = 82.61695660136726\n",
      "loss on batch = 83.8106711980158\n",
      "loss on batch = 77.31245548889738\n",
      "loss on batch = 85.24512900510115\n",
      "loss on batch = 79.43648868560541\n",
      "loss on batch = 84.27087296329537\n",
      "loss on batch = 81.06060032484908\n",
      "loss on batch = 81.51126244817928\n",
      "loss on batch = 83.87899589009888\n",
      "loss on batch = 78.11962082555772\n",
      "loss on batch = 80.45305991949644\n",
      "Acc: 0.8315\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=True)\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(500)\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(50)\n",
    "\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=1, batch_size=50, lr=.01, method='rmsprop', method_param=0.2)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "on batch = 15.927791839487483\n",
      "loss on batch = 16.518367210535132\n",
      "loss on batch = 15.92805057863\n",
      "loss on batch = 15.771888976059909\n",
      "loss on batch = 15.191375037509642\n",
      "loss on batch = 16.43810285442617\n",
      "loss on batch = 17.337090109098234\n",
      "loss on batch = 17.64978441920854\n",
      "loss on batch = 16.02431761379116\n",
      "loss on batch = 17.608416434572526\n",
      "loss on batch = 16.906405730039655\n",
      "loss on batch = 16.33307491925413\n",
      "loss on batch = 14.84077839430768\n",
      "loss on batch = 15.87565655756176\n",
      "loss on batch = 16.697541520124528\n",
      "loss on batch = 15.827898317830288\n",
      "loss on batch = 17.47201614073365\n",
      "loss on batch = 17.244222963473188\n",
      "loss on batch = 14.607476010669895\n",
      "loss on batch = 15.837024141124552\n",
      "loss on batch = 16.447216050227205\n",
      "loss on batch = 16.43092577045047\n",
      "loss on batch = 16.18602329968121\n",
      "loss on batch = 16.4085418786207\n",
      "loss on batch = 15.369725974527006\n",
      "loss on batch = 15.25897922174087\n",
      "loss on batch = 17.53232036356222\n",
      "loss on batch = 16.775219641551658\n",
      "loss on batch = 17.397703456370916\n",
      "loss on batch = 15.822631966489023\n",
      "loss on batch = 17.947550301055696\n",
      "loss on batch = 15.843350609754633\n",
      "loss on batch = 16.11413994461498\n",
      "loss on batch = 16.52284617193029\n",
      "loss on batch = 16.860664586632023\n",
      "loss on batch = 18.13303837711951\n",
      "loss on batch = 15.667110341591572\n",
      "loss on batch = 17.746461239213904\n",
      "loss on batch = 15.338864371831122\n",
      "loss on batch = 16.089960254827652\n",
      "loss on batch = 16.59458234307988\n",
      "loss on batch = 16.19353203237341\n",
      "loss on batch = 14.736654392741096\n",
      "loss on batch = 15.533699376366481\n",
      "loss on batch = 16.19020888756788\n",
      "loss on batch = 15.672857508141185\n",
      "loss on batch = 16.7912171175679\n",
      "loss on batch = 15.988729345242467\n",
      "loss on batch = 17.110246845975308\n",
      "loss on batch = 15.223793169606681\n",
      "loss on batch = 15.866489467137585\n",
      "loss on batch = 17.146920168459896\n",
      "loss on batch = 17.383069995078728\n",
      "loss on batch = 15.671723983223043\n",
      "loss on batch = 17.183543342351435\n",
      "loss on batch = 15.999658121210583\n",
      "loss on batch = 15.219435340842459\n",
      "loss on batch = 17.458207507739544\n",
      "loss on batch = 15.977009521785114\n",
      "loss on batch = 16.054673477075603\n",
      "loss on batch = 16.36062406729648\n",
      "loss on batch = 15.438406536195961\n",
      "loss on batch = 15.251729447237308\n",
      "loss on batch = 15.109605365950168\n",
      "loss on batch = 16.9673070491233\n",
      "loss on batch = 17.123708587475253\n",
      "loss on batch = 16.333364912671968\n",
      "loss on batch = 15.989189845383432\n",
      "loss on batch = 16.644284671490148\n",
      "loss on batch = 16.23413612367513\n",
      "loss on batch = 17.47379502954697\n",
      "loss on batch = 16.463593076400333\n",
      "loss on batch = 14.285122889295804\n",
      "loss on batch = 15.24455562997004\n",
      "loss on batch = 15.804920629798247\n",
      "loss on batch = 16.926512792327365\n",
      "loss on batch = 15.621127730928041\n",
      "loss on batch = 16.983739711868246\n",
      "loss on batch = 14.796487936021325\n",
      "loss on batch = 14.766791550282058\n",
      "loss on batch = 17.709476344861677\n",
      "loss on batch = 18.011231718108714\n",
      "loss on batch = 17.02312106857543\n",
      "loss on batch = 15.870819177904561\n",
      "loss on batch = 14.891005311233936\n",
      "loss on batch = 15.31551676638405\n",
      "loss on batch = 17.79721453221307\n",
      "loss on batch = 15.543529978269572\n",
      "loss on batch = 17.348050473969487\n",
      "loss on batch = 15.641620164386204\n",
      "loss on batch = 14.399840438235328\n",
      "loss on batch = 18.152842092897515\n",
      "loss on batch = 15.981433933454497\n",
      "loss on batch = 14.614886386311458\n",
      "loss on batch = 16.794511252436816\n",
      "loss on batch = 15.889108618493797\n",
      "loss on batch = 16.418721917826506\n",
      "loss on batch = 15.523382107774044\n",
      "loss on batch = 16.156830769620314\n",
      "loss on batch = 15.454477686321344\n",
      "loss on batch = 17.093268476254767\n",
      "loss on batch = 16.18148079182458\n",
      "loss on batch = 16.933507230960615\n",
      "loss on batch = 16.680330783759743\n",
      "loss on batch = 15.179609613542489\n",
      "loss on batch = 17.273077273385258\n",
      "loss on batch = 16.211508397856505\n",
      "loss on batch = 16.9022955525609\n",
      "loss on batch = 15.20339225900958\n",
      "loss on batch = 15.07977032899013\n",
      "loss on batch = 15.953528647603813\n",
      "loss on batch = 17.502135967394665\n",
      "loss on batch = 16.045658572596363\n",
      "loss on batch = 16.684729623764323\n",
      "loss on batch = 16.999694186149835\n",
      "loss on batch = 15.68316426611531\n",
      "loss on batch = 15.902311787273419\n",
      "loss on batch = 15.123049174945066\n",
      "loss on batch = 16.91620240314844\n",
      "loss on batch = 16.929194457824316\n",
      "loss on batch = 16.601311027705844\n",
      "loss on batch = 14.911358980401006\n",
      "loss on batch = 17.07252530194577\n",
      "loss on batch = 17.1451043883607\n",
      "loss on batch = 17.513052476373268\n",
      "loss on batch = 16.84735417907477\n",
      "loss on batch = 16.380100607923303\n",
      "loss on batch = 16.844508164777338\n",
      "loss on batch = 16.626412656812406\n",
      "loss on batch = 16.570600173295897\n",
      "loss on batch = 16.07489956343301\n",
      "loss on batch = 16.30222535781044\n",
      "loss on batch = 16.46414680309151\n",
      "loss on batch = 16.32036121990881\n",
      "loss on batch = 16.184931999383313\n",
      "loss on batch = 15.23129304624311\n",
      "loss on batch = 14.9961120454467\n",
      "loss on batch = 15.23013616278363\n",
      "loss on batch = 15.277006662814411\n",
      "loss on batch = 15.783165334946727\n",
      "loss on batch = 15.357017346204433\n",
      "loss on batch = 16.67648943177017\n",
      "loss on batch = 17.280704530873642\n",
      "loss on batch = 15.235634644660777\n",
      "loss on batch = 14.59933242816738\n",
      "loss on batch = 17.131482613799566\n",
      "loss on batch = 19.148560098369305\n",
      "loss on batch = 17.498547022066354\n",
      "loss on batch = 16.881950127761364\n",
      "loss on batch = 17.05891436882056\n",
      "loss on batch = 15.358622768915287\n",
      "loss on batch = 17.72848329056493\n",
      "loss on batch = 18.063681752462507\n",
      "loss on batch = 14.408629036989996\n",
      "loss on batch = 17.74211552125646\n",
      "loss on batch = 15.21515283668223\n",
      "loss on batch = 17.19786497045598\n",
      "loss on batch = 15.39298344294368\n",
      "loss on batch = 16.00635974557869\n",
      "loss on batch = 16.27721254521949\n",
      "loss on batch = 16.422148190707606\n",
      "loss on batch = 18.48351363152086\n",
      "loss on batch = 16.268135676584997\n",
      "loss on batch = 15.145504850840485\n",
      "loss on batch = 16.50551623810068\n",
      "loss on batch = 16.158212694563755\n",
      "loss on batch = 16.67436221912621\n",
      "loss on batch = 16.401327779652718\n",
      "loss on batch = 15.634890443819026\n",
      "loss on batch = 15.954288804635555\n",
      "loss on batch = 16.677916682232798\n",
      "loss on batch = 16.113230283238817\n",
      "loss on batch = 16.09387614498326\n",
      "loss on batch = 16.323801530463466\n",
      "loss on batch = 16.773328607065682\n",
      "loss on batch = 16.871131689862594\n",
      "loss on batch = 16.26160519626175\n",
      "loss on batch = 16.373207083097977\n",
      "loss on batch = 15.44869483961204\n",
      "loss on batch = 16.4547729406919\n",
      "loss on batch = 15.79673370911345\n",
      "loss on batch = 16.389362232972935\n",
      "loss on batch = 15.852380625343454\n",
      "loss on batch = 15.129554552957863\n",
      "loss on batch = 16.822147106424836\n",
      "loss on batch = 19.098987362352776\n",
      "loss on batch = 16.45934495503756\n",
      "loss on batch = 16.172583696555794\n",
      "loss on batch = 16.682128165587475\n",
      "loss on batch = 15.135053079048813\n",
      "loss on batch = 14.881028336731633\n",
      "loss on batch = 13.77194776159358\n",
      "loss on batch = 14.851852675514156\n",
      "loss on batch = 16.795223717371872\n",
      "loss on batch = 16.218127923973363\n",
      "loss on batch = 15.901829981819223\n",
      "loss on batch = 17.270621045393167\n",
      "loss on batch = 16.166330866948154\n",
      "loss on batch = 17.100246906952883\n",
      "loss on batch = 16.658411356860434\n",
      "loss on batch = 16.772759591831935\n",
      "loss on batch = 17.4229415902206\n",
      "loss on batch = 17.478012141900898\n",
      "loss on batch = 16.382841528762086\n",
      "loss on batch = 15.58031257925429\n",
      "loss on batch = 17.108802550305548\n",
      "loss on batch = 17.496606648164676\n",
      "loss on batch = 17.39583119151534\n",
      "loss on batch = 15.890790595564582\n",
      "loss on batch = 17.31403019559037\n",
      "loss on batch = 15.992985013236474\n",
      "loss on batch = 15.780482723170428\n",
      "loss on batch = 14.756168653843782\n",
      "loss on batch = 15.852842211830929\n",
      "loss on batch = 15.729657313933519\n",
      "loss on batch = 16.369584776871655\n",
      "loss on batch = 15.946658679698771\n",
      "loss on batch = 16.673078118017862\n",
      "loss on batch = 18.480017837228935\n",
      "loss on batch = 16.0501907957808\n",
      "loss on batch = 17.5071350787209\n",
      "loss on batch = 16.09770852837821\n",
      "loss on batch = 17.231620714818998\n",
      "loss on batch = 16.280732768876078\n",
      "loss on batch = 17.392256394965745\n",
      "loss on batch = 16.79400461901966\n",
      "loss on batch = 17.304148844747694\n",
      "loss on batch = 16.07225619354864\n",
      "loss on batch = 17.358961641981395\n",
      "loss on batch = 17.425491734743975\n",
      "loss on batch = 16.337962191130877\n",
      "loss on batch = 16.225063499540802\n",
      "loss on batch = 16.320236988439287\n",
      "loss on batch = 15.308425329497272\n",
      "loss on batch = 15.916089824083077\n",
      "loss on batch = 16.015455076305763\n",
      "loss on batch = 16.59650870606182\n",
      "loss on batch = 15.588850309634234\n",
      "loss on batch = 15.032761465550607\n",
      "loss on batch = 16.710279468723005\n",
      "loss on batch = 14.843703570938843\n",
      "loss on batch = 16.887393245406187\n",
      "loss on batch = 18.004396368678727\n",
      "loss on batch = 16.321789585719348\n",
      "loss on batch = 16.775448045056535\n",
      "loss on batch = 16.071966315972215\n",
      "loss on batch = 16.221626379444757\n",
      "loss on batch = 17.623246171167693\n",
      "loss on batch = 14.374933157146916\n",
      "loss on batch = 17.276669873350816\n",
      "loss on batch = 15.2216775978054\n",
      "loss on batch = 16.387928315342716\n",
      "loss on batch = 14.147533459737966\n",
      "loss on batch = 16.72303821174519\n",
      "loss on batch = 17.725140994683272\n",
      "loss on batch = 16.612229326589453\n",
      "loss on batch = 16.87142597150433\n",
      "loss on batch = 16.913786774662796\n",
      "loss on batch = 15.355593955440956\n",
      "loss on batch = 16.64731582711377\n",
      "loss on batch = 17.225747437345035\n",
      "loss on batch = 17.260040092489962\n",
      "loss on batch = 19.546882000774495\n",
      "loss on batch = 15.168968398187989\n",
      "loss on batch = 15.568071807407307\n",
      "loss on batch = 16.0648825847992\n",
      "loss on batch = 16.53712227848868\n",
      "loss on batch = 16.410172717797195\n",
      "loss on batch = 14.518634754449177\n",
      "loss on batch = 14.881554856195656\n",
      "loss on batch = 14.2636873349579\n",
      "loss on batch = 16.642009800648953\n",
      "loss on batch = 15.127225628228299\n",
      "loss on batch = 17.60996006895437\n",
      "loss on batch = 17.758553987561527\n",
      "loss on batch = 15.299352163336069\n",
      "loss on batch = 17.389031373182892\n",
      "loss on batch = 18.26301988299125\n",
      "loss on batch = 16.76347603519395\n",
      "loss on batch = 17.10879545371526\n",
      "loss on batch = 15.522990679692967\n",
      "loss on batch = 15.402865114061875\n",
      "loss on batch = 15.304579854121414\n",
      "loss on batch = 16.295837752263598\n",
      "loss on batch = 15.88604064686978\n",
      "loss on batch = 15.870373306766716\n",
      "loss on batch = 15.227620555309361\n",
      "loss on batch = 14.945774240137508\n",
      "loss on batch = 15.659008029295492\n",
      "loss on batch = 16.175516168992097\n",
      "loss on batch = 16.068977492681135\n",
      "loss on batch = 16.48070980957889\n",
      "loss on batch = 14.733440187672725\n",
      "loss on batch = 18.085952929834118\n",
      "loss on batch = 16.211464190383424\n",
      "loss on batch = 18.430641287471023\n",
      "loss on batch = 16.874633187071183\n",
      "loss on batch = 18.023392774067638\n",
      "loss on batch = 16.016022620760033\n",
      "loss on batch = 16.577054038099128\n",
      "loss on batch = 16.367682034234726\n",
      "loss on batch = 15.273526607175702\n",
      "loss on batch = 15.25448228805573\n",
      "loss on batch = 13.835203928601809\n",
      "loss on batch = 14.423276094439979\n",
      "loss on batch = 15.518009438523091\n",
      "loss on batch = 15.582626836960456\n",
      "loss on batch = 16.69740473840232\n",
      "loss on batch = 16.79629999740429\n",
      "loss on batch = 16.311787211629266\n",
      "loss on batch = 17.378996831902203\n",
      "loss on batch = 16.420989806641487\n",
      "loss on batch = 16.107591482241766\n",
      "loss on batch = 16.335124375296886\n",
      "loss on batch = 14.894242091072144\n",
      "loss on batch = 15.473835974155435\n",
      "loss on batch = 17.122169572795848\n",
      "loss on batch = 17.23599560942265\n",
      "loss on batch = 15.42366804634404\n",
      "loss on batch = 14.918799833119971\n",
      "loss on batch = 14.303505213880328\n",
      "loss on batch = 15.460826569500126\n",
      "loss on batch = 15.064150113767688\n",
      "loss on batch = 16.519031242617864\n",
      "loss on batch = 17.581302366757402\n",
      "loss on batch = 17.338490023263255\n",
      "loss on batch = 15.912743837143767\n",
      "loss on batch = 14.020430785787381\n",
      "loss on batch = 14.558584209760772\n",
      "loss on batch = 15.510666796296807\n",
      "loss on batch = 15.02529935627975\n",
      "loss on batch = 15.711975759153114\n",
      "loss on batch = 15.189170297791213\n",
      "loss on batch = 14.587643442902852\n",
      "loss on batch = 15.246664708373672\n",
      "loss on batch = 16.24917186354157\n",
      "loss on batch = 16.87199941782279\n",
      "loss on batch = 19.272148811289\n",
      "loss on batch = 14.95162160047921\n",
      "loss on batch = 17.726281596191747\n",
      "loss on batch = 15.035823472410927\n",
      "loss on batch = 17.822564712169797\n",
      "loss on batch = 16.659046535540938\n",
      "loss on batch = 17.507880773898727\n",
      "loss on batch = 13.854458028970333\n",
      "loss on batch = 16.167454905674845\n",
      "loss on batch = 18.32764950659807\n",
      "loss on batch = 14.97012782824952\n",
      "loss on batch = 15.413363485754767\n",
      "loss on batch = 17.663738481002092\n",
      "loss on batch = 17.592519697857917\n",
      "loss on batch = 15.127777233747961\n",
      "loss on batch = 14.919384578814944\n",
      "loss on batch = 16.092181267688716\n",
      "loss on batch = 17.21691216907564\n",
      "loss on batch = 18.023571862855274\n",
      "loss on batch = 16.349037791009675\n",
      "loss on batch = 16.57111672255408\n",
      "loss on batch = 16.60488819142644\n",
      "loss on batch = 16.277597585757313\n",
      "loss on batch = 15.594807722900436\n",
      "loss on batch = 14.825894734767521\n",
      "loss on batch = 15.347430303157894\n",
      "loss on batch = 13.944818788717766\n",
      "loss on batch = 16.68623414263135\n",
      "loss on batch = 16.83680311713466\n",
      "loss on batch = 16.38153124580615\n",
      "loss on batch = 17.66509363002762\n",
      "loss on batch = 16.788882797926288\n",
      "loss on batch = 16.502527389901267\n",
      "loss on batch = 16.893195321410953\n",
      "loss on batch = 19.75865188590024\n",
      "loss on batch = 16.64618838199083\n",
      "loss on batch = 16.25035496766356\n",
      "loss on batch = 15.359661017963663\n",
      "loss on batch = 17.214713384157093\n",
      "loss on batch = 16.009411420154404\n",
      "loss on batch = 17.43943463251658\n",
      "loss on batch = 17.06745016823313\n",
      "loss on batch = 15.677238033910069\n",
      "loss on batch = 18.58861893933151\n",
      "loss on batch = 15.659946886599787\n",
      "loss on batch = 15.804546021918267\n",
      "loss on batch = 16.0947392432499\n",
      "loss on batch = 17.804436870433054\n",
      "loss on batch = 17.79683796452933\n",
      "loss on batch = 17.835048711206284\n",
      "loss on batch = 17.889419526347343\n",
      "loss on batch = 14.604059490673354\n",
      "loss on batch = 15.623569865256508\n",
      "loss on batch = 14.822906108083071\n",
      "loss on batch = 16.016200097270595\n",
      "loss on batch = 16.509412248019412\n",
      "loss on batch = 15.571511557648092\n",
      "loss on batch = 16.059710035577332\n",
      "loss on batch = 17.641514195193555\n",
      "loss on batch = 16.456644035480647\n",
      "loss on batch = 15.316219714403974\n",
      "loss on batch = 16.517284693346216\n",
      "loss on batch = 15.377595593246415\n",
      "loss on batch = 14.086397138878771\n",
      "loss on batch = 16.11727608870417\n",
      "loss on batch = 15.93589821410288\n",
      "loss on batch = 16.905732869123664\n",
      "loss on batch = 18.33771924658962\n",
      "loss on batch = 15.648001027920662\n",
      "loss on batch = 16.83726994974272\n",
      "loss on batch = 13.775337622294886\n",
      "loss on batch = 15.996117653875242\n",
      "loss on batch = 17.43414097526398\n",
      "loss on batch = 16.47127195089234\n",
      "loss on batch = 15.468835348251448\n",
      "loss on batch = 16.80043602839082\n",
      "loss on batch = 14.91690128809414\n",
      "loss on batch = 15.100947707143003\n",
      "loss on batch = 14.30242309767927\n",
      "loss on batch = 15.57511462313628\n",
      "loss on batch = 15.781681740189207\n",
      "loss on batch = 15.259618824113558\n",
      "loss on batch = 15.808107772834951\n",
      "loss on batch = 15.593290484568591\n",
      "loss on batch = 16.79411205740663\n",
      "loss on batch = 16.303704191517227\n",
      "loss on batch = 17.586058706645076\n",
      "loss on batch = 19.176033360545652\n",
      "loss on batch = 16.385384770369665\n",
      "loss on batch = 16.298339886249703\n",
      "loss on batch = 15.127754676527042\n",
      "loss on batch = 16.82443553294219\n",
      "loss on batch = 16.22738740136932\n",
      "loss on batch = 14.996041816033017\n",
      "loss on batch = 16.0892635236631\n",
      "loss on batch = 16.70512137582999\n",
      "loss on batch = 16.344546678727305\n",
      "loss on batch = 16.68412320347393\n",
      "loss on batch = 17.605860022474843\n",
      "loss on batch = 16.307614042848925\n",
      "loss on batch = 16.5523965589355\n",
      "loss on batch = 17.349138174944184\n",
      "loss on batch = 16.024006271702916\n",
      "loss on batch = 14.912342743731854\n",
      "loss on batch = 15.427625228141014\n",
      "loss on batch = 15.304350971808493\n",
      "loss on batch = 16.7514075975348\n",
      "loss on batch = 14.942004639030298\n",
      "loss on batch = 17.234264348187875\n",
      "loss on batch = 18.375374626834557\n",
      "loss on batch = 16.994768563754484\n",
      "loss on batch = 15.670417093395098\n",
      "loss on batch = 17.91287578408327\n",
      "loss on batch = 18.199419271375675\n",
      "loss on batch = 17.31641701661044\n",
      "loss on batch = 15.817799678625349\n",
      "loss on batch = 16.423707479070735\n",
      "loss on batch = 15.656513657359048\n",
      "loss on batch = 14.6988302793095\n",
      "loss on batch = 16.823013325138827\n",
      "loss on batch = 15.871628572634823\n",
      "loss on batch = 17.319709605533603\n",
      "loss on batch = 16.427394436778908\n",
      "loss on batch = 15.904563387014786\n",
      "loss on batch = 15.604963598233738\n",
      "loss on batch = 14.900922093174458\n",
      "loss on batch = 14.316775131719975\n",
      "loss on batch = 18.029979956981503\n",
      "loss on batch = 17.701341381050852\n",
      "loss on batch = 16.535160199339156\n",
      "loss on batch = 17.91852342007271\n",
      "loss on batch = 17.427220170302334\n",
      "loss on batch = 16.61081382735647\n",
      "loss on batch = 17.24121023545485\n",
      "loss on batch = 18.35544459315633\n",
      "loss on batch = 16.641818923185625\n",
      "loss on batch = 15.97983296061992\n",
      "loss on batch = 16.277583281826708\n",
      "loss on batch = 15.061682587381902\n",
      "loss on batch = 14.03053391967824\n",
      "loss on batch = 15.904305385920557\n",
      "loss on batch = 15.289892011262559\n",
      "loss on batch = 14.219705382417455\n",
      "loss on batch = 14.561251256872282\n",
      "loss on batch = 15.93367863867246\n",
      "loss on batch = 15.6156435126726\n",
      "loss on batch = 16.640055639060293\n",
      "loss on batch = 17.576521278458387\n",
      "loss on batch = 18.290706953670707\n",
      "loss on batch = 15.5497692855554\n",
      "loss on batch = 16.835637328018663\n",
      "loss on batch = 15.7524639075885\n",
      "loss on batch = 17.04490017970534\n",
      "loss on batch = 16.03194702377645\n",
      "loss on batch = 17.634115868304157\n",
      "loss on batch = 15.303921513242972\n",
      "loss on batch = 15.642874670027691\n",
      "loss on batch = 13.440289815930061\n",
      "loss on batch = 15.942193819321258\n",
      "loss on batch = 16.58676820817144\n",
      "loss on batch = 17.217245825613922\n",
      "loss on batch = 18.394306323989568\n",
      "loss on batch = 16.465768079066386\n",
      "loss on batch = 17.813585600509775\n",
      "loss on batch = 16.032643257647187\n",
      "loss on batch = 14.037911680644164\n",
      "loss on batch = 16.59991590740366\n",
      "loss on batch = 15.515488453504096\n",
      "loss on batch = 16.38361156560189\n",
      "loss on batch = 15.506214690254806\n",
      "loss on batch = 17.393814169137585\n",
      "loss on batch = 13.09589984116693\n",
      "loss on batch = 16.588664275498374\n",
      "loss on batch = 15.874857470582818\n",
      "loss on batch = 16.902909361784246\n",
      "loss on batch = 16.559830184012966\n",
      "loss on batch = 16.007287704956937\n",
      "loss on batch = 17.252477867409333\n",
      "loss on batch = 17.65196024993827\n",
      "loss on batch = 13.857300085746855\n",
      "loss on batch = 16.662501316894044\n",
      "loss on batch = 16.18266646284632\n",
      "loss on batch = 17.05755394080685\n",
      "loss on batch = 15.285257851321166\n",
      "loss on batch = 16.65433759590936\n",
      "loss on batch = 17.51822753355158\n",
      "loss on batch = 17.26699551143792\n",
      "loss on batch = 17.92684877524512\n",
      "loss on batch = 16.54357434989782\n",
      "loss on batch = 16.684447050622666\n",
      "loss on batch = 15.53032961437712\n",
      "loss on batch = 16.115010011738747\n",
      "loss on batch = 16.18110154715973\n",
      "loss on batch = 17.34100377530151\n",
      "loss on batch = 17.133481772007613\n",
      "loss on batch = 15.903546844176496\n",
      "loss on batch = 16.4790240873275\n",
      "loss on batch = 17.297483788944753\n",
      "loss on batch = 15.685897892896785\n",
      "loss on batch = 17.283906176563903\n",
      "loss on batch = 15.642086189997528\n",
      "loss on batch = 15.205587584322712\n",
      "loss on batch = 17.652903742229405\n",
      "loss on batch = 16.790943664150177\n",
      "loss on batch = 15.159067863607175\n",
      "loss on batch = 15.512765680074027\n",
      "loss on batch = 16.41878288008892\n",
      "loss on batch = 16.082166097061254\n",
      "loss on batch = 16.705422284948575\n",
      "loss on batch = 17.351377907283343\n",
      "loss on batch = 18.12636949860884\n",
      "loss on batch = 16.281085378974048\n",
      "loss on batch = 16.27835090326542\n",
      "loss on batch = 16.77304785459293\n",
      "loss on batch = 16.24268509830859\n",
      "loss on batch = 18.06172769895582\n",
      "loss on batch = 15.811980328718125\n",
      "loss on batch = 16.16972196928141\n",
      "loss on batch = 15.137702218636061\n",
      "loss on batch = 15.809462714331342\n",
      "loss on batch = 16.284459425395667\n",
      "loss on batch = 15.270514008108577\n",
      "loss on batch = 15.84380772679999\n",
      "loss on batch = 15.648842579366516\n",
      "loss on batch = 17.152505115987644\n",
      "loss on batch = 17.112182530447747\n",
      "loss on batch = 14.7569161306662\n",
      "loss on batch = 16.225831949764064\n",
      "loss on batch = 16.179622222876915\n",
      "loss on batch = 16.244315053521134\n",
      "loss on batch = 15.717063264005539\n",
      "loss on batch = 15.747196878939063\n",
      "loss on batch = 15.215359911214378\n",
      "loss on batch = 16.291531788621167\n",
      "loss on batch = 16.484904133758267\n",
      "loss on batch = 16.581515458748893\n",
      "loss on batch = 17.13305913152199\n",
      "loss on batch = 14.230359986940076\n",
      "loss on batch = 16.468063220183517\n",
      "loss on batch = 17.35990810559195\n",
      "Acc: 0.8634\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=True)\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(200)\n",
    "nn.add_layer(80)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=1, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n batch = 14.572000605769903\n",
      "loss on batch = 15.202963195083786\n",
      "loss on batch = 16.539745383310255\n",
      "loss on batch = 14.808924786371142\n",
      "loss on batch = 15.263791830236768\n",
      "loss on batch = 15.377144839246338\n",
      "loss on batch = 14.852911757403739\n",
      "loss on batch = 14.068169917309081\n",
      "loss on batch = 17.682202196847335\n",
      "loss on batch = 15.454443500293802\n",
      "loss on batch = 15.119483232611207\n",
      "loss on batch = 15.586574365745143\n",
      "loss on batch = 15.386647336300843\n",
      "loss on batch = 14.493193319796536\n",
      "loss on batch = 16.422194166907325\n",
      "loss on batch = 16.425386990597303\n",
      "loss on batch = 14.81357764865727\n",
      "loss on batch = 15.472163541842825\n",
      "loss on batch = 15.674180401376919\n",
      "loss on batch = 14.646249588739597\n",
      "loss on batch = 15.092759459755543\n",
      "loss on batch = 15.922818921233196\n",
      "loss on batch = 15.058508075990865\n",
      "loss on batch = 16.185759024390993\n",
      "loss on batch = 15.726102129569759\n",
      "loss on batch = 15.397597697145235\n",
      "loss on batch = 16.986110927400297\n",
      "loss on batch = 15.312297428412773\n",
      "loss on batch = 15.5564031131124\n",
      "loss on batch = 15.89413395041708\n",
      "loss on batch = 15.205866886353034\n",
      "loss on batch = 15.746880930778662\n",
      "loss on batch = 15.703477033137712\n",
      "loss on batch = 15.001584203034591\n",
      "loss on batch = 14.814350094749864\n",
      "loss on batch = 14.704432562956121\n",
      "loss on batch = 15.926652555177302\n",
      "loss on batch = 16.407199034772972\n",
      "loss on batch = 15.865613948900648\n",
      "loss on batch = 15.018703615396348\n",
      "loss on batch = 14.963643184153478\n",
      "loss on batch = 15.273771326789788\n",
      "loss on batch = 14.892120991302885\n",
      "loss on batch = 15.415251630473744\n",
      "loss on batch = 16.773825449864134\n",
      "loss on batch = 14.69314564623975\n",
      "loss on batch = 16.17512822269676\n",
      "loss on batch = 15.619261015207453\n",
      "loss on batch = 15.684698531717748\n",
      "loss on batch = 16.9935362289905\n",
      "loss on batch = 14.932051135056632\n",
      "loss on batch = 14.82632598776378\n",
      "loss on batch = 15.644087710489206\n",
      "loss on batch = 15.538126188192798\n",
      "loss on batch = 15.829864534458686\n",
      "loss on batch = 14.678627641141674\n",
      "loss on batch = 14.567359165516557\n",
      "loss on batch = 14.773580501709024\n",
      "loss on batch = 14.798401763324925\n",
      "loss on batch = 14.401086708927822\n",
      "loss on batch = 14.346261846328497\n",
      "loss on batch = 16.063251443138\n",
      "loss on batch = 15.682013508044388\n",
      "loss on batch = 14.836331122301107\n",
      "loss on batch = 15.085476477853764\n",
      "loss on batch = 14.619321803408143\n",
      "loss on batch = 14.91908379261362\n",
      "loss on batch = 14.539603966367345\n",
      "loss on batch = 15.223210509983046\n",
      "loss on batch = 15.05966965725789\n",
      "loss on batch = 16.47935616168953\n",
      "loss on batch = 16.770115323458423\n",
      "loss on batch = 14.215141516795791\n",
      "loss on batch = 14.462280004939075\n",
      "loss on batch = 16.728790339575106\n",
      "loss on batch = 15.215738976018713\n",
      "loss on batch = 16.29549714309224\n",
      "loss on batch = 15.355341706345271\n",
      "loss on batch = 14.238582001507076\n",
      "loss on batch = 15.638210418187015\n",
      "loss on batch = 14.909873064988663\n",
      "loss on batch = 14.572202342679455\n",
      "loss on batch = 16.26151943053369\n",
      "loss on batch = 15.924239482235148\n",
      "loss on batch = 15.0399308901528\n",
      "loss on batch = 15.389935531066076\n",
      "loss on batch = 14.4958957916718\n",
      "loss on batch = 14.558966588038174\n",
      "loss on batch = 14.796088324606941\n",
      "loss on batch = 15.08131593655698\n",
      "loss on batch = 16.023624660158024\n",
      "loss on batch = 15.477260337935741\n",
      "loss on batch = 14.604375060218839\n",
      "loss on batch = 14.776696884295355\n",
      "loss on batch = 14.210574793642065\n",
      "loss on batch = 15.79214678612847\n",
      "loss on batch = 14.9409134324783\n",
      "loss on batch = 15.190202734795218\n",
      "loss on batch = 14.400370808573049\n",
      "loss on batch = 14.625760583431656\n",
      "loss on batch = 14.640396606864153\n",
      "loss on batch = 14.596992241621528\n",
      "loss on batch = 15.521789126610507\n",
      "loss on batch = 16.14861279498255\n",
      "loss on batch = 16.841588736000304\n",
      "loss on batch = 15.568838199550385\n",
      "loss on batch = 15.021317482551813\n",
      "loss on batch = 15.243618266934927\n",
      "loss on batch = 14.944678894826426\n",
      "loss on batch = 15.17774162999771\n",
      "loss on batch = 15.422029236445294\n",
      "loss on batch = 15.991440317627355\n",
      "loss on batch = 15.809922882775801\n",
      "loss on batch = 14.955293253626351\n",
      "loss on batch = 15.893076116287286\n",
      "loss on batch = 17.113726731851763\n",
      "loss on batch = 15.511886221726417\n",
      "loss on batch = 15.07103114911213\n",
      "loss on batch = 16.26904286299972\n",
      "loss on batch = 14.687276498849492\n",
      "loss on batch = 14.855328077862293\n",
      "loss on batch = 16.178818642210352\n",
      "loss on batch = 15.53588268389711\n",
      "loss on batch = 15.159814697141256\n",
      "loss on batch = 14.914820844060065\n",
      "loss on batch = 15.529771368200695\n",
      "loss on batch = 15.335547037157255\n",
      "loss on batch = 14.559247050967487\n",
      "loss on batch = 15.194374831448036\n",
      "loss on batch = 15.849829698972435\n",
      "loss on batch = 14.83230704546584\n",
      "loss on batch = 16.295889029246336\n",
      "loss on batch = 15.02039077618059\n",
      "loss on batch = 15.907943429642215\n",
      "loss on batch = 14.816666224280873\n",
      "loss on batch = 15.620449604363529\n",
      "loss on batch = 15.453176305580007\n",
      "loss on batch = 14.577868273789871\n",
      "loss on batch = 14.400121900225503\n",
      "loss on batch = 15.614678354932161\n",
      "loss on batch = 15.329907373380834\n",
      "loss on batch = 15.756074817957657\n",
      "loss on batch = 15.702068374406137\n",
      "loss on batch = 15.816073342128542\n",
      "loss on batch = 14.532032096710378\n",
      "loss on batch = 15.256792979929196\n",
      "loss on batch = 14.31812575114273\n",
      "loss on batch = 14.851296214931804\n",
      "loss on batch = 15.876787608898404\n",
      "loss on batch = 15.710454944853668\n",
      "loss on batch = 15.227227698007976\n",
      "loss on batch = 15.739085063673357\n",
      "loss on batch = 14.847419174224564\n",
      "loss on batch = 15.362066599074772\n",
      "loss on batch = 15.450830199782732\n",
      "loss on batch = 15.637078049079493\n",
      "loss on batch = 15.668245864408226\n",
      "loss on batch = 15.855522436842335\n",
      "loss on batch = 15.35953583039137\n",
      "loss on batch = 16.260781906358716\n",
      "loss on batch = 16.160391143984754\n",
      "loss on batch = 15.762754747519844\n",
      "loss on batch = 16.04707570550701\n",
      "loss on batch = 17.018691881581468\n",
      "loss on batch = 16.672157701519474\n",
      "loss on batch = 17.078331764189258\n",
      "loss on batch = 15.626970548789828\n",
      "loss on batch = 15.791329101628298\n",
      "loss on batch = 14.894547217145531\n",
      "loss on batch = 17.45168139246339\n",
      "loss on batch = 15.219351543352586\n",
      "loss on batch = 14.971286470752503\n",
      "loss on batch = 15.18175265013328\n",
      "loss on batch = 15.478871317673155\n",
      "loss on batch = 15.65821068344712\n",
      "loss on batch = 14.907063437022737\n",
      "loss on batch = 14.886701323085058\n",
      "loss on batch = 14.519711280934484\n",
      "loss on batch = 15.519883630359256\n",
      "loss on batch = 15.09492982547394\n",
      "loss on batch = 16.13484910013835\n",
      "loss on batch = 14.613004716463182\n",
      "loss on batch = 14.832618904411174\n",
      "loss on batch = 14.801041900624584\n",
      "loss on batch = 15.305221630480807\n",
      "loss on batch = 14.92136581322417\n",
      "loss on batch = 15.486551378727686\n",
      "loss on batch = 15.700537776513059\n",
      "loss on batch = 14.805132790947027\n",
      "loss on batch = 14.69636916127979\n",
      "loss on batch = 15.761004366024368\n",
      "loss on batch = 14.868392790964819\n",
      "loss on batch = 15.24331962043989\n",
      "loss on batch = 14.38752417181989\n",
      "loss on batch = 14.837853352390407\n",
      "loss on batch = 14.977850391208833\n",
      "loss on batch = 16.390623490896434\n",
      "loss on batch = 14.66360629123158\n",
      "loss on batch = 14.389770318635243\n",
      "loss on batch = 15.472204781591516\n",
      "loss on batch = 14.652364943934376\n",
      "loss on batch = 15.596299163017012\n",
      "loss on batch = 14.812145232797803\n",
      "loss on batch = 14.926341862070613\n",
      "loss on batch = 15.504277604130648\n",
      "loss on batch = 15.772730013663647\n",
      "loss on batch = 15.479026265654749\n",
      "loss on batch = 15.610454060551225\n",
      "loss on batch = 15.028109823147885\n",
      "loss on batch = 16.827510734193147\n",
      "loss on batch = 15.500371830350952\n",
      "loss on batch = 14.723679687769424\n",
      "loss on batch = 16.018097131246428\n",
      "loss on batch = 14.870101759896066\n",
      "loss on batch = 15.036144770758305\n",
      "loss on batch = 15.239859174811617\n",
      "loss on batch = 17.088004671351285\n",
      "loss on batch = 15.17380929513806\n",
      "loss on batch = 14.893947735816571\n",
      "loss on batch = 15.05714225443807\n",
      "loss on batch = 15.299449411787645\n",
      "loss on batch = 15.357368922408822\n",
      "loss on batch = 15.93304247558304\n",
      "loss on batch = 15.495163029106134\n",
      "loss on batch = 14.948980440193358\n",
      "loss on batch = 14.85786175077527\n",
      "loss on batch = 14.98463156627903\n",
      "loss on batch = 14.405228148999655\n",
      "loss on batch = 14.682635288293307\n",
      "loss on batch = 15.172285783745908\n",
      "loss on batch = 15.574872875852938\n",
      "loss on batch = 15.234991725381537\n",
      "loss on batch = 15.638781708234074\n",
      "loss on batch = 15.0032387186006\n",
      "loss on batch = 15.342249745262624\n",
      "loss on batch = 14.90818674795644\n",
      "loss on batch = 14.818846916882542\n",
      "loss on batch = 14.755096919223003\n",
      "loss on batch = 14.949991532689237\n",
      "loss on batch = 16.73233279214632\n",
      "loss on batch = 15.939873902655384\n",
      "loss on batch = 15.450943240760388\n",
      "loss on batch = 14.583047952133201\n",
      "loss on batch = 14.55938965399022\n",
      "loss on batch = 15.224267698125267\n",
      "loss on batch = 15.016024287619556\n",
      "loss on batch = 15.584747960061724\n",
      "loss on batch = 16.15634571921091\n",
      "loss on batch = 15.4959781496824\n",
      "loss on batch = 14.830329090981339\n",
      "loss on batch = 14.76131519732255\n",
      "loss on batch = 15.898316686901946\n",
      "loss on batch = 15.55682420604018\n",
      "loss on batch = 15.314001157505063\n",
      "loss on batch = 15.208249026759582\n",
      "loss on batch = 16.039714865996764\n",
      "loss on batch = 16.728956164124767\n",
      "loss on batch = 15.919143164736992\n",
      "loss on batch = 16.42229239824243\n",
      "loss on batch = 15.035214637750336\n",
      "loss on batch = 15.989516606792241\n",
      "loss on batch = 14.75036833151848\n",
      "loss on batch = 16.980509120597098\n",
      "loss on batch = 15.88346185296337\n",
      "loss on batch = 15.111349345573515\n",
      "loss on batch = 15.870749147084474\n",
      "loss on batch = 15.001832487698765\n",
      "loss on batch = 15.402254757539541\n",
      "loss on batch = 14.683746338412472\n",
      "loss on batch = 15.25482695903108\n",
      "loss on batch = 15.086779321749132\n",
      "loss on batch = 15.093496462124126\n",
      "loss on batch = 15.395866719906621\n",
      "loss on batch = 15.51710078797505\n",
      "loss on batch = 15.527573683375781\n",
      "loss on batch = 14.963530081042109\n",
      "loss on batch = 16.026697349416153\n",
      "loss on batch = 15.678749385028965\n",
      "loss on batch = 15.537448157888306\n",
      "loss on batch = 15.153821331295175\n",
      "loss on batch = 14.777516375884097\n",
      "loss on batch = 16.70176595511612\n",
      "loss on batch = 15.43044754557147\n",
      "loss on batch = 15.146251222510147\n",
      "loss on batch = 14.613098576847511\n",
      "loss on batch = 16.075757770100935\n",
      "loss on batch = 15.344650572808012\n",
      "loss on batch = 14.607921985215745\n",
      "loss on batch = 17.478071068362187\n",
      "loss on batch = 16.201886503402193\n",
      "loss on batch = 15.91335547953802\n",
      "loss on batch = 15.976858405592953\n",
      "loss on batch = 14.929291670324563\n",
      "loss on batch = 15.467258351834387\n",
      "loss on batch = 14.607434588661299\n",
      "loss on batch = 15.692711061329444\n",
      "loss on batch = 14.874087224455689\n",
      "loss on batch = 17.02300102603093\n",
      "loss on batch = 14.981732372825789\n",
      "loss on batch = 15.759212688858618\n",
      "loss on batch = 14.566791536931106\n",
      "loss on batch = 14.601083996280256\n",
      "loss on batch = 14.795079907916323\n",
      "loss on batch = 14.514356599935004\n",
      "loss on batch = 15.278895150858753\n",
      "loss on batch = 14.289677017066758\n",
      "loss on batch = 14.753494514450951\n",
      "loss on batch = 14.704696464336113\n",
      "loss on batch = 15.503065555879104\n",
      "loss on batch = 15.363795668228065\n",
      "loss on batch = 16.118786860546543\n",
      "loss on batch = 15.462424246714765\n",
      "loss on batch = 14.848342949169762\n",
      "loss on batch = 14.698937883482351\n",
      "loss on batch = 15.439767344218337\n",
      "loss on batch = 15.805736643043062\n",
      "loss on batch = 14.879407317369154\n",
      "loss on batch = 14.960645759862189\n",
      "loss on batch = 15.943576899972033\n",
      "loss on batch = 14.955011847311729\n",
      "loss on batch = 15.824577339692144\n",
      "loss on batch = 14.391699678508472\n",
      "loss on batch = 15.259717523593665\n",
      "loss on batch = 15.869581569658123\n",
      "loss on batch = 15.422963569579764\n",
      "loss on batch = 14.937306686955408\n",
      "loss on batch = 15.810074848472517\n",
      "loss on batch = 15.620896239011238\n",
      "loss on batch = 15.597933532943252\n",
      "loss on batch = 15.289371797742772\n",
      "loss on batch = 14.688817284636853\n",
      "loss on batch = 15.937967303503042\n",
      "loss on batch = 16.618932570503166\n",
      "loss on batch = 14.460451230890301\n",
      "loss on batch = 14.729400930725983\n",
      "loss on batch = 14.811865007140915\n",
      "loss on batch = 15.62722960730176\n",
      "loss on batch = 14.946775382962429\n",
      "loss on batch = 15.076039853423463\n",
      "loss on batch = 14.70781285666407\n",
      "loss on batch = 14.078259661131193\n",
      "loss on batch = 14.779723888696857\n",
      "loss on batch = 14.848236207584652\n",
      "loss on batch = 14.386772194009602\n",
      "loss on batch = 15.20496050239899\n",
      "loss on batch = 15.253627890778025\n",
      "loss on batch = 15.697718283462237\n",
      "loss on batch = 16.23224001748852\n",
      "loss on batch = 16.249382753797345\n",
      "loss on batch = 16.6891233311784\n",
      "loss on batch = 15.218459624100252\n",
      "loss on batch = 15.388039986263436\n",
      "loss on batch = 14.883389926044563\n",
      "loss on batch = 14.916020756158003\n",
      "loss on batch = 14.798465053176612\n",
      "loss on batch = 15.587690651515059\n",
      "loss on batch = 15.282758695062094\n",
      "loss on batch = 15.253127639052733\n",
      "loss on batch = 14.715833880641839\n",
      "loss on batch = 15.936356817205688\n",
      "loss on batch = 14.595049418289543\n",
      "loss on batch = 14.989795039820198\n",
      "loss on batch = 14.74602079156901\n",
      "loss on batch = 15.014089663581895\n",
      "loss on batch = 14.90171656066915\n",
      "loss on batch = 14.995607549376814\n",
      "loss on batch = 14.708573492070402\n",
      "loss on batch = 14.315326488819501\n",
      "loss on batch = 15.177102221055677\n",
      "loss on batch = 14.155055266804496\n",
      "loss on batch = 14.599231222707129\n",
      "loss on batch = 15.954848911141635\n",
      "loss on batch = 15.537443620941492\n",
      "loss on batch = 15.088552486391892\n",
      "loss on batch = 15.727148804150765\n",
      "loss on batch = 15.440823197703324\n",
      "loss on batch = 16.11223168188469\n",
      "loss on batch = 15.206940660325328\n",
      "loss on batch = 14.89059283903001\n",
      "loss on batch = 14.179335060753417\n",
      "loss on batch = 16.459503052973695\n",
      "loss on batch = 15.282471284933003\n",
      "loss on batch = 14.974151430806684\n",
      "loss on batch = 14.672676582653507\n",
      "loss on batch = 16.23802104581116\n",
      "loss on batch = 15.62994274919798\n",
      "loss on batch = 14.87682803523311\n",
      "loss on batch = 14.672331467979012\n",
      "loss on batch = 15.254824712669462\n",
      "loss on batch = 14.777473508561107\n",
      "loss on batch = 14.969542120149363\n",
      "loss on batch = 15.072030783312265\n",
      "loss on batch = 16.868687568164404\n",
      "loss on batch = 14.555429379859149\n",
      "loss on batch = 16.167363685498266\n",
      "loss on batch = 15.649256392141051\n",
      "loss on batch = 15.65126462699815\n",
      "loss on batch = 16.234877091282698\n",
      "loss on batch = 14.967392536397584\n",
      "loss on batch = 16.00395821651056\n",
      "loss on batch = 14.8647749065587\n",
      "loss on batch = 15.780729368826572\n",
      "loss on batch = 15.03001776699342\n",
      "loss on batch = 14.660308930819696\n",
      "loss on batch = 15.078407150665383\n",
      "loss on batch = 16.975464842222472\n",
      "loss on batch = 14.930669379025645\n",
      "loss on batch = 15.912436713069516\n",
      "loss on batch = 16.30738477931151\n",
      "loss on batch = 16.084833225109605\n",
      "loss on batch = 14.582951267997935\n",
      "loss on batch = 14.8441141218977\n",
      "loss on batch = 16.987492618616002\n",
      "loss on batch = 15.943382344055308\n",
      "loss on batch = 14.903256510646067\n",
      "loss on batch = 15.315586656047724\n",
      "loss on batch = 15.0373016049407\n",
      "loss on batch = 16.26332904008499\n",
      "loss on batch = 15.699355217556015\n",
      "loss on batch = 16.12916342832161\n",
      "loss on batch = 15.609886336409968\n",
      "loss on batch = 14.992573109058721\n",
      "loss on batch = 14.911015525856998\n",
      "loss on batch = 15.18471782837628\n",
      "loss on batch = 14.869098866394626\n",
      "loss on batch = 16.206927485982934\n",
      "loss on batch = 15.756714101129331\n",
      "loss on batch = 15.567417247415761\n",
      "loss on batch = 16.91283346555736\n",
      "loss on batch = 14.905592918380206\n",
      "loss on batch = 14.474844319394483\n",
      "loss on batch = 14.559701723543597\n",
      "loss on batch = 14.890136192657739\n",
      "loss on batch = 14.921969009917202\n",
      "loss on batch = 15.39078524706431\n",
      "loss on batch = 15.443773341460174\n",
      "loss on batch = 14.908592699463325\n",
      "loss on batch = 14.43533286909646\n",
      "loss on batch = 15.604662488275284\n",
      "loss on batch = 15.208894358848374\n",
      "loss on batch = 16.420387771323476\n",
      "loss on batch = 15.11505533041072\n",
      "loss on batch = 15.1061477979116\n",
      "loss on batch = 14.856596510058353\n",
      "loss on batch = 15.760019314989261\n",
      "loss on batch = 14.899229416930517\n",
      "loss on batch = 16.912024926590604\n",
      "loss on batch = 15.418232100082868\n",
      "loss on batch = 14.936354985328173\n",
      "loss on batch = 16.322732502652972\n",
      "loss on batch = 16.97558767594507\n",
      "loss on batch = 15.654458427151534\n",
      "loss on batch = 16.112052788539245\n",
      "loss on batch = 15.312221667780827\n",
      "loss on batch = 16.151798879363675\n",
      "loss on batch = 15.045395913987774\n",
      "loss on batch = 15.318083448869693\n",
      "loss on batch = 14.827988207012211\n",
      "loss on batch = 15.437348323303697\n",
      "loss on batch = 15.20673439939081\n",
      "loss on batch = 15.673575227282658\n",
      "loss on batch = 17.21238358176738\n",
      "loss on batch = 15.116644570350582\n",
      "loss on batch = 15.881605377440462\n",
      "loss on batch = 15.434467016641705\n",
      "loss on batch = 14.84206002306842\n",
      "loss on batch = 14.926391127754002\n",
      "loss on batch = 16.280136794072003\n",
      "loss on batch = 14.78865481798368\n",
      "loss on batch = 14.965539290619994\n",
      "loss on batch = 17.7137058974422\n",
      "loss on batch = 15.231420712391605\n",
      "loss on batch = 14.507573667450123\n",
      "loss on batch = 16.18560877187222\n",
      "loss on batch = 14.8572878216406\n",
      "loss on batch = 14.372631676762055\n",
      "loss on batch = 15.741511922958932\n",
      "loss on batch = 14.061680835607373\n",
      "loss on batch = 14.84341698208738\n",
      "loss on batch = 14.49316983198205\n",
      "loss on batch = 14.753710926833012\n",
      "loss on batch = 14.912942947968016\n",
      "loss on batch = 15.755335637027633\n",
      "loss on batch = 14.593265681130182\n",
      "loss on batch = 14.363271942020951\n",
      "loss on batch = 14.701786649300336\n",
      "loss on batch = 15.056685805742765\n",
      "loss on batch = 15.239928772571634\n",
      "loss on batch = 15.438104357166381\n",
      "loss on batch = 15.464658497548236\n",
      "loss on batch = 15.624764427274524\n",
      "loss on batch = 14.980177445333561\n",
      "loss on batch = 15.29083346069394\n",
      "loss on batch = 14.86666934254562\n",
      "loss on batch = 14.822094740647607\n",
      "loss on batch = 15.025782621563582\n",
      "loss on batch = 16.874667460082225\n",
      "loss on batch = 15.18470522492274\n",
      "loss on batch = 14.749384919262011\n",
      "loss on batch = 15.46723567741769\n",
      "loss on batch = 14.660570215601236\n",
      "loss on batch = 14.484432300626715\n",
      "loss on batch = 15.630937076207882\n",
      "loss on batch = 14.759349660393987\n",
      "loss on batch = 14.742802501541163\n",
      "loss on batch = 14.836713191541683\n",
      "loss on batch = 15.225637921677652\n",
      "loss on batch = 14.982460630129674\n",
      "loss on batch = 14.427287419150163\n",
      "loss on batch = 16.676415483401573\n",
      "loss on batch = 15.149084402131667\n",
      "loss on batch = 16.75224835967802\n",
      "loss on batch = 15.944546859111101\n",
      "loss on batch = 14.945365504112345\n",
      "loss on batch = 15.77748798451275\n",
      "loss on batch = 17.40196004105617\n",
      "loss on batch = 15.523166286510294\n",
      "loss on batch = 17.086417534196514\n",
      "loss on batch = 16.774696502069997\n",
      "loss on batch = 16.102858443066804\n",
      "loss on batch = 15.272101634230257\n",
      "loss on batch = 15.669697381404522\n",
      "loss on batch = 14.78828334423309\n",
      "loss on batch = 17.64798661691494\n",
      "loss on batch = 15.1541395668301\n",
      "loss on batch = 16.457643624332306\n",
      "loss on batch = 14.864769106415277\n",
      "loss on batch = 15.359619398227034\n",
      "loss on batch = 16.14255906274372\n",
      "loss on batch = 15.006706454604862\n",
      "loss on batch = 14.980931432422945\n",
      "loss on batch = 15.995930666464488\n",
      "loss on batch = 15.254844673170574\n",
      "loss on batch = 15.092098635400978\n",
      "loss on batch = 14.971962989669123\n",
      "loss on batch = 15.281748964173628\n",
      "loss on batch = 16.141661718164848\n",
      "loss on batch = 15.765285682070214\n",
      "loss on batch = 15.09046152633883\n",
      "loss on batch = 15.767140132560373\n",
      "loss on batch = 16.617255634621845\n",
      "loss on batch = 14.706145743492467\n",
      "loss on batch = 15.058446493863892\n",
      "loss on batch = 15.253182673591283\n",
      "loss on batch = 15.364586181621643\n",
      "loss on batch = 15.508565889322375\n",
      "loss on batch = 14.868277910942885\n",
      "loss on batch = 14.339246900626186\n",
      "loss on batch = 15.044592064105515\n",
      "loss on batch = 15.20439184788468\n",
      "loss on batch = 16.188512907609844\n",
      "loss on batch = 15.842585625879744\n",
      "loss on batch = 15.466303040568818\n",
      "loss on batch = 14.888734385698633\n",
      "loss on batch = 15.659570693480502\n",
      "loss on batch = 15.617857846189686\n",
      "loss on batch = 14.919013766884817\n",
      "loss on batch = 15.062602306396588\n",
      "loss on batch = 14.41908651397816\n",
      "loss on batch = 15.672405346183897\n",
      "loss on batch = 14.830200190304417\n",
      "loss on batch = 15.106446799616421\n",
      "loss on batch = 14.924387368021819\n",
      "loss on batch = 14.90547365599867\n",
      "loss on batch = 14.489067376730956\n",
      "loss on batch = 14.724184303362724\n",
      "loss on batch = 15.13789257876908\n",
      "loss on batch = 15.015266082413184\n",
      "loss on batch = 14.966639704564482\n",
      "loss on batch = 14.444265940334162\n",
      "loss on batch = 15.695034686104112\n",
      "loss on batch = 15.137470707078608\n",
      "loss on batch = 14.49093176504914\n",
      "loss on batch = 14.623310156799493\n",
      "loss on batch = 15.168439187249323\n",
      "Acc: 0.9126\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=True,  activation_function ='tanh')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(196)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(20)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=4, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "391636645420121\n",
      "loss on batch = 14.639313152792447\n",
      "loss on batch = 15.993440665648157\n",
      "loss on batch = 17.294457006825716\n",
      "loss on batch = 15.590144733968526\n",
      "loss on batch = 16.03789035933278\n",
      "loss on batch = 14.990677415060976\n",
      "loss on batch = 15.426720047312031\n",
      "loss on batch = 14.690385293307505\n",
      "loss on batch = 16.64994199730276\n",
      "loss on batch = 15.517577028326459\n",
      "loss on batch = 15.482643740461935\n",
      "loss on batch = 15.004101993207868\n",
      "loss on batch = 14.81533320483238\n",
      "loss on batch = 13.920552053550361\n",
      "loss on batch = 16.573712877271678\n",
      "loss on batch = 15.86726018844237\n",
      "loss on batch = 14.399448962249826\n",
      "loss on batch = 16.692476798594953\n",
      "loss on batch = 14.456447415228372\n",
      "loss on batch = 15.142212381642171\n",
      "loss on batch = 13.522004718202039\n",
      "loss on batch = 15.643577708445537\n",
      "loss on batch = 16.05996401776989\n",
      "loss on batch = 13.33264126478989\n",
      "loss on batch = 15.210294950077136\n",
      "loss on batch = 15.420725904158406\n",
      "loss on batch = 17.19520854859501\n",
      "loss on batch = 14.808148845517014\n",
      "loss on batch = 16.200130156358686\n",
      "loss on batch = 15.673809329694684\n",
      "loss on batch = 16.408599003072034\n",
      "loss on batch = 15.538068460121451\n",
      "loss on batch = 14.35515777133575\n",
      "loss on batch = 15.327451666717762\n",
      "loss on batch = 13.988829849795074\n",
      "loss on batch = 15.425616347223851\n",
      "loss on batch = 14.525047855566022\n",
      "loss on batch = 15.99093857694806\n",
      "loss on batch = 17.157508682384513\n",
      "loss on batch = 15.7729784591359\n",
      "loss on batch = 14.221810620771315\n",
      "loss on batch = 14.861969992084187\n",
      "loss on batch = 14.524581942642367\n",
      "loss on batch = 16.072794115730623\n",
      "loss on batch = 17.66991344499846\n",
      "loss on batch = 14.31175261712088\n",
      "loss on batch = 16.20019096109296\n",
      "loss on batch = 15.297326923865741\n",
      "loss on batch = 15.207726610367418\n",
      "loss on batch = 16.968133312653045\n",
      "loss on batch = 15.969874599653053\n",
      "loss on batch = 13.997801091186698\n",
      "loss on batch = 14.311016895140071\n",
      "loss on batch = 16.877468052457893\n",
      "loss on batch = 14.77405901252726\n",
      "loss on batch = 14.70771607587385\n",
      "loss on batch = 14.281747861579651\n",
      "loss on batch = 13.534125674270122\n",
      "loss on batch = 14.2761156918124\n",
      "loss on batch = 14.396185784758208\n",
      "loss on batch = 15.499429524214333\n",
      "loss on batch = 14.934478328450073\n",
      "loss on batch = 16.08683046179783\n",
      "loss on batch = 14.330110403943877\n",
      "loss on batch = 15.74672883327672\n",
      "loss on batch = 13.928021297840964\n",
      "loss on batch = 15.277051479255908\n",
      "loss on batch = 14.938686631023804\n",
      "loss on batch = 15.566157208196378\n",
      "loss on batch = 13.691554948812911\n",
      "loss on batch = 15.599470903827989\n",
      "loss on batch = 15.266051330885311\n",
      "loss on batch = 15.33786175817579\n",
      "loss on batch = 15.359475786006414\n",
      "loss on batch = 16.43267234955895\n",
      "loss on batch = 15.027940516031544\n",
      "loss on batch = 15.111466361951322\n",
      "loss on batch = 16.62201311141322\n",
      "loss on batch = 16.187863761762763\n",
      "loss on batch = 15.649155210434454\n",
      "loss on batch = 14.676032478727132\n",
      "loss on batch = 14.995728364786235\n",
      "loss on batch = 15.860230008679771\n",
      "loss on batch = 15.795475613810549\n",
      "loss on batch = 15.660273012330887\n",
      "loss on batch = 15.577075492081699\n",
      "loss on batch = 13.994356999586955\n",
      "loss on batch = 15.169345402825499\n",
      "loss on batch = 16.28166460057052\n",
      "loss on batch = 17.090956186248096\n",
      "loss on batch = 15.86737370157894\n",
      "loss on batch = 16.546520663108645\n",
      "loss on batch = 14.056889087803318\n",
      "loss on batch = 15.03466815287156\n",
      "loss on batch = 15.155869451988675\n",
      "loss on batch = 13.894578445220068\n",
      "loss on batch = 15.582206242497536\n",
      "loss on batch = 14.281128981442677\n",
      "loss on batch = 15.621311919965446\n",
      "loss on batch = 14.694091851886498\n",
      "loss on batch = 14.963667386412215\n",
      "loss on batch = 15.30269045373545\n",
      "loss on batch = 15.79525870711381\n",
      "loss on batch = 16.227721609047098\n",
      "loss on batch = 15.56406202578303\n",
      "loss on batch = 17.7883338910938\n",
      "loss on batch = 15.66401946087566\n",
      "loss on batch = 15.78953989149123\n",
      "loss on batch = 14.090970332369022\n",
      "loss on batch = 15.12737692358953\n",
      "loss on batch = 14.533957943191412\n",
      "loss on batch = 15.0432341003539\n",
      "loss on batch = 14.890826524181364\n",
      "loss on batch = 14.832118882867032\n",
      "loss on batch = 14.472733758285512\n",
      "loss on batch = 15.512137870407713\n",
      "loss on batch = 14.793745011176659\n",
      "loss on batch = 15.380182431137651\n",
      "loss on batch = 15.620341894166526\n",
      "loss on batch = 15.424608906030619\n",
      "loss on batch = 15.187718596111988\n",
      "loss on batch = 14.33323430681831\n",
      "loss on batch = 13.94105957004636\n",
      "loss on batch = 14.907408274893221\n",
      "loss on batch = 14.749498164727566\n",
      "loss on batch = 16.006631742445244\n",
      "loss on batch = 16.376470035280768\n",
      "loss on batch = 13.95945723516279\n",
      "loss on batch = 15.326546907043372\n",
      "loss on batch = 15.981807195792406\n",
      "loss on batch = 14.038266733419556\n",
      "loss on batch = 16.596688999139044\n",
      "loss on batch = 14.44088303033346\n",
      "loss on batch = 15.282987448021789\n",
      "loss on batch = 14.91934308473425\n",
      "loss on batch = 15.68802986683125\n",
      "loss on batch = 13.962057466561618\n",
      "loss on batch = 13.707570114629497\n",
      "loss on batch = 15.29363234237911\n",
      "loss on batch = 15.251270335018088\n",
      "loss on batch = 17.090770450314423\n",
      "loss on batch = 14.488571834193412\n",
      "loss on batch = 15.673363222731076\n",
      "loss on batch = 16.19311472749367\n",
      "loss on batch = 14.685433538272148\n",
      "loss on batch = 15.049388988067866\n",
      "loss on batch = 14.904382557306107\n",
      "loss on batch = 15.30032051726922\n",
      "loss on batch = 15.335936259075833\n",
      "loss on batch = 15.553143235419178\n",
      "loss on batch = 14.75757316608516\n",
      "loss on batch = 14.988538295282314\n",
      "loss on batch = 15.677410785631325\n",
      "loss on batch = 16.404597756806137\n",
      "loss on batch = 15.833268898363134\n",
      "loss on batch = 15.042966119987936\n",
      "loss on batch = 15.617905100042485\n",
      "loss on batch = 17.002319805195828\n",
      "loss on batch = 14.638768972912967\n",
      "loss on batch = 16.42369101723967\n",
      "loss on batch = 16.45276402994348\n",
      "loss on batch = 15.354877022120572\n",
      "loss on batch = 17.04979956441976\n",
      "loss on batch = 16.016718915519952\n",
      "loss on batch = 15.819033213344383\n",
      "loss on batch = 17.25292187018864\n",
      "loss on batch = 14.326695841237768\n",
      "loss on batch = 15.069694798230742\n",
      "loss on batch = 13.891517573210784\n",
      "loss on batch = 16.463681515464383\n",
      "loss on batch = 14.795924178324478\n",
      "loss on batch = 15.555711860037023\n",
      "loss on batch = 13.9392508565228\n",
      "loss on batch = 15.256863404717755\n",
      "loss on batch = 15.816161355723235\n",
      "loss on batch = 14.940576567896557\n",
      "loss on batch = 14.668295218414103\n",
      "loss on batch = 14.779821615188292\n",
      "loss on batch = 16.34177992880098\n",
      "loss on batch = 15.318539335522535\n",
      "loss on batch = 15.458633972910942\n",
      "loss on batch = 14.21174255277306\n",
      "loss on batch = 14.472279024334117\n",
      "loss on batch = 14.017158364954778\n",
      "loss on batch = 16.711511449857085\n",
      "loss on batch = 14.62040635121171\n",
      "loss on batch = 14.639925197749115\n",
      "loss on batch = 14.94170134057893\n",
      "loss on batch = 14.101880692670829\n",
      "loss on batch = 15.121936193553442\n",
      "loss on batch = 16.14569694479849\n",
      "loss on batch = 16.312411408879527\n",
      "loss on batch = 15.562742339080724\n",
      "loss on batch = 16.094585907380644\n",
      "loss on batch = 13.70189345196409\n",
      "loss on batch = 15.312610976951088\n",
      "loss on batch = 15.162475439146263\n",
      "loss on batch = 15.253886731046267\n",
      "loss on batch = 15.81936180476348\n",
      "loss on batch = 15.548838633610549\n",
      "loss on batch = 15.30471280421357\n",
      "loss on batch = 15.391794718300588\n",
      "loss on batch = 15.234882733652547\n",
      "loss on batch = 15.993293801977996\n",
      "loss on batch = 15.857432385527174\n",
      "loss on batch = 15.772073691104747\n",
      "loss on batch = 15.18793861554275\n",
      "loss on batch = 14.69418031664933\n",
      "loss on batch = 14.190717247240359\n",
      "loss on batch = 15.477711618652524\n",
      "loss on batch = 15.064139825569269\n",
      "loss on batch = 15.610308904341347\n",
      "loss on batch = 16.902990472318113\n",
      "loss on batch = 16.764556094358454\n",
      "loss on batch = 14.067540369759087\n",
      "loss on batch = 14.473700860741303\n",
      "loss on batch = 16.887624523321296\n",
      "loss on batch = 15.141967004982916\n",
      "loss on batch = 14.633300387076103\n",
      "loss on batch = 14.739396922166602\n",
      "loss on batch = 15.849116297401164\n",
      "loss on batch = 14.44863016628279\n",
      "loss on batch = 15.247655622856572\n",
      "loss on batch = 15.308210063431257\n",
      "loss on batch = 14.061923326225926\n",
      "loss on batch = 15.956335405730306\n",
      "loss on batch = 14.64306812125312\n",
      "loss on batch = 13.443883025283588\n",
      "loss on batch = 14.055054050452995\n",
      "loss on batch = 14.937509486866155\n",
      "loss on batch = 13.751984643819018\n",
      "loss on batch = 15.057258767002818\n",
      "loss on batch = 14.6454666071079\n",
      "loss on batch = 16.10158737581407\n",
      "loss on batch = 14.534710559963028\n",
      "loss on batch = 14.974657776829584\n",
      "loss on batch = 15.269633526277\n",
      "loss on batch = 13.861452242566445\n",
      "loss on batch = 14.509073138667768\n",
      "loss on batch = 15.37407748538799\n",
      "loss on batch = 15.793215075416887\n",
      "loss on batch = 15.068536303097591\n",
      "loss on batch = 15.299925537551635\n",
      "loss on batch = 14.569403701218295\n",
      "loss on batch = 15.256629307577796\n",
      "loss on batch = 15.058725175572786\n",
      "loss on batch = 14.509596629864909\n",
      "loss on batch = 16.532421402017718\n",
      "loss on batch = 14.202512507350988\n",
      "loss on batch = 14.543582768461006\n",
      "loss on batch = 14.908690388503748\n",
      "loss on batch = 15.199392053489621\n",
      "loss on batch = 16.05948770480692\n",
      "loss on batch = 14.94427403743255\n",
      "loss on batch = 14.129438923290191\n",
      "loss on batch = 15.65996856591746\n",
      "loss on batch = 14.948909452617364\n",
      "loss on batch = 15.66400872918052\n",
      "loss on batch = 15.791334124959622\n",
      "loss on batch = 14.517683646643757\n",
      "loss on batch = 14.94117499957416\n",
      "loss on batch = 15.781652671440554\n",
      "loss on batch = 16.06500898047472\n",
      "loss on batch = 14.875457666492508\n",
      "loss on batch = 14.686764089608824\n",
      "loss on batch = 14.023970807016406\n",
      "loss on batch = 16.817964774792937\n",
      "loss on batch = 14.930365589317105\n",
      "loss on batch = 15.442527820869964\n",
      "loss on batch = 14.946227335793955\n",
      "loss on batch = 14.214680678876377\n",
      "loss on batch = 14.781481423797056\n",
      "loss on batch = 14.867767792974568\n",
      "loss on batch = 16.859463598988146\n",
      "loss on batch = 16.869838476882705\n",
      "loss on batch = 14.7842815297236\n",
      "loss on batch = 15.417986982136568\n",
      "loss on batch = 16.921736607619074\n",
      "loss on batch = 16.022105418931048\n",
      "loss on batch = 13.856918611813136\n",
      "loss on batch = 15.179168377596126\n",
      "loss on batch = 15.592878334831294\n",
      "loss on batch = 14.813146220723674\n",
      "loss on batch = 15.690727837385591\n",
      "loss on batch = 14.898716983001005\n",
      "loss on batch = 15.34340394786223\n",
      "loss on batch = 14.36797400732406\n",
      "loss on batch = 15.083379104361804\n",
      "loss on batch = 17.4517380510624\n",
      "loss on batch = 15.745535565773089\n",
      "loss on batch = 16.70038714017849\n",
      "loss on batch = 15.67979588740254\n",
      "loss on batch = 15.775316085111735\n",
      "loss on batch = 15.020954604453376\n",
      "loss on batch = 16.181718044788575\n",
      "loss on batch = 15.250979698625292\n",
      "loss on batch = 14.897828413850773\n",
      "loss on batch = 17.2601395179532\n",
      "loss on batch = 15.690374220819233\n",
      "loss on batch = 15.816800194675254\n",
      "loss on batch = 15.420921169893747\n",
      "loss on batch = 15.466264321469492\n",
      "loss on batch = 15.113315439854109\n",
      "loss on batch = 13.759815099575707\n",
      "loss on batch = 15.080722825275142\n",
      "loss on batch = 15.400123006197838\n",
      "loss on batch = 13.944489632775271\n",
      "loss on batch = 15.395017054139611\n",
      "loss on batch = 15.51275281742709\n",
      "loss on batch = 15.981519151620663\n",
      "loss on batch = 16.666784240148946\n",
      "loss on batch = 15.993440137505289\n",
      "loss on batch = 14.3972642966296\n",
      "loss on batch = 14.563293397128213\n",
      "loss on batch = 14.929784781459539\n",
      "loss on batch = 15.024399838985516\n",
      "loss on batch = 16.443257943345934\n",
      "loss on batch = 15.073626134710402\n",
      "loss on batch = 15.2436063001027\n",
      "loss on batch = 15.365988883195326\n",
      "loss on batch = 15.307578667515136\n",
      "loss on batch = 16.377935476737093\n",
      "loss on batch = 15.77623248778346\n",
      "loss on batch = 16.562694902063203\n",
      "loss on batch = 16.585122614131066\n",
      "loss on batch = 15.054425690425271\n",
      "loss on batch = 16.36362212299411\n",
      "loss on batch = 15.643395869193427\n",
      "loss on batch = 15.580347038150409\n",
      "loss on batch = 14.835801347581688\n",
      "loss on batch = 14.733165032274151\n",
      "loss on batch = 15.027836704281635\n",
      "loss on batch = 16.158327075496526\n",
      "loss on batch = 14.73264276535261\n",
      "loss on batch = 14.422671956237727\n",
      "loss on batch = 15.647236930687741\n",
      "loss on batch = 17.15689358684281\n",
      "loss on batch = 14.9249996132308\n",
      "loss on batch = 15.676487257213857\n",
      "loss on batch = 15.064601166752634\n",
      "loss on batch = 15.790693706797043\n",
      "loss on batch = 16.186320491683652\n",
      "loss on batch = 15.224704053928876\n",
      "loss on batch = 15.6956948190915\n",
      "loss on batch = 15.39073665058123\n",
      "loss on batch = 15.386317968657972\n",
      "loss on batch = 14.50419440316249\n",
      "loss on batch = 15.079405605595667\n",
      "loss on batch = 15.237964422623225\n",
      "loss on batch = 15.923293360930934\n",
      "loss on batch = 13.677377201644251\n",
      "loss on batch = 15.336383393485526\n",
      "loss on batch = 13.783176472200555\n",
      "loss on batch = 14.867101082238205\n",
      "loss on batch = 15.090989160964083\n",
      "loss on batch = 14.606193782271665\n",
      "loss on batch = 15.430236392158433\n",
      "loss on batch = 16.086846573356148\n",
      "loss on batch = 14.631203077195615\n",
      "loss on batch = 12.794826302079823\n",
      "loss on batch = 14.688640378768273\n",
      "loss on batch = 15.27576429110491\n",
      "loss on batch = 13.376123610738542\n",
      "loss on batch = 14.957046835210647\n",
      "loss on batch = 15.894519582750334\n",
      "loss on batch = 16.53898625221072\n",
      "loss on batch = 16.174884943336366\n",
      "loss on batch = 14.05792336593356\n",
      "loss on batch = 15.44856185499128\n",
      "loss on batch = 15.177144733496498\n",
      "loss on batch = 15.016584742834102\n",
      "loss on batch = 16.11532004157378\n",
      "loss on batch = 14.65330789527762\n",
      "loss on batch = 14.12904631260625\n",
      "loss on batch = 15.689308190230017\n",
      "loss on batch = 14.288358782393466\n",
      "loss on batch = 16.307395676443114\n",
      "loss on batch = 14.274796648381475\n",
      "loss on batch = 14.591787651773725\n",
      "loss on batch = 14.46864117314653\n",
      "loss on batch = 15.573732470393946\n",
      "loss on batch = 13.922219813545276\n",
      "loss on batch = 15.600299547127054\n",
      "loss on batch = 13.962991424129797\n",
      "loss on batch = 16.258709526914252\n",
      "loss on batch = 16.784596300258436\n",
      "loss on batch = 14.663922588165896\n",
      "loss on batch = 15.317645018885415\n",
      "loss on batch = 14.619268154290792\n",
      "loss on batch = 15.073746207704506\n",
      "loss on batch = 14.30929717296214\n",
      "loss on batch = 15.470609661878864\n",
      "loss on batch = 16.23927375376678\n",
      "loss on batch = 14.907694449990167\n",
      "loss on batch = 15.198645695840908\n",
      "loss on batch = 14.892804779360068\n",
      "loss on batch = 15.401391683795646\n",
      "loss on batch = 16.696126866960075\n",
      "loss on batch = 14.039276097228042\n",
      "loss on batch = 16.182628015910073\n",
      "loss on batch = 13.819399471124502\n",
      "loss on batch = 16.48176292710364\n",
      "loss on batch = 16.684716621655884\n",
      "loss on batch = 15.704878577345877\n",
      "loss on batch = 17.051064972021248\n",
      "loss on batch = 15.155418600439086\n",
      "loss on batch = 14.709677955167416\n",
      "loss on batch = 15.440904439443049\n",
      "loss on batch = 16.42116284612688\n",
      "loss on batch = 13.998439156230473\n",
      "loss on batch = 15.57933794092617\n",
      "loss on batch = 15.033267926318624\n",
      "loss on batch = 16.04466770679157\n",
      "loss on batch = 16.788257585751367\n",
      "loss on batch = 17.651925979380888\n",
      "loss on batch = 15.804931858256705\n",
      "loss on batch = 14.053639748789742\n",
      "loss on batch = 15.589380125083306\n",
      "loss on batch = 14.292798126664431\n",
      "loss on batch = 14.768019731400077\n",
      "loss on batch = 15.405524446725824\n",
      "loss on batch = 15.238235921791308\n",
      "loss on batch = 13.895933826344105\n",
      "loss on batch = 16.572871257212057\n",
      "loss on batch = 14.241752494723094\n",
      "loss on batch = 14.245464072199313\n",
      "loss on batch = 15.518619389355724\n",
      "loss on batch = 14.8318160021869\n",
      "loss on batch = 16.77263360457276\n",
      "loss on batch = 14.548184360892776\n",
      "loss on batch = 13.908380378443237\n",
      "loss on batch = 14.759305513674091\n",
      "loss on batch = 14.841224799419194\n",
      "loss on batch = 15.0359398528004\n",
      "loss on batch = 16.80403867727059\n",
      "loss on batch = 15.264658890297621\n",
      "loss on batch = 14.49283873820552\n",
      "loss on batch = 15.012450221486827\n",
      "loss on batch = 16.24963993465339\n",
      "loss on batch = 15.395962772108746\n",
      "loss on batch = 16.179711900307154\n",
      "loss on batch = 14.907415589655324\n",
      "loss on batch = 13.764055810848854\n",
      "loss on batch = 14.819728749663348\n",
      "loss on batch = 14.742290770895957\n",
      "loss on batch = 14.300039189975754\n",
      "loss on batch = 16.09315976381776\n",
      "loss on batch = 14.534820686916104\n",
      "loss on batch = 14.741679414299536\n",
      "loss on batch = 16.55479149224292\n",
      "loss on batch = 16.670689004024673\n",
      "loss on batch = 15.158555739654417\n",
      "loss on batch = 15.881551254373617\n",
      "loss on batch = 13.631518729252917\n",
      "loss on batch = 16.962472337389432\n",
      "loss on batch = 14.057326575524337\n",
      "loss on batch = 13.645059107754365\n",
      "loss on batch = 14.180127940655604\n",
      "loss on batch = 13.759014912821\n",
      "loss on batch = 15.362635439933642\n",
      "loss on batch = 16.202133463104012\n",
      "loss on batch = 15.772698594512658\n",
      "loss on batch = 14.601847785757709\n",
      "loss on batch = 16.04600444891145\n",
      "loss on batch = 14.757052206634972\n",
      "loss on batch = 14.762947492030376\n",
      "loss on batch = 14.968752893353646\n",
      "loss on batch = 16.027824586409086\n",
      "loss on batch = 15.527874479792118\n",
      "loss on batch = 13.866781074704345\n",
      "loss on batch = 16.344767749073377\n",
      "loss on batch = 13.950849653830833\n",
      "loss on batch = 15.08996522470316\n",
      "loss on batch = 15.832696266165101\n",
      "loss on batch = 13.998619416950673\n",
      "loss on batch = 14.253357000885451\n",
      "loss on batch = 14.636253512847956\n",
      "loss on batch = 14.722750394008926\n",
      "loss on batch = 17.706685371236738\n",
      "loss on batch = 14.89902758008849\n",
      "loss on batch = 15.796818617344726\n",
      "loss on batch = 14.653105280047544\n",
      "loss on batch = 17.261257486070388\n",
      "loss on batch = 14.214563884085617\n",
      "loss on batch = 14.520119350014255\n",
      "loss on batch = 14.341618412352767\n",
      "loss on batch = 13.824906833876025\n",
      "loss on batch = 15.199759028272693\n",
      "loss on batch = 15.760929420972442\n",
      "loss on batch = 16.493004972262803\n",
      "loss on batch = 14.319530253341185\n",
      "loss on batch = 15.370796805273834\n",
      "loss on batch = 16.095860672885046\n",
      "loss on batch = 14.644324932804508\n",
      "loss on batch = 14.033153259202152\n",
      "loss on batch = 14.460338499982988\n",
      "loss on batch = 15.033502554402208\n",
      "loss on batch = 14.933610509008712\n",
      "loss on batch = 15.20791105351771\n",
      "loss on batch = 15.403490439235473\n",
      "loss on batch = 14.3366598879799\n",
      "loss on batch = 15.08068241008816\n",
      "loss on batch = 15.2088292808712\n",
      "loss on batch = 13.821009187436722\n",
      "loss on batch = 15.259981845869204\n",
      "loss on batch = 15.244680630870167\n",
      "loss on batch = 15.521500832280172\n",
      "loss on batch = 15.928581654964246\n",
      "loss on batch = 13.95747579922625\n",
      "loss on batch = 14.974457594939512\n",
      "loss on batch = 14.150124394491485\n",
      "loss on batch = 17.155163339410958\n",
      "loss on batch = 14.828735683274871\n",
      "loss on batch = 14.825278389386572\n",
      "loss on batch = 15.699605266238414\n",
      "loss on batch = 16.158476542002184\n",
      "loss on batch = 15.341816303000794\n",
      "loss on batch = 16.20683090181324\n",
      "loss on batch = 15.759315586416555\n",
      "loss on batch = 16.134504918333192\n",
      "loss on batch = 16.312749127626837\n",
      "loss on batch = 14.581572447079477\n",
      "loss on batch = 15.016695239273309\n",
      "loss on batch = 16.64541983530173\n",
      "loss on batch = 15.147989330934045\n",
      "loss on batch = 15.880256051489763\n",
      "loss on batch = 14.20317652521899\n",
      "loss on batch = 14.476604616106984\n",
      "loss on batch = 16.292158624054366\n",
      "loss on batch = 13.18072369527989\n",
      "loss on batch = 15.137363877513256\n",
      "loss on batch = 16.638230310779864\n",
      "loss on batch = 15.295778076608576\n",
      "loss on batch = 16.000026046595366\n",
      "loss on batch = 14.119253069824339\n",
      "loss on batch = 13.812061081844044\n",
      "loss on batch = 17.082711869417395\n",
      "loss on batch = 15.442480409771145\n",
      "loss on batch = 16.384430404124625\n",
      "loss on batch = 16.42490421551357\n",
      "loss on batch = 15.493431206447188\n",
      "loss on batch = 15.82137333064107\n",
      "loss on batch = 16.1339790984382\n",
      "loss on batch = 14.943859896803408\n",
      "loss on batch = 14.916996010919334\n",
      "loss on batch = 16.761474001160067\n",
      "loss on batch = 14.762599801037348\n",
      "loss on batch = 14.630987225360332\n",
      "loss on batch = 14.1380226196499\n",
      "loss on batch = 14.15944171771918\n",
      "loss on batch = 16.68497277107185\n",
      "loss on batch = 15.361900462493908\n",
      "loss on batch = 14.776119877369734\n",
      "loss on batch = 14.857879967825426\n",
      "loss on batch = 16.616427978953332\n",
      "loss on batch = 15.587736346425963\n",
      "loss on batch = 13.94712587356177\n",
      "loss on batch = 15.94804802608036\n",
      "loss on batch = 15.280435545807581\n",
      "loss on batch = 14.912042514041513\n",
      "loss on batch = 14.832428310969128\n",
      "loss on batch = 15.573840061157496\n",
      "loss on batch = 14.562447807357671\n",
      "loss on batch = 15.060896600977726\n",
      "loss on batch = 15.528937094869153\n",
      "loss on batch = 15.062512659786194\n",
      "loss on batch = 14.894161475344786\n",
      "loss on batch = 15.869731827085062\n",
      "loss on batch = 14.989923503970747\n",
      "loss on batch = 13.722722422644177\n",
      "loss on batch = 14.773915587768904\n",
      "loss on batch = 15.564889536242458\n",
      "loss on batch = 15.201770703973112\n",
      "loss on batch = 14.847774029992195\n",
      "loss on batch = 14.925935407296627\n",
      "Acc: 0.9227\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=True,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(196)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(20)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=4, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "= 14.406891529117981\n",
      "loss on batch = 14.95302006970255\n",
      "loss on batch = 15.300358187129243\n",
      "loss on batch = 17.69426803929003\n",
      "loss on batch = 14.597402437119202\n",
      "loss on batch = 15.781744085700597\n",
      "loss on batch = 14.195432317285196\n",
      "loss on batch = 15.587729970056904\n",
      "loss on batch = 14.068831514077369\n",
      "loss on batch = 16.407783652644788\n",
      "loss on batch = 13.937891937307842\n",
      "loss on batch = 15.224056844417632\n",
      "loss on batch = 15.35780900254586\n",
      "loss on batch = 15.748936604877473\n",
      "loss on batch = 17.21923287692341\n",
      "loss on batch = 14.60117521157767\n",
      "loss on batch = 16.673996472750325\n",
      "loss on batch = 15.551216527271297\n",
      "loss on batch = 14.077106883097827\n",
      "loss on batch = 16.37709612467092\n",
      "loss on batch = 14.491297067963416\n",
      "loss on batch = 14.910370480222557\n",
      "loss on batch = 16.16197634713235\n",
      "loss on batch = 15.730191925452498\n",
      "loss on batch = 16.576310029636566\n",
      "loss on batch = 14.357614030938041\n",
      "loss on batch = 14.332433387252816\n",
      "loss on batch = 17.73117824865148\n",
      "loss on batch = 15.751738239881966\n",
      "loss on batch = 15.570256892058516\n",
      "loss on batch = 16.67940156284609\n",
      "loss on batch = 16.43837322246722\n",
      "loss on batch = 14.156649854815118\n",
      "loss on batch = 14.029502449303873\n",
      "loss on batch = 14.270574693087019\n",
      "loss on batch = 15.203610460630824\n",
      "loss on batch = 14.96556361752953\n",
      "loss on batch = 15.833625530955826\n",
      "loss on batch = 15.10891135639553\n",
      "loss on batch = 17.367428948479493\n",
      "loss on batch = 14.849381475809714\n",
      "loss on batch = 15.980558250047551\n",
      "loss on batch = 14.473835049095156\n",
      "loss on batch = 15.963126623221882\n",
      "loss on batch = 16.716085159638148\n",
      "loss on batch = 17.804699549396748\n",
      "loss on batch = 13.598733599899692\n",
      "loss on batch = 16.49525518783289\n",
      "loss on batch = 15.51231580505262\n",
      "loss on batch = 15.462515123679154\n",
      "loss on batch = 16.75850995075012\n",
      "loss on batch = 15.03322581986351\n",
      "loss on batch = 15.288504306223002\n",
      "loss on batch = 16.609709545116157\n",
      "loss on batch = 14.46931938078732\n",
      "loss on batch = 16.362881387939538\n",
      "loss on batch = 14.457939850484209\n",
      "loss on batch = 14.71585414379123\n",
      "loss on batch = 15.331115099624986\n",
      "loss on batch = 15.240106900055816\n",
      "loss on batch = 15.007480751389256\n",
      "loss on batch = 14.833950693150324\n",
      "loss on batch = 15.970754485275158\n",
      "loss on batch = 14.15178203484004\n",
      "loss on batch = 15.62735580227324\n",
      "loss on batch = 14.537885259890484\n",
      "loss on batch = 14.6989772816578\n",
      "loss on batch = 15.449410698931862\n",
      "loss on batch = 15.329204449679105\n",
      "loss on batch = 14.203923902334866\n",
      "loss on batch = 14.146553445303164\n",
      "loss on batch = 15.293991393983502\n",
      "loss on batch = 15.050999576650874\n",
      "loss on batch = 14.891904951057755\n",
      "loss on batch = 15.82936684634328\n",
      "loss on batch = 14.766262729785858\n",
      "loss on batch = 15.02764231993409\n",
      "loss on batch = 15.553013836072243\n",
      "loss on batch = 14.355887366052226\n",
      "loss on batch = 15.994328729649169\n",
      "loss on batch = 15.058169515185828\n",
      "loss on batch = 15.841104574658354\n",
      "loss on batch = 15.038599769634237\n",
      "loss on batch = 15.235298683378769\n",
      "loss on batch = 15.386763629906252\n",
      "loss on batch = 15.638706347086316\n",
      "loss on batch = 15.202658305421842\n",
      "loss on batch = 15.959392578982571\n",
      "loss on batch = 12.655831718504091\n",
      "loss on batch = 15.737717301244459\n",
      "loss on batch = 14.899537843427698\n",
      "loss on batch = 16.761558779145698\n",
      "loss on batch = 15.654897528369268\n",
      "loss on batch = 14.749684623195074\n",
      "loss on batch = 11.745807905620183\n",
      "loss on batch = 18.135945160552552\n",
      "loss on batch = 14.562867587598822\n",
      "loss on batch = 15.146592735892712\n",
      "loss on batch = 15.12158731499583\n",
      "loss on batch = 14.744037873955664\n",
      "loss on batch = 13.562721646961624\n",
      "loss on batch = 15.469064643390999\n",
      "loss on batch = 14.48735049166475\n",
      "loss on batch = 15.044760827128151\n",
      "loss on batch = 16.499861472008583\n",
      "loss on batch = 15.399452337206633\n",
      "loss on batch = 15.176497781412515\n",
      "loss on batch = 14.322217319986617\n",
      "loss on batch = 15.174388693691796\n",
      "loss on batch = 14.885089615544093\n",
      "loss on batch = 15.705467466591369\n",
      "loss on batch = 15.461678718032145\n",
      "loss on batch = 15.88653374095803\n",
      "loss on batch = 15.935306219849139\n",
      "loss on batch = 12.514288183223755\n",
      "loss on batch = 15.98734996543204\n",
      "loss on batch = 16.425348471824297\n",
      "loss on batch = 14.764793673345368\n",
      "loss on batch = 14.463743107674253\n",
      "loss on batch = 16.692035915138735\n",
      "loss on batch = 15.144419275838686\n",
      "loss on batch = 16.436581544199274\n",
      "loss on batch = 14.493663481287093\n",
      "loss on batch = 15.948931860536852\n",
      "loss on batch = 14.306463447234782\n",
      "loss on batch = 18.044612808883258\n",
      "loss on batch = 16.03110156002305\n",
      "loss on batch = 15.648434390558183\n",
      "loss on batch = 13.874167214448638\n",
      "loss on batch = 14.163188850730103\n",
      "loss on batch = 15.6109755943451\n",
      "loss on batch = 15.846104193918551\n",
      "loss on batch = 15.578901726976442\n",
      "loss on batch = 16.19308066435583\n",
      "loss on batch = 15.711740476471125\n",
      "loss on batch = 14.958084708025133\n",
      "loss on batch = 14.936349768435957\n",
      "loss on batch = 15.490041116052447\n",
      "loss on batch = 14.41619575667697\n",
      "loss on batch = 15.652237493557788\n",
      "loss on batch = 15.436841815572194\n",
      "loss on batch = 16.994484189119255\n",
      "loss on batch = 14.788244081829497\n",
      "loss on batch = 14.905863063583258\n",
      "loss on batch = 15.460639722990546\n",
      "loss on batch = 16.577692513469383\n",
      "loss on batch = 15.603201860299823\n",
      "loss on batch = 15.885914215642925\n",
      "loss on batch = 14.295417474608556\n",
      "loss on batch = 16.76212566142036\n",
      "loss on batch = 14.132513828630593\n",
      "loss on batch = 14.320428542938608\n",
      "loss on batch = 13.220973541721305\n",
      "loss on batch = 14.190445126811124\n",
      "loss on batch = 16.92735364926762\n",
      "loss on batch = 15.822496985458901\n",
      "loss on batch = 14.554878548721891\n",
      "loss on batch = 16.006764966447008\n",
      "loss on batch = 15.294270011160407\n",
      "loss on batch = 16.65421422837762\n",
      "loss on batch = 17.254175119263905\n",
      "loss on batch = 16.202434190312378\n",
      "loss on batch = 17.506887326861914\n",
      "loss on batch = 17.489722064621546\n",
      "loss on batch = 18.62800968004309\n",
      "loss on batch = 16.682543163538522\n",
      "loss on batch = 18.10786954597222\n",
      "loss on batch = 15.705867566183326\n",
      "loss on batch = 14.268448763711715\n",
      "loss on batch = 14.398530293405745\n",
      "loss on batch = 14.84669594584072\n",
      "loss on batch = 14.747120230478092\n",
      "loss on batch = 15.708239197122216\n",
      "loss on batch = 14.371982012690829\n",
      "loss on batch = 16.532665826901084\n",
      "loss on batch = 16.97214063492688\n",
      "loss on batch = 16.044741200453085\n",
      "loss on batch = 15.585971086565573\n",
      "loss on batch = 15.73063850539337\n",
      "loss on batch = 15.763184067919727\n",
      "loss on batch = 16.626472063099328\n",
      "loss on batch = 15.4351654382647\n",
      "loss on batch = 16.45374994210568\n",
      "loss on batch = 15.885526314405944\n",
      "loss on batch = 14.250378818295957\n",
      "loss on batch = 15.440046448624672\n",
      "loss on batch = 14.832301476357939\n",
      "loss on batch = 15.47624984979521\n",
      "loss on batch = 14.30462523710732\n",
      "loss on batch = 14.089256834924772\n",
      "loss on batch = 16.055581440071293\n",
      "loss on batch = 14.469008271147773\n",
      "loss on batch = 17.25144782722236\n",
      "loss on batch = 15.90334288865522\n",
      "loss on batch = 14.489533234033143\n",
      "loss on batch = 14.532258372586185\n",
      "loss on batch = 14.661975476741903\n",
      "loss on batch = 17.226386438502107\n",
      "loss on batch = 14.938627862071872\n",
      "loss on batch = 17.782128433838942\n",
      "loss on batch = 16.918058872011713\n",
      "loss on batch = 16.441095235104893\n",
      "loss on batch = 17.87090548074243\n",
      "loss on batch = 14.507943424596945\n",
      "loss on batch = 17.210329556178753\n",
      "loss on batch = 14.949387594365195\n",
      "loss on batch = 15.453193787345144\n",
      "loss on batch = 13.438829462669002\n",
      "loss on batch = 15.554220543583265\n",
      "loss on batch = 14.764947311345631\n",
      "loss on batch = 17.77026612981034\n",
      "loss on batch = 14.892885153711445\n",
      "loss on batch = 15.656420865009835\n",
      "loss on batch = 15.142693399802193\n",
      "loss on batch = 14.391667186265707\n",
      "loss on batch = 15.535461734708088\n",
      "loss on batch = 14.891312448302713\n",
      "loss on batch = 15.53577447742531\n",
      "loss on batch = 15.637995712213167\n",
      "loss on batch = 15.068616469211829\n",
      "loss on batch = 16.595916124214853\n",
      "loss on batch = 15.86605176478344\n",
      "loss on batch = 15.100748201059162\n",
      "loss on batch = 16.314905062381865\n",
      "loss on batch = 15.51516096196269\n",
      "loss on batch = 13.898534622582476\n",
      "loss on batch = 16.32563960788382\n",
      "loss on batch = 15.531419345258584\n",
      "loss on batch = 13.855090658183851\n",
      "loss on batch = 15.725443525059397\n",
      "loss on batch = 15.833193513286231\n",
      "loss on batch = 14.344047276488604\n",
      "loss on batch = 15.843780266876873\n",
      "loss on batch = 13.618673254974276\n",
      "loss on batch = 16.00004372405281\n",
      "loss on batch = 15.201764422778997\n",
      "loss on batch = 16.6510427120734\n",
      "loss on batch = 15.999812986673405\n",
      "loss on batch = 14.905466742931264\n",
      "loss on batch = 14.659821812241429\n",
      "loss on batch = 16.767330242579533\n",
      "loss on batch = 16.308002748180904\n",
      "loss on batch = 15.393236961995614\n",
      "loss on batch = 15.723862872510654\n",
      "loss on batch = 15.548714391735183\n",
      "loss on batch = 16.129951632962552\n",
      "loss on batch = 17.288567588497294\n",
      "loss on batch = 14.438344296655059\n",
      "loss on batch = 16.292889117844\n",
      "loss on batch = 15.603058559778145\n",
      "loss on batch = 14.365289121841768\n",
      "loss on batch = 14.327036484924637\n",
      "loss on batch = 16.849259665810322\n",
      "loss on batch = 15.146787161254977\n",
      "loss on batch = 14.18813087018755\n",
      "loss on batch = 15.72780236594337\n",
      "loss on batch = 16.29337418740466\n",
      "loss on batch = 17.479196342754335\n",
      "loss on batch = 14.792795879427661\n",
      "loss on batch = 16.733587229196374\n",
      "loss on batch = 16.09692225046639\n",
      "loss on batch = 15.692504477271175\n",
      "loss on batch = 16.395207345555956\n",
      "loss on batch = 15.60691621659625\n",
      "loss on batch = 15.360818647681047\n",
      "loss on batch = 15.332323767518838\n",
      "loss on batch = 15.980913460068058\n",
      "loss on batch = 15.667036251676283\n",
      "loss on batch = 14.57689932490209\n",
      "loss on batch = 15.75040032298358\n",
      "loss on batch = 17.04504776745739\n",
      "loss on batch = 16.304978885547555\n",
      "loss on batch = 16.1701128985109\n",
      "loss on batch = 15.742891076204433\n",
      "loss on batch = 15.596814808501549\n",
      "loss on batch = 17.564045183890446\n",
      "loss on batch = 15.226561129650516\n",
      "loss on batch = 16.406209867943364\n",
      "loss on batch = 16.29872366313451\n",
      "loss on batch = 14.264265738590373\n",
      "loss on batch = 14.594255053354495\n",
      "loss on batch = 16.162309373433345\n",
      "loss on batch = 15.72162934075028\n",
      "loss on batch = 15.111979538653221\n",
      "loss on batch = 16.091827938525192\n",
      "loss on batch = 15.82473001833147\n",
      "loss on batch = 16.90206215793388\n",
      "loss on batch = 16.331272572649883\n",
      "loss on batch = 15.370384082693587\n",
      "loss on batch = 16.29416485764842\n",
      "loss on batch = 16.52287305819675\n",
      "loss on batch = 15.32103920524406\n",
      "loss on batch = 14.237304722874743\n",
      "loss on batch = 15.156626131986172\n",
      "loss on batch = 16.238880949332835\n",
      "loss on batch = 14.505256216762811\n",
      "loss on batch = 15.818676649777018\n",
      "loss on batch = 14.655014117310138\n",
      "loss on batch = 17.488824376621498\n",
      "loss on batch = 12.666535087347699\n",
      "loss on batch = 17.06101268280673\n",
      "loss on batch = 12.913706502045205\n",
      "loss on batch = 16.403551769888278\n",
      "loss on batch = 14.987282312892962\n",
      "loss on batch = 15.481727320911833\n",
      "loss on batch = 14.495863389492664\n",
      "loss on batch = 15.92725117387012\n",
      "loss on batch = 14.876164352078352\n",
      "loss on batch = 14.452478521922385\n",
      "loss on batch = 16.156748692293533\n",
      "loss on batch = 16.372179279934915\n",
      "loss on batch = 15.07150759064097\n",
      "loss on batch = 15.461824533917431\n",
      "loss on batch = 15.654499256494486\n",
      "loss on batch = 15.337097370446106\n",
      "loss on batch = 16.075701729204066\n",
      "loss on batch = 14.77076288474015\n",
      "loss on batch = 15.272782316443443\n",
      "loss on batch = 16.353372890325225\n",
      "loss on batch = 15.476001095535434\n",
      "loss on batch = 17.41379712230351\n",
      "loss on batch = 17.352171885899217\n",
      "loss on batch = 18.323624211814188\n",
      "loss on batch = 15.985518945457612\n",
      "loss on batch = 15.236133597129086\n",
      "loss on batch = 15.93798956608343\n",
      "loss on batch = 15.727004239343096\n",
      "loss on batch = 17.186903303022763\n",
      "loss on batch = 16.266932547248874\n",
      "loss on batch = 16.001157647791537\n",
      "loss on batch = 16.01465185561138\n",
      "loss on batch = 14.429926711599965\n",
      "loss on batch = 17.025789642134022\n",
      "loss on batch = 15.015352915457312\n",
      "loss on batch = 13.842308749741292\n",
      "loss on batch = 15.458949055733774\n",
      "loss on batch = 13.30389455204552\n",
      "loss on batch = 15.361683305397161\n",
      "loss on batch = 15.511194922301694\n",
      "loss on batch = 16.6730862571262\n",
      "loss on batch = 15.215677593035341\n",
      "loss on batch = 15.497612209836932\n",
      "loss on batch = 17.81142457947415\n",
      "loss on batch = 16.55058204649144\n",
      "loss on batch = 16.118636530282114\n",
      "loss on batch = 15.470068241388343\n",
      "loss on batch = 16.5267545202436\n",
      "loss on batch = 16.153690854245827\n",
      "loss on batch = 16.34625770353251\n",
      "loss on batch = 14.93983022432976\n",
      "loss on batch = 16.996647117249005\n",
      "loss on batch = 15.44825271206441\n",
      "loss on batch = 15.362813414017424\n",
      "loss on batch = 17.14580418655985\n",
      "loss on batch = 14.854581382369997\n",
      "loss on batch = 16.85750710450321\n",
      "loss on batch = 15.902817969647863\n",
      "loss on batch = 16.034603434233457\n",
      "loss on batch = 16.02835011830253\n",
      "loss on batch = 16.226843613912983\n",
      "loss on batch = 16.250601292166337\n",
      "loss on batch = 16.17732697120496\n",
      "loss on batch = 16.900957201600562\n",
      "loss on batch = 13.113017313757755\n",
      "loss on batch = 14.45065832976473\n",
      "loss on batch = 16.15974960929077\n",
      "loss on batch = 14.587406803176254\n",
      "loss on batch = 14.50301555593842\n",
      "loss on batch = 13.8504514671455\n",
      "loss on batch = 16.451845528002863\n",
      "loss on batch = 15.823259510918783\n",
      "loss on batch = 14.970707419782364\n",
      "loss on batch = 17.034255223106243\n",
      "loss on batch = 16.106004539385275\n",
      "loss on batch = 15.015070214795472\n",
      "loss on batch = 16.58564231580519\n",
      "loss on batch = 15.797070205954583\n",
      "loss on batch = 17.02017408812763\n",
      "loss on batch = 16.74524939669416\n",
      "loss on batch = 14.679334365004475\n",
      "loss on batch = 15.272963092368917\n",
      "loss on batch = 16.084963806050393\n",
      "loss on batch = 15.628193838515946\n",
      "loss on batch = 15.01052991422409\n",
      "loss on batch = 17.12953455599126\n",
      "loss on batch = 17.25789825240883\n",
      "loss on batch = 15.968911424641794\n",
      "loss on batch = 14.901159178808133\n",
      "loss on batch = 15.383791927242406\n",
      "loss on batch = 14.711384965711751\n",
      "loss on batch = 16.78284122315963\n",
      "loss on batch = 13.800919340080466\n",
      "loss on batch = 13.521674032383054\n",
      "loss on batch = 18.003774680667163\n",
      "loss on batch = 16.920099783326023\n",
      "loss on batch = 16.888215313059867\n",
      "loss on batch = 17.16802456357994\n",
      "loss on batch = 16.099786461529185\n",
      "loss on batch = 18.638466023953093\n",
      "loss on batch = 13.795447568886516\n",
      "loss on batch = 17.22480477681282\n",
      "loss on batch = 14.926001345737568\n",
      "loss on batch = 16.125779371493696\n",
      "loss on batch = 16.83750228866124\n",
      "loss on batch = 15.45537836761281\n",
      "loss on batch = 16.704388629844033\n",
      "loss on batch = 17.20369673234677\n",
      "loss on batch = 15.378531702247164\n",
      "loss on batch = 14.989221075201321\n",
      "loss on batch = 16.76307649052179\n",
      "loss on batch = 15.226535682646439\n",
      "loss on batch = 15.413675085519001\n",
      "loss on batch = 14.474829006474417\n",
      "loss on batch = 16.768397520481592\n",
      "loss on batch = 14.654142131100807\n",
      "loss on batch = 16.719065823920886\n",
      "loss on batch = 15.69520429766085\n",
      "loss on batch = 14.129156795845084\n",
      "loss on batch = 13.687836448546522\n",
      "loss on batch = 16.226442393071764\n",
      "loss on batch = 16.054281091725407\n",
      "loss on batch = 14.30951675969698\n",
      "loss on batch = 17.597591123758562\n",
      "loss on batch = 14.494216193130375\n",
      "loss on batch = 18.148504085123047\n",
      "loss on batch = 16.947630841996308\n",
      "loss on batch = 15.513763781594019\n",
      "loss on batch = 17.222153572563663\n",
      "loss on batch = 16.637341822645038\n",
      "loss on batch = 17.76109741950423\n",
      "loss on batch = 16.215635000749213\n",
      "loss on batch = 14.12080257479139\n",
      "loss on batch = 15.994258584594842\n",
      "loss on batch = 15.448197015859138\n",
      "loss on batch = 14.078694908745664\n",
      "loss on batch = 15.81520771011446\n",
      "loss on batch = 14.888513265813497\n",
      "loss on batch = 15.25161286084232\n",
      "loss on batch = 13.404527505337747\n",
      "loss on batch = 18.096742953689308\n",
      "loss on batch = 14.798633354034505\n",
      "loss on batch = 16.102463851056896\n",
      "loss on batch = 15.987967945252118\n",
      "loss on batch = 15.228162225837938\n",
      "loss on batch = 15.387281864270268\n",
      "loss on batch = 16.657747650457917\n",
      "loss on batch = 13.728655176382073\n",
      "loss on batch = 17.157977382596293\n",
      "loss on batch = 15.797903167762431\n",
      "loss on batch = 13.903385309922303\n",
      "loss on batch = 16.684307735307602\n",
      "loss on batch = 16.21578093802981\n",
      "loss on batch = 16.36828671935672\n",
      "loss on batch = 16.62276181667975\n",
      "loss on batch = 15.726484845802693\n",
      "loss on batch = 16.471638071819523\n",
      "loss on batch = 15.712843297560466\n",
      "loss on batch = 15.811097458135006\n",
      "loss on batch = 15.35824494907208\n",
      "loss on batch = 14.230778337135542\n",
      "loss on batch = 16.038994143002988\n",
      "loss on batch = 16.44461944947793\n",
      "loss on batch = 16.7742222157943\n",
      "loss on batch = 15.626819296777352\n",
      "loss on batch = 15.432884514767895\n",
      "loss on batch = 14.832394910415811\n",
      "loss on batch = 15.42622755312132\n",
      "loss on batch = 15.613116424013262\n",
      "loss on batch = 16.950717296943985\n",
      "loss on batch = 15.810791580994014\n",
      "loss on batch = 15.671312019451822\n",
      "loss on batch = 16.68185939089963\n",
      "loss on batch = 15.989744935116573\n",
      "loss on batch = 15.524917483002401\n",
      "loss on batch = 16.78652616532672\n",
      "loss on batch = 16.5072931395434\n",
      "loss on batch = 16.57242111264727\n",
      "loss on batch = 16.29315322705747\n",
      "loss on batch = 16.281498105303164\n",
      "loss on batch = 16.323765910986037\n",
      "loss on batch = 15.626663651918156\n",
      "loss on batch = 16.236967230747997\n",
      "loss on batch = 17.174016610602912\n",
      "loss on batch = 17.560985691965634\n",
      "loss on batch = 15.710999017201924\n",
      "loss on batch = 14.873753441237078\n",
      "loss on batch = 14.61149074178671\n",
      "loss on batch = 13.995524951256218\n",
      "loss on batch = 17.0490443215536\n",
      "loss on batch = 15.21946357135835\n",
      "loss on batch = 16.244530552988852\n",
      "loss on batch = 16.62169642523377\n",
      "loss on batch = 17.108304574402723\n",
      "loss on batch = 14.816478166148487\n",
      "loss on batch = 14.83631903499683\n",
      "loss on batch = 15.757217474143767\n",
      "loss on batch = 15.596845396396674\n",
      "loss on batch = 16.314594978652615\n",
      "loss on batch = 14.665012999227082\n",
      "loss on batch = 14.936571914811749\n",
      "loss on batch = 16.557243387269867\n",
      "loss on batch = 15.519500417959831\n",
      "loss on batch = 14.710320620220774\n",
      "loss on batch = 14.153631591030134\n",
      "loss on batch = 15.283828372555488\n",
      "loss on batch = 13.675101095462573\n",
      "loss on batch = 16.047447774558847\n",
      "loss on batch = 15.522864170654557\n",
      "loss on batch = 17.075251713363667\n",
      "loss on batch = 15.241176770275551\n",
      "loss on batch = 15.734881362164522\n",
      "loss on batch = 13.84020107640275\n",
      "loss on batch = 14.581533901329763\n",
      "loss on batch = 15.124071219758816\n",
      "loss on batch = 16.193805504185168\n",
      "loss on batch = 17.36168538394934\n",
      "loss on batch = 16.15121220064835\n",
      "loss on batch = 16.596278967041677\n",
      "loss on batch = 17.082295472651108\n",
      "loss on batch = 16.77728849518752\n",
      "loss on batch = 16.344303691940922\n",
      "loss on batch = 15.688054523081725\n",
      "loss on batch = 15.129321198286709\n",
      "loss on batch = 15.720914402668477\n",
      "loss on batch = 16.456024875932236\n",
      "loss on batch = 16.55299123040011\n",
      "loss on batch = 15.848156776212466\n",
      "loss on batch = 15.503981605629669\n",
      "loss on batch = 17.216248099131132\n",
      "loss on batch = 14.897281427368004\n",
      "loss on batch = 16.268508912227624\n",
      "loss on batch = 15.287086265663003\n",
      "loss on batch = 13.576697363829783\n",
      "loss on batch = 15.523115848880856\n",
      "loss on batch = 15.86070035269784\n",
      "loss on batch = 15.400143608537661\n",
      "loss on batch = 15.836220481494845\n",
      "loss on batch = 15.994091200361861\n",
      "loss on batch = 15.990495421541382\n",
      "loss on batch = 15.671966554119237\n",
      "loss on batch = 15.888525596012743\n",
      "loss on batch = 15.110616998972782\n",
      "loss on batch = 16.132524399794903\n",
      "loss on batch = 15.247777676442455\n",
      "loss on batch = 15.541871037810168\n",
      "loss on batch = 15.122338459084267\n",
      "loss on batch = 14.756152947288358\n",
      "loss on batch = 15.948517859031702\n",
      "loss on batch = 16.493750063963816\n",
      "loss on batch = 16.44763737259355\n",
      "loss on batch = 16.092927673968028\n",
      "loss on batch = 17.26597621070493\n",
      "loss on batch = 16.82270942314455\n",
      "loss on batch = 14.931045241818623\n",
      "loss on batch = 14.81775689883789\n",
      "loss on batch = 14.930151807246752\n",
      "loss on batch = 16.019253753184163\n",
      "loss on batch = 14.821570312769017\n",
      "loss on batch = 16.5325248721606\n",
      "loss on batch = 15.336383679805595\n",
      "loss on batch = 15.194030814077646\n",
      "loss on batch = 16.31032706212406\n",
      "loss on batch = 15.722379133815362\n",
      "loss on batch = 16.370109514734644\n",
      "loss on batch = 14.506867118028191\n",
      "loss on batch = 16.458942907474267\n",
      "loss on batch = 15.486531618594679\n",
      "loss on batch = 17.249939502428873\n",
      "loss on batch = 15.423485230118493\n",
      "loss on batch = 16.65607651874234\n",
      "loss on batch = 13.84133027132448\n",
      "loss on batch = 15.677514139681449\n",
      "loss on batch = 15.574555838407989\n",
      "loss on batch = 14.696195809419608\n",
      "loss on batch = 14.723101086897584\n",
      "loss on batch = 15.56146650908345\n",
      "Acc: 0.8969\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=True,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(196)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(20)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=4, batch_size=10, lr=.01, method='rmsprop', method_param=0.05)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "batch = 14.737710486484097\n",
      "loss on batch = 14.572355995924783\n",
      "loss on batch = 16.71248643547966\n",
      "loss on batch = 14.686964277758587\n",
      "loss on batch = 15.609203558530487\n",
      "loss on batch = 15.573783844031102\n",
      "loss on batch = 15.55292047743616\n",
      "loss on batch = 15.885508636341271\n",
      "loss on batch = 16.302187393883102\n",
      "loss on batch = 14.334641594692926\n",
      "loss on batch = 16.24737162037089\n",
      "loss on batch = 15.025638456482875\n",
      "loss on batch = 15.15113912825361\n",
      "loss on batch = 16.139289055384097\n",
      "loss on batch = 15.823850532414507\n",
      "loss on batch = 16.928863606296517\n",
      "loss on batch = 14.286548139947353\n",
      "loss on batch = 14.908002416869364\n",
      "loss on batch = 14.806907161709509\n",
      "loss on batch = 14.01795995250917\n",
      "loss on batch = 15.55854639227178\n",
      "loss on batch = 14.654399920634521\n",
      "loss on batch = 16.87156262497567\n",
      "loss on batch = 15.238970362324025\n",
      "loss on batch = 14.456160171820367\n",
      "loss on batch = 15.889465376428845\n",
      "loss on batch = 17.22777744486241\n",
      "loss on batch = 15.476927910932176\n",
      "loss on batch = 15.199254939130542\n",
      "loss on batch = 14.91241741273181\n",
      "loss on batch = 15.49481280994945\n",
      "loss on batch = 14.876674008490834\n",
      "loss on batch = 14.777992056649886\n",
      "loss on batch = 15.018063737274764\n",
      "loss on batch = 15.586162088238876\n",
      "loss on batch = 14.642539900711082\n",
      "loss on batch = 15.57396170330401\n",
      "loss on batch = 14.946994447683366\n",
      "loss on batch = 15.17872383089212\n",
      "loss on batch = 14.485726401823138\n",
      "loss on batch = 14.937849965329974\n",
      "loss on batch = 14.429132516856619\n",
      "loss on batch = 15.091178586715905\n",
      "loss on batch = 16.993661419680997\n",
      "loss on batch = 15.974922774049094\n",
      "loss on batch = 14.326352770610324\n",
      "loss on batch = 15.682581992144515\n",
      "loss on batch = 16.73569101166748\n",
      "loss on batch = 14.862121406383308\n",
      "loss on batch = 16.140126755551087\n",
      "loss on batch = 15.470933046135219\n",
      "loss on batch = 14.863860414134821\n",
      "loss on batch = 15.894627348314582\n",
      "loss on batch = 14.793736926620774\n",
      "loss on batch = 15.847373835872252\n",
      "loss on batch = 14.852096585410667\n",
      "loss on batch = 14.84891145441717\n",
      "loss on batch = 15.228501480432481\n",
      "loss on batch = 14.643996131288215\n",
      "loss on batch = 15.47480686586707\n",
      "loss on batch = 14.879809093930934\n",
      "loss on batch = 16.60537203788561\n",
      "loss on batch = 15.516915011497552\n",
      "loss on batch = 15.894030604536471\n",
      "loss on batch = 15.544175671868121\n",
      "loss on batch = 14.836818788762688\n",
      "loss on batch = 14.300751260438117\n",
      "loss on batch = 15.138813439740801\n",
      "loss on batch = 14.993730483747658\n",
      "loss on batch = 14.480150583961576\n",
      "loss on batch = 16.35072570333157\n",
      "loss on batch = 14.582414412953728\n",
      "loss on batch = 15.617870028826049\n",
      "loss on batch = 14.527509619806747\n",
      "loss on batch = 14.668804923823217\n",
      "loss on batch = 15.71739650969625\n",
      "loss on batch = 15.215609085568943\n",
      "loss on batch = 16.44642884619483\n",
      "loss on batch = 15.367982881731589\n",
      "loss on batch = 16.602020081263642\n",
      "loss on batch = 14.922951225458512\n",
      "loss on batch = 14.562590592686927\n",
      "loss on batch = 15.20526289644935\n",
      "loss on batch = 15.085025888847055\n",
      "loss on batch = 15.997161755044553\n",
      "loss on batch = 15.47476011752141\n",
      "loss on batch = 14.859127850304205\n",
      "loss on batch = 14.03494845297859\n",
      "loss on batch = 14.627451191337293\n",
      "loss on batch = 15.278654210739319\n",
      "loss on batch = 15.26835793670898\n",
      "loss on batch = 16.246691711925035\n",
      "loss on batch = 13.992212869121044\n",
      "loss on batch = 14.575328526796538\n",
      "loss on batch = 14.29493483550949\n",
      "loss on batch = 14.212017164770895\n",
      "loss on batch = 15.075042324985946\n",
      "loss on batch = 14.914810800394228\n",
      "loss on batch = 14.704930792856274\n",
      "loss on batch = 14.369479526192627\n",
      "loss on batch = 15.421569539589951\n",
      "loss on batch = 14.816723401491018\n",
      "loss on batch = 13.980480185593239\n",
      "loss on batch = 15.845697914601063\n",
      "loss on batch = 15.37722100863769\n",
      "loss on batch = 15.278855660386792\n",
      "loss on batch = 15.078427333845578\n",
      "loss on batch = 14.132708273495965\n",
      "loss on batch = 14.93162788232665\n",
      "loss on batch = 17.35866897703763\n",
      "loss on batch = 16.232894534061582\n",
      "loss on batch = 16.178796873311825\n",
      "loss on batch = 16.069413479646762\n",
      "loss on batch = 14.573229504737583\n",
      "loss on batch = 14.144301866891055\n",
      "loss on batch = 17.92250677526075\n",
      "loss on batch = 14.537238862554357\n",
      "loss on batch = 14.519289591747153\n",
      "loss on batch = 15.778790276586001\n",
      "loss on batch = 15.45237140953723\n",
      "loss on batch = 16.663657453262715\n",
      "loss on batch = 16.546411281062934\n",
      "loss on batch = 16.76408325906734\n",
      "loss on batch = 14.156944648092123\n",
      "loss on batch = 14.882001711382648\n",
      "loss on batch = 15.617020949088513\n",
      "loss on batch = 15.353871261624665\n",
      "loss on batch = 14.7673290661054\n",
      "loss on batch = 14.370844450615174\n",
      "loss on batch = 15.809682914126501\n",
      "loss on batch = 15.008320947139262\n",
      "loss on batch = 14.834234786940609\n",
      "loss on batch = 16.20360808323975\n",
      "loss on batch = 14.739327274161969\n",
      "loss on batch = 15.69512376037315\n",
      "loss on batch = 14.948427058448111\n",
      "loss on batch = 14.971796628217348\n",
      "loss on batch = 14.17314469794048\n",
      "loss on batch = 13.994534757624075\n",
      "loss on batch = 16.846693177994208\n",
      "loss on batch = 16.84934336265119\n",
      "loss on batch = 15.084748666590121\n",
      "loss on batch = 15.741370269098843\n",
      "loss on batch = 17.118654263905647\n",
      "loss on batch = 16.48160683624414\n",
      "loss on batch = 14.36300412821825\n",
      "loss on batch = 14.056237084228844\n",
      "loss on batch = 14.759361052116336\n",
      "loss on batch = 15.263818949620795\n",
      "loss on batch = 14.301534548058347\n",
      "loss on batch = 14.601229904797387\n",
      "loss on batch = 14.639660490073776\n",
      "loss on batch = 14.129932429218258\n",
      "loss on batch = 15.331743512961712\n",
      "loss on batch = 14.758534430044215\n",
      "loss on batch = 14.833585907215571\n",
      "loss on batch = 14.034903863238908\n",
      "loss on batch = 15.825626462329781\n",
      "loss on batch = 15.128229330093713\n",
      "loss on batch = 16.97227210717918\n",
      "loss on batch = 15.7543909210548\n",
      "loss on batch = 17.081558312258682\n",
      "loss on batch = 16.479963533159346\n",
      "loss on batch = 16.417768556133307\n",
      "loss on batch = 17.057891424442538\n",
      "loss on batch = 16.51076316585289\n",
      "loss on batch = 14.59678747299893\n",
      "loss on batch = 13.882526298396694\n",
      "loss on batch = 14.244182641431511\n",
      "loss on batch = 17.04616336770576\n",
      "loss on batch = 15.524546102572382\n",
      "loss on batch = 16.06181373608618\n",
      "loss on batch = 15.89759644691367\n",
      "loss on batch = 15.271451992826469\n",
      "loss on batch = 15.424452294973618\n",
      "loss on batch = 15.246470347274357\n",
      "loss on batch = 14.3636868469188\n",
      "loss on batch = 15.286995698865605\n",
      "loss on batch = 15.64243745215763\n",
      "loss on batch = 16.61506413522971\n",
      "loss on batch = 14.786888850725473\n",
      "loss on batch = 14.162274656504916\n",
      "loss on batch = 13.837476735274265\n",
      "loss on batch = 14.786629817298895\n",
      "loss on batch = 15.307744713523345\n",
      "loss on batch = 14.412108956163374\n",
      "loss on batch = 15.702823761601751\n",
      "loss on batch = 15.13259454340326\n",
      "loss on batch = 13.782503716935683\n",
      "loss on batch = 14.166187171729472\n",
      "loss on batch = 15.171809069929857\n",
      "loss on batch = 13.248982327031595\n",
      "loss on batch = 15.379031214132288\n",
      "loss on batch = 14.233859483049866\n",
      "loss on batch = 14.057323164501264\n",
      "loss on batch = 14.461181511763984\n",
      "loss on batch = 15.690269114471338\n",
      "loss on batch = 16.343290508325115\n",
      "loss on batch = 16.049015455544684\n",
      "loss on batch = 16.39871067485386\n",
      "loss on batch = 16.008403549155062\n",
      "loss on batch = 15.718871137687692\n",
      "loss on batch = 15.073326959397711\n",
      "loss on batch = 15.377904633904187\n",
      "loss on batch = 16.92444219976504\n",
      "loss on batch = 16.67136147313916\n",
      "loss on batch = 14.921489599053295\n",
      "loss on batch = 15.309829508802116\n",
      "loss on batch = 14.477833759694391\n",
      "loss on batch = 15.892476995029636\n",
      "loss on batch = 15.748054435399323\n",
      "loss on batch = 14.318161361422124\n",
      "loss on batch = 16.34569992915715\n",
      "loss on batch = 15.310864311453187\n",
      "loss on batch = 15.361561645480638\n",
      "loss on batch = 14.886945698203313\n",
      "loss on batch = 16.478895485063084\n",
      "loss on batch = 14.86771570044669\n",
      "loss on batch = 14.920059086472449\n",
      "loss on batch = 15.376224990882173\n",
      "loss on batch = 16.200634677389736\n",
      "loss on batch = 14.512247152887724\n",
      "loss on batch = 14.263027701111811\n",
      "loss on batch = 14.848888596247411\n",
      "loss on batch = 14.36100078019696\n",
      "loss on batch = 15.170220977160323\n",
      "loss on batch = 15.453510916233615\n",
      "loss on batch = 14.502650713144003\n",
      "loss on batch = 14.762018617297679\n",
      "loss on batch = 14.68487260299016\n",
      "loss on batch = 15.929095522996906\n",
      "loss on batch = 14.896385842074103\n",
      "loss on batch = 15.16572236716138\n",
      "loss on batch = 16.446433599259827\n",
      "loss on batch = 14.794046911639043\n",
      "loss on batch = 14.951427592202736\n",
      "loss on batch = 15.588263516405286\n",
      "loss on batch = 14.937323223941837\n",
      "loss on batch = 14.172616265108765\n",
      "loss on batch = 14.928142784714462\n",
      "loss on batch = 15.133302443632411\n",
      "loss on batch = 14.389158934805137\n",
      "loss on batch = 14.122416222425075\n",
      "loss on batch = 14.966870345595531\n",
      "loss on batch = 14.96852065372063\n",
      "loss on batch = 15.568753830738554\n",
      "loss on batch = 15.206762201155316\n",
      "loss on batch = 17.0854833655602\n",
      "loss on batch = 15.494165524228311\n",
      "loss on batch = 14.783711342283176\n",
      "loss on batch = 15.07625460316557\n",
      "loss on batch = 14.662107754366158\n",
      "loss on batch = 15.586632405691548\n",
      "loss on batch = 13.989225047794474\n",
      "loss on batch = 15.570692932501563\n",
      "loss on batch = 14.751118213612436\n",
      "loss on batch = 15.852813722158785\n",
      "loss on batch = 15.690835309455373\n",
      "loss on batch = 16.246821650962417\n",
      "loss on batch = 14.575505202023205\n",
      "loss on batch = 15.236227967747606\n",
      "loss on batch = 14.949316675885807\n",
      "loss on batch = 14.923027430190258\n",
      "loss on batch = 14.348520177128092\n",
      "loss on batch = 14.830024236468965\n",
      "loss on batch = 15.560213999773495\n",
      "loss on batch = 15.825120637630928\n",
      "loss on batch = 14.263864470876836\n",
      "loss on batch = 14.477422626818967\n",
      "loss on batch = 15.70259586802812\n",
      "loss on batch = 14.911938784079094\n",
      "loss on batch = 14.922471457638657\n",
      "loss on batch = 13.841081666907566\n",
      "loss on batch = 15.922888990021741\n",
      "loss on batch = 14.93283791069934\n",
      "loss on batch = 15.16620672789502\n",
      "loss on batch = 15.678851080677632\n",
      "loss on batch = 15.329036195096771\n",
      "loss on batch = 14.738725901758547\n",
      "loss on batch = 14.59300437712951\n",
      "loss on batch = 15.358344072286727\n",
      "loss on batch = 15.472355177326262\n",
      "loss on batch = 15.316296773953837\n",
      "loss on batch = 15.215957817469949\n",
      "loss on batch = 14.866039643099286\n",
      "loss on batch = 15.598333531673301\n",
      "loss on batch = 15.044127960778578\n",
      "loss on batch = 14.393712067272913\n",
      "loss on batch = 16.67048136853354\n",
      "loss on batch = 17.45210006169576\n",
      "loss on batch = 15.370249290902546\n",
      "loss on batch = 15.8112012943706\n",
      "loss on batch = 14.204388636406152\n",
      "loss on batch = 15.367182296103355\n",
      "loss on batch = 14.761226064089975\n",
      "loss on batch = 16.05858393807712\n",
      "loss on batch = 14.790778306047962\n",
      "loss on batch = 16.17196975338019\n",
      "loss on batch = 14.673536141661641\n",
      "loss on batch = 15.571546141883951\n",
      "loss on batch = 14.514061384893084\n",
      "loss on batch = 15.358058941351947\n",
      "loss on batch = 14.844704876204924\n",
      "loss on batch = 15.2448531783756\n",
      "loss on batch = 14.601368880999246\n",
      "loss on batch = 14.363693666944044\n",
      "loss on batch = 13.590755906429688\n",
      "loss on batch = 14.659472148063884\n",
      "loss on batch = 15.219258880219831\n",
      "loss on batch = 14.856500571904402\n",
      "loss on batch = 15.205610264901829\n",
      "loss on batch = 16.148316064940243\n",
      "loss on batch = 16.33687109219022\n",
      "loss on batch = 15.17709921583501\n",
      "loss on batch = 15.493115411497993\n",
      "loss on batch = 16.22199557468608\n",
      "loss on batch = 15.730173432505467\n",
      "loss on batch = 15.00144948444795\n",
      "loss on batch = 13.834237325045681\n",
      "loss on batch = 14.669537545010016\n",
      "loss on batch = 15.735331867857145\n",
      "loss on batch = 14.848917052292547\n",
      "loss on batch = 14.898183173485315\n",
      "loss on batch = 15.601093332550793\n",
      "loss on batch = 14.637439143287839\n",
      "loss on batch = 15.536146040193863\n",
      "loss on batch = 16.161414461825093\n",
      "loss on batch = 15.71153147570939\n",
      "loss on batch = 14.86834871006377\n",
      "loss on batch = 14.153020904739082\n",
      "loss on batch = 14.081406060826765\n",
      "loss on batch = 16.28614903740553\n",
      "loss on batch = 15.85309254997037\n",
      "loss on batch = 16.23950803186978\n",
      "loss on batch = 14.84090761821244\n",
      "loss on batch = 14.649883208994051\n",
      "loss on batch = 16.304677973150827\n",
      "loss on batch = 16.446706996035406\n",
      "loss on batch = 15.020010503187782\n",
      "loss on batch = 14.266387733406138\n",
      "loss on batch = 14.2588341247219\n",
      "loss on batch = 14.396883489982859\n",
      "loss on batch = 15.543227021053342\n",
      "loss on batch = 15.0323431776529\n",
      "loss on batch = 15.092671968027798\n",
      "loss on batch = 16.035255888029916\n",
      "loss on batch = 14.36251447538973\n",
      "loss on batch = 14.715265782644375\n",
      "loss on batch = 15.972524944335365\n",
      "loss on batch = 15.281162817516979\n",
      "loss on batch = 15.298256437620811\n",
      "loss on batch = 14.371763218638893\n",
      "loss on batch = 14.553811623521046\n",
      "loss on batch = 14.826561869383173\n",
      "loss on batch = 15.673078998431874\n",
      "loss on batch = 15.279457515239935\n",
      "loss on batch = 16.176623824387267\n",
      "loss on batch = 15.211734567480482\n",
      "loss on batch = 15.474725304842234\n",
      "loss on batch = 15.085096866995034\n",
      "loss on batch = 15.4009517246922\n",
      "loss on batch = 14.647140976986888\n",
      "loss on batch = 14.321486596305059\n",
      "loss on batch = 16.372139164999666\n",
      "loss on batch = 14.80809714223627\n",
      "loss on batch = 15.092247624552845\n",
      "loss on batch = 14.725338498569808\n",
      "loss on batch = 13.873307943579086\n",
      "loss on batch = 15.342609638043973\n",
      "loss on batch = 14.45444963885551\n",
      "loss on batch = 15.022026183512633\n",
      "loss on batch = 15.75975663843543\n",
      "loss on batch = 14.217317937130026\n",
      "loss on batch = 15.135823309268652\n",
      "loss on batch = 16.441493994969658\n",
      "loss on batch = 15.89724165045494\n",
      "loss on batch = 17.04051851451686\n",
      "loss on batch = 16.965905132844227\n",
      "loss on batch = 14.452548293518696\n",
      "loss on batch = 14.544545702914007\n",
      "loss on batch = 16.315866525653682\n",
      "loss on batch = 14.314208441230182\n",
      "loss on batch = 14.159372559719648\n",
      "loss on batch = 14.917152700766357\n",
      "loss on batch = 15.892601734582426\n",
      "loss on batch = 14.73758344719854\n",
      "loss on batch = 15.441962795858553\n",
      "loss on batch = 15.340629190732166\n",
      "loss on batch = 15.242382641205811\n",
      "loss on batch = 15.959575223816502\n",
      "loss on batch = 15.914152511004168\n",
      "loss on batch = 14.383514642279899\n",
      "loss on batch = 16.85035804418598\n",
      "loss on batch = 14.764106501581221\n",
      "loss on batch = 15.31238700497675\n",
      "loss on batch = 16.303474135421027\n",
      "loss on batch = 15.683218041247711\n",
      "loss on batch = 15.818946962998844\n",
      "loss on batch = 14.847051130850229\n",
      "loss on batch = 15.37443490250369\n",
      "loss on batch = 15.816030844332818\n",
      "loss on batch = 14.662737729269239\n",
      "loss on batch = 14.593165318257906\n",
      "loss on batch = 15.20251871611837\n",
      "loss on batch = 15.581747939854631\n",
      "loss on batch = 15.548292927077767\n",
      "loss on batch = 15.485471515249202\n",
      "loss on batch = 15.352289387694977\n",
      "loss on batch = 16.630657635406045\n",
      "loss on batch = 14.292121050217895\n",
      "loss on batch = 15.737762356629862\n",
      "loss on batch = 14.864508640700352\n",
      "loss on batch = 17.030074456099825\n",
      "loss on batch = 15.981904999461808\n",
      "loss on batch = 15.83220503013109\n",
      "loss on batch = 16.324499635420867\n",
      "loss on batch = 15.83005643765364\n",
      "loss on batch = 15.729214550612475\n",
      "loss on batch = 15.066102385975773\n",
      "loss on batch = 15.608111795240289\n",
      "loss on batch = 14.678574975447384\n",
      "loss on batch = 15.561701841614132\n",
      "loss on batch = 14.257876006428504\n",
      "loss on batch = 15.93179045479746\n",
      "loss on batch = 15.707425644364797\n",
      "loss on batch = 15.700639250658892\n",
      "loss on batch = 15.202811653766606\n",
      "loss on batch = 13.773591107411246\n",
      "loss on batch = 16.131177719186073\n",
      "loss on batch = 14.940648775773209\n",
      "loss on batch = 14.494828803762456\n",
      "loss on batch = 14.532969785489716\n",
      "loss on batch = 15.533499371421094\n",
      "loss on batch = 14.891396265177873\n",
      "loss on batch = 15.729208393351836\n",
      "loss on batch = 14.628717001852946\n",
      "loss on batch = 14.559164768397306\n",
      "loss on batch = 14.644826723986395\n",
      "loss on batch = 15.62348183876706\n",
      "loss on batch = 14.973111496905744\n",
      "loss on batch = 16.328847118183322\n",
      "loss on batch = 13.944921537870963\n",
      "loss on batch = 14.349983851246254\n",
      "loss on batch = 15.549378694778197\n",
      "loss on batch = 14.76797161926829\n",
      "loss on batch = 16.193124354271035\n",
      "loss on batch = 17.28176478693913\n",
      "loss on batch = 14.881507277502244\n",
      "loss on batch = 15.125124939102832\n",
      "loss on batch = 16.824632597987087\n",
      "loss on batch = 16.443700919944813\n",
      "loss on batch = 15.225806630152732\n",
      "loss on batch = 16.148467823339058\n",
      "loss on batch = 14.642925807572677\n",
      "loss on batch = 16.07213611236356\n",
      "loss on batch = 15.276249505247687\n",
      "loss on batch = 14.755808194876208\n",
      "loss on batch = 15.493644520060277\n",
      "loss on batch = 16.24502213357132\n",
      "loss on batch = 15.813940854750115\n",
      "loss on batch = 15.213360646684553\n",
      "loss on batch = 16.72792323368151\n",
      "loss on batch = 14.375036989141163\n",
      "loss on batch = 16.687530314567837\n",
      "loss on batch = 15.559594478734725\n",
      "loss on batch = 15.310502642252482\n",
      "loss on batch = 15.104826681887037\n",
      "loss on batch = 14.798934991016502\n",
      "loss on batch = 14.871650197468457\n",
      "loss on batch = 14.682802584516025\n",
      "loss on batch = 16.916789669471857\n",
      "loss on batch = 14.861484831398261\n",
      "loss on batch = 15.442843442353617\n",
      "loss on batch = 17.263505033021417\n",
      "loss on batch = 15.34001731266493\n",
      "loss on batch = 14.037818780605816\n",
      "loss on batch = 15.01153981752857\n",
      "loss on batch = 13.796277688134488\n",
      "loss on batch = 14.427434013147568\n",
      "loss on batch = 14.929230750639567\n",
      "loss on batch = 14.82905742772185\n",
      "loss on batch = 14.17009228576686\n",
      "loss on batch = 16.315812298998978\n",
      "loss on batch = 14.924059259096385\n",
      "loss on batch = 14.09331304450309\n",
      "loss on batch = 14.654563819209192\n",
      "loss on batch = 14.626241175023349\n",
      "loss on batch = 14.666563451345445\n",
      "loss on batch = 15.035895704273923\n",
      "loss on batch = 15.532806133889785\n",
      "loss on batch = 15.395026288801816\n",
      "loss on batch = 14.915683878532647\n",
      "loss on batch = 14.988139936616053\n",
      "loss on batch = 14.742555555964305\n",
      "loss on batch = 16.28789117920491\n",
      "loss on batch = 14.328328919582201\n",
      "loss on batch = 16.16549284800601\n",
      "loss on batch = 14.964323537326083\n",
      "loss on batch = 15.672541762040652\n",
      "loss on batch = 15.213497592820293\n",
      "loss on batch = 14.136011874790658\n",
      "loss on batch = 14.835003652458653\n",
      "loss on batch = 15.568595391127989\n",
      "loss on batch = 14.353439264354616\n",
      "loss on batch = 13.940371743753788\n",
      "loss on batch = 15.076117829477154\n",
      "loss on batch = 15.521532405219716\n",
      "loss on batch = 14.924861441470403\n",
      "loss on batch = 14.338042639697527\n",
      "loss on batch = 15.149825301023744\n",
      "loss on batch = 14.171953213481832\n",
      "loss on batch = 16.642436607217157\n",
      "loss on batch = 14.191476600926082\n",
      "loss on batch = 16.215482145375745\n",
      "loss on batch = 15.069295199071647\n",
      "loss on batch = 16.542066022231445\n",
      "loss on batch = 16.614579326446496\n",
      "loss on batch = 15.920635112780568\n",
      "loss on batch = 15.84285317526502\n",
      "loss on batch = 15.519797290637909\n",
      "loss on batch = 15.08960858072425\n",
      "loss on batch = 15.500508905497838\n",
      "loss on batch = 13.980099984179182\n",
      "loss on batch = 16.786591765105356\n",
      "loss on batch = 14.246722578743224\n",
      "loss on batch = 15.736407289567957\n",
      "loss on batch = 14.674601452963909\n",
      "loss on batch = 14.730387214921215\n",
      "loss on batch = 16.055734396271742\n",
      "loss on batch = 16.505372704729442\n",
      "loss on batch = 15.596304494038947\n",
      "loss on batch = 14.594446587487262\n",
      "loss on batch = 15.682988619117042\n",
      "loss on batch = 15.831898385336729\n",
      "loss on batch = 14.371496244149597\n",
      "loss on batch = 14.91440306506184\n",
      "loss on batch = 17.04436778466047\n",
      "loss on batch = 15.149490021108388\n",
      "loss on batch = 15.649703701174655\n",
      "loss on batch = 15.045646822311298\n",
      "loss on batch = 15.54201596767463\n",
      "loss on batch = 14.707345496435186\n",
      "loss on batch = 14.758610023483802\n",
      "loss on batch = 15.226217329092506\n",
      "loss on batch = 15.306738213290842\n",
      "loss on batch = 15.097552400719945\n",
      "loss on batch = 14.030150873849323\n",
      "loss on batch = 14.856073228904137\n",
      "loss on batch = 14.774666455702617\n",
      "loss on batch = 15.032136005407473\n",
      "loss on batch = 16.57921512505439\n",
      "loss on batch = 16.22103489938769\n",
      "loss on batch = 15.291656924893703\n",
      "loss on batch = 15.210903245306756\n",
      "loss on batch = 15.976109377720041\n",
      "loss on batch = 15.90936802134709\n",
      "loss on batch = 14.399047431847682\n",
      "loss on batch = 17.12147511863149\n",
      "loss on batch = 15.088969568029308\n",
      "loss on batch = 14.161949642529326\n",
      "loss on batch = 14.3288662059602\n",
      "loss on batch = 15.623942351698298\n",
      "loss on batch = 14.282821729181782\n",
      "loss on batch = 15.954593502080172\n",
      "loss on batch = 14.739086116215322\n",
      "loss on batch = 15.075117654495177\n",
      "loss on batch = 15.482124677734145\n",
      "loss on batch = 15.143367074633364\n",
      "loss on batch = 14.611848291486838\n",
      "loss on batch = 15.013672714255065\n",
      "loss on batch = 14.644972937961047\n",
      "loss on batch = 14.905411660217421\n",
      "loss on batch = 14.831856647087765\n",
      "loss on batch = 14.607007792849627\n",
      "loss on batch = 14.795444977135109\n",
      "Acc: 0.9122\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=True,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(196)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(20)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=4, batch_size=10, lr=.01, method='rmsprop', method_param=0.15)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9172\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(196)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(20)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.917\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(196)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(20)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=4, batch_size=10, lr=.012, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9176\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(80)\n",
    "nn.add_layer(25)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9092\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(196)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(20)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=3, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9197\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(150)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(20)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=3, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9151\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(210)\n",
    "nn.add_layer(55)\n",
    "nn.add_layer(20)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=4, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9289\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(196)\n",
    "nn.add_layer(49)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=4, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9209\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(150)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(20)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=4, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9299\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(150)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=4, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9238\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(300)\n",
    "nn.add_layer(150)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=1, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9318\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.939\n"
     ]
    }
   ],
   "source": [
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-1-adb9f669f150>:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (np.exp(-x) + 1)\n",
      "Acc: 0.9349\n"
     ]
    }
   ],
   "source": [
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=1, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9241\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.08)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.08)\n",
    "pred = nn.predict(test_X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9281\n"
     ]
    }
   ],
   "source": [
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9113\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(200)\n",
    "nn.add_layer(80)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-1-adb9f669f150>:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (np.exp(-x) + 1)\n",
      "Acc: 0.9204\n"
     ]
    }
   ],
   "source": [
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'NeuralNetwork' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c455903d9908>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mactivation_function\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'sigma'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Multiple layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NeuralNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_std, train_y_onehot, epochs=10, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_std)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.938\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.8641\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_centered, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_centered)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "36020495\n",
      "loss on batch = 14.800520751524518\n",
      "loss on batch = 15.485943604457766\n",
      "loss on batch = 14.805754016936326\n",
      "loss on batch = 14.570986591256123\n",
      "loss on batch = 15.16971585660474\n",
      "loss on batch = 14.743118910294722\n",
      "loss on batch = 14.802508456159797\n",
      "loss on batch = 14.655103203596529\n",
      "loss on batch = 15.325407061266771\n",
      "loss on batch = 15.442947181789567\n",
      "loss on batch = 16.408109528406193\n",
      "loss on batch = 14.789963717499933\n",
      "loss on batch = 14.447969771099363\n",
      "loss on batch = 14.413868444254728\n",
      "loss on batch = 15.343247497289711\n",
      "loss on batch = 14.581568431356363\n",
      "loss on batch = 14.761743265518426\n",
      "loss on batch = 14.261780980025526\n",
      "loss on batch = 14.736093177137631\n",
      "loss on batch = 15.822203793903427\n",
      "loss on batch = 16.275937712299424\n",
      "loss on batch = 14.578667105935077\n",
      "loss on batch = 14.860697387030195\n",
      "loss on batch = 16.072671336170416\n",
      "loss on batch = 15.622248405004381\n",
      "loss on batch = 15.948167799667313\n",
      "loss on batch = 14.927369491231529\n",
      "loss on batch = 14.371516574726556\n",
      "loss on batch = 14.635673531911937\n",
      "loss on batch = 16.409439676675852\n",
      "loss on batch = 14.78064376179973\n",
      "loss on batch = 15.520448396828995\n",
      "loss on batch = 16.707687401361042\n",
      "loss on batch = 15.884889620203023\n",
      "loss on batch = 15.282727447097372\n",
      "loss on batch = 14.487827903687613\n",
      "loss on batch = 14.613020359180558\n",
      "loss on batch = 15.185501884486083\n",
      "loss on batch = 15.673974959394801\n",
      "loss on batch = 15.41159278158354\n",
      "loss on batch = 14.919476028140714\n",
      "loss on batch = 15.488412432809659\n",
      "loss on batch = 15.87827238211211\n",
      "loss on batch = 15.304045917054061\n",
      "loss on batch = 16.810310196302712\n",
      "loss on batch = 14.646868203837947\n",
      "loss on batch = 15.28468332482677\n",
      "loss on batch = 15.406364366749646\n",
      "loss on batch = 14.53641849176623\n",
      "loss on batch = 15.29552882399075\n",
      "loss on batch = 14.470574238374695\n",
      "loss on batch = 14.730764188272438\n",
      "loss on batch = 16.836614876447356\n",
      "loss on batch = 16.48730708199636\n",
      "loss on batch = 14.672181604721768\n",
      "loss on batch = 14.773179720282346\n",
      "loss on batch = 14.497291648727058\n",
      "loss on batch = 15.647140022043912\n",
      "loss on batch = 15.546201785916892\n",
      "loss on batch = 14.959918629959418\n",
      "loss on batch = 14.619648330240317\n",
      "loss on batch = 14.402840733217372\n",
      "loss on batch = 15.368029572272915\n",
      "loss on batch = 14.669549158572181\n",
      "loss on batch = 14.647133575808295\n",
      "loss on batch = 15.868972891165452\n",
      "loss on batch = 15.569510931912992\n",
      "loss on batch = 15.72595298823119\n",
      "loss on batch = 14.776149543867755\n",
      "loss on batch = 14.402246662684792\n",
      "loss on batch = 15.605504323804015\n",
      "loss on batch = 14.095518166419502\n",
      "loss on batch = 15.544250590732796\n",
      "loss on batch = 14.77871210561571\n",
      "loss on batch = 14.372017873519807\n",
      "loss on batch = 16.400562987305705\n",
      "loss on batch = 15.396640824139409\n",
      "loss on batch = 15.475843634541555\n",
      "loss on batch = 15.303965179504994\n",
      "loss on batch = 15.463978906044337\n",
      "loss on batch = 15.755514997143047\n",
      "loss on batch = 14.657063159436753\n",
      "loss on batch = 14.567516050703682\n",
      "loss on batch = 14.813680417971115\n",
      "loss on batch = 15.43527895419216\n",
      "loss on batch = 15.595203242237286\n",
      "loss on batch = 14.87930390221236\n",
      "loss on batch = 14.476069258347893\n",
      "loss on batch = 15.781097111918658\n",
      "loss on batch = 15.299045305217682\n",
      "loss on batch = 14.38917877227519\n",
      "loss on batch = 16.222392606933653\n",
      "loss on batch = 14.276826529309444\n",
      "loss on batch = 15.625793281320068\n",
      "loss on batch = 14.668577786521976\n",
      "loss on batch = 15.65815987748983\n",
      "loss on batch = 14.746477608914645\n",
      "loss on batch = 15.756540279890014\n",
      "loss on batch = 15.479145977812333\n",
      "loss on batch = 15.827629182471156\n",
      "loss on batch = 15.528756590817657\n",
      "loss on batch = 14.812450127065505\n",
      "loss on batch = 15.064403116639989\n",
      "loss on batch = 15.425928246226437\n",
      "loss on batch = 15.576106266880883\n",
      "loss on batch = 14.522387016830917\n",
      "loss on batch = 17.127520154746506\n",
      "loss on batch = 16.165191447318506\n",
      "loss on batch = 15.3604990148084\n",
      "loss on batch = 15.493170833454824\n",
      "loss on batch = 16.334921844319037\n",
      "loss on batch = 14.575160269474537\n",
      "loss on batch = 14.58909346690933\n",
      "loss on batch = 13.965867502902526\n",
      "loss on batch = 14.682554156890763\n",
      "loss on batch = 14.335862731358997\n",
      "loss on batch = 15.410345029488939\n",
      "loss on batch = 15.401924325724064\n",
      "loss on batch = 14.486437063603026\n",
      "loss on batch = 14.666958996353438\n",
      "loss on batch = 15.376771981577344\n",
      "loss on batch = 14.581909385490771\n",
      "loss on batch = 15.465656735174296\n",
      "loss on batch = 15.585417268266294\n",
      "loss on batch = 15.291284580451787\n",
      "loss on batch = 15.451560011994687\n",
      "loss on batch = 14.433605086273687\n",
      "loss on batch = 16.4733573473777\n",
      "loss on batch = 15.451900713085985\n",
      "loss on batch = 15.717531446937942\n",
      "loss on batch = 15.237033349547996\n",
      "loss on batch = 14.922148395235434\n",
      "loss on batch = 14.462562318617586\n",
      "loss on batch = 14.556099299027595\n",
      "loss on batch = 14.946923244323315\n",
      "loss on batch = 15.346541755286854\n",
      "loss on batch = 15.174961665268922\n",
      "loss on batch = 14.737785436488766\n",
      "loss on batch = 15.503427765957955\n",
      "loss on batch = 14.383629408865898\n",
      "loss on batch = 16.567991272426955\n",
      "loss on batch = 14.724487527075127\n",
      "loss on batch = 15.162148991575172\n",
      "loss on batch = 14.805477363207165\n",
      "loss on batch = 15.326267182554947\n",
      "loss on batch = 15.267469874890098\n",
      "loss on batch = 14.799854261527571\n",
      "loss on batch = 14.800335034889851\n",
      "loss on batch = 14.615017334305618\n",
      "loss on batch = 15.615197850730747\n",
      "loss on batch = 14.576742099910467\n",
      "loss on batch = 15.35918337051329\n",
      "loss on batch = 14.822379523464384\n",
      "loss on batch = 16.255545830302196\n",
      "loss on batch = 15.42331236789774\n",
      "loss on batch = 14.567350499762624\n",
      "loss on batch = 16.205777707272844\n",
      "loss on batch = 14.717064512429442\n",
      "loss on batch = 15.738093963536244\n",
      "loss on batch = 14.446362974169228\n",
      "loss on batch = 14.406990413288685\n",
      "loss on batch = 15.582966472580438\n",
      "loss on batch = 14.572309299354558\n",
      "loss on batch = 14.658257541783678\n",
      "loss on batch = 14.650566740386614\n",
      "loss on batch = 15.207861054805349\n",
      "loss on batch = 14.370203327897645\n",
      "loss on batch = 15.095443417088076\n",
      "loss on batch = 15.457462547892236\n",
      "loss on batch = 14.525476431717037\n",
      "loss on batch = 14.767355060932099\n",
      "loss on batch = 15.095535631063061\n",
      "loss on batch = 14.608000017543741\n",
      "loss on batch = 14.791499374722695\n",
      "loss on batch = 14.464340514230996\n",
      "loss on batch = 14.839421306220704\n",
      "loss on batch = 16.37611732143526\n",
      "loss on batch = 15.163106944081722\n",
      "loss on batch = 16.256568505029612\n",
      "loss on batch = 15.270249146110277\n",
      "loss on batch = 14.579852982992204\n",
      "loss on batch = 14.660478601540202\n",
      "loss on batch = 14.664208011630905\n",
      "loss on batch = 16.51550632464563\n",
      "loss on batch = 15.461419037710478\n",
      "loss on batch = 14.769513256735287\n",
      "loss on batch = 14.678019557655327\n",
      "loss on batch = 14.743202118286101\n",
      "loss on batch = 15.643993028934176\n",
      "loss on batch = 15.533286514976087\n",
      "loss on batch = 15.347748668273177\n",
      "loss on batch = 15.231521090597106\n",
      "loss on batch = 14.464398218144186\n",
      "loss on batch = 14.588682479264243\n",
      "loss on batch = 14.514721587875547\n",
      "loss on batch = 16.46607961162401\n",
      "loss on batch = 14.6214444288286\n",
      "loss on batch = 15.391703341516859\n",
      "loss on batch = 14.990928340300615\n",
      "loss on batch = 17.288498844689954\n",
      "loss on batch = 14.447915930075434\n",
      "loss on batch = 15.469643973264702\n",
      "loss on batch = 15.51222366503506\n",
      "loss on batch = 16.566563048408742\n",
      "loss on batch = 15.811268982286025\n",
      "loss on batch = 15.734900366770441\n",
      "loss on batch = 14.654452947310093\n",
      "loss on batch = 16.453389750583817\n",
      "loss on batch = 14.706556227854243\n",
      "loss on batch = 15.478254575648325\n",
      "loss on batch = 14.642669452339858\n",
      "loss on batch = 14.62559702983727\n",
      "loss on batch = 14.430034719949111\n",
      "loss on batch = 15.13882988540238\n",
      "loss on batch = 15.439116353582225\n",
      "loss on batch = 14.895996263551593\n",
      "loss on batch = 14.535201115523163\n",
      "loss on batch = 14.352585129879744\n",
      "loss on batch = 14.810612205324656\n",
      "loss on batch = 14.448647868699457\n",
      "loss on batch = 15.44392081922462\n",
      "loss on batch = 14.76032849784211\n",
      "loss on batch = 15.550998116615208\n",
      "loss on batch = 16.024833158610555\n",
      "loss on batch = 15.327543885490462\n",
      "loss on batch = 14.351886300771207\n",
      "loss on batch = 14.567887153690279\n",
      "loss on batch = 14.741794778852306\n",
      "loss on batch = 14.537865950003402\n",
      "loss on batch = 14.655688007896998\n",
      "loss on batch = 14.865305363776073\n",
      "loss on batch = 15.163194666461177\n",
      "loss on batch = 14.762996546538094\n",
      "loss on batch = 14.670214122613014\n",
      "loss on batch = 15.471786468122506\n",
      "loss on batch = 16.580184688736544\n",
      "loss on batch = 14.664747442097857\n",
      "loss on batch = 14.586445062743016\n",
      "loss on batch = 15.793804043573513\n",
      "loss on batch = 15.96081062327443\n",
      "loss on batch = 14.704019996652049\n",
      "loss on batch = 15.906464241162343\n",
      "loss on batch = 14.87703449473289\n",
      "loss on batch = 15.520837793444981\n",
      "loss on batch = 14.679201930819127\n",
      "loss on batch = 14.654997038282406\n",
      "loss on batch = 14.292733690634215\n",
      "loss on batch = 14.687514790726228\n",
      "loss on batch = 14.552300917022256\n",
      "loss on batch = 16.30369789044063\n",
      "loss on batch = 16.89545684996037\n",
      "loss on batch = 15.729982518923716\n",
      "loss on batch = 14.604749445196685\n",
      "loss on batch = 15.337637924116223\n",
      "loss on batch = 15.214291284750567\n",
      "loss on batch = 14.896157701489551\n",
      "loss on batch = 14.493676360152403\n",
      "loss on batch = 15.600495705643617\n",
      "loss on batch = 15.930674634005236\n",
      "loss on batch = 15.406848230337543\n",
      "loss on batch = 15.830082221554624\n",
      "loss on batch = 14.45668008903079\n",
      "loss on batch = 14.726708064875572\n",
      "loss on batch = 14.717872048930154\n",
      "loss on batch = 14.633945968502987\n",
      "loss on batch = 14.60189412232978\n",
      "loss on batch = 15.93618991030389\n",
      "loss on batch = 15.71884632124053\n",
      "loss on batch = 15.250905315141463\n",
      "loss on batch = 14.52708499421085\n",
      "loss on batch = 16.04972026663243\n",
      "loss on batch = 14.779272393491048\n",
      "loss on batch = 14.860198396270572\n",
      "loss on batch = 14.586767767862067\n",
      "loss on batch = 14.303287011254135\n",
      "loss on batch = 14.736516260217805\n",
      "loss on batch = 16.482112561331153\n",
      "loss on batch = 15.343862831896136\n",
      "loss on batch = 14.449115406375928\n",
      "loss on batch = 15.633763945299318\n",
      "loss on batch = 14.95870796648358\n",
      "loss on batch = 15.330401656253601\n",
      "loss on batch = 16.193293821530673\n",
      "loss on batch = 14.464385485689645\n",
      "loss on batch = 14.272914209781256\n",
      "loss on batch = 15.509143755787633\n",
      "loss on batch = 15.752498155600621\n",
      "loss on batch = 15.442120144057935\n",
      "loss on batch = 14.999933377462689\n",
      "loss on batch = 15.012770033647854\n",
      "loss on batch = 15.69626865604311\n",
      "loss on batch = 15.654291580199178\n",
      "loss on batch = 16.267458608945127\n",
      "loss on batch = 15.51678023303295\n",
      "loss on batch = 16.229629835444037\n",
      "loss on batch = 16.26209777184766\n",
      "loss on batch = 15.359669034378214\n",
      "loss on batch = 14.202418523209783\n",
      "loss on batch = 14.98071044582042\n",
      "loss on batch = 15.303852520606533\n",
      "loss on batch = 17.408349894493732\n",
      "loss on batch = 15.712387680010673\n",
      "loss on batch = 14.497788721357416\n",
      "loss on batch = 14.82331195441565\n",
      "loss on batch = 14.56266532818663\n",
      "loss on batch = 14.291193525840542\n",
      "loss on batch = 14.832863322059215\n",
      "loss on batch = 15.38636407309682\n",
      "loss on batch = 14.780051366951698\n",
      "loss on batch = 14.492639503309213\n",
      "loss on batch = 14.633640810845991\n",
      "loss on batch = 15.455812927608187\n",
      "loss on batch = 15.81395778485107\n",
      "loss on batch = 14.8220092418855\n",
      "loss on batch = 15.073066977544913\n",
      "loss on batch = 14.566185807093296\n",
      "loss on batch = 15.547924851758623\n",
      "loss on batch = 15.333127456627256\n",
      "loss on batch = 14.473172755771706\n",
      "loss on batch = 14.865763185117912\n",
      "loss on batch = 16.328257636013433\n",
      "loss on batch = 15.137427904451153\n",
      "loss on batch = 14.477944803321073\n",
      "loss on batch = 15.501076560414187\n",
      "loss on batch = 14.740234974360007\n",
      "loss on batch = 14.6694215342839\n",
      "loss on batch = 14.521832428929672\n",
      "loss on batch = 14.55251746225603\n",
      "loss on batch = 14.504671848480301\n",
      "loss on batch = 16.631811045612697\n",
      "loss on batch = 15.391970498771148\n",
      "loss on batch = 16.32677174851618\n",
      "loss on batch = 15.418451941165682\n",
      "loss on batch = 14.466956881156564\n",
      "loss on batch = 14.94952404463431\n",
      "loss on batch = 14.837157373346617\n",
      "loss on batch = 15.389190384531494\n",
      "loss on batch = 15.331477610047129\n",
      "loss on batch = 14.9390304508957\n",
      "loss on batch = 14.728842468746752\n",
      "loss on batch = 14.351822261239452\n",
      "loss on batch = 15.561882238784536\n",
      "loss on batch = 15.872932139087558\n",
      "loss on batch = 15.49557786717216\n",
      "loss on batch = 14.447012684431488\n",
      "loss on batch = 15.285587846320427\n",
      "loss on batch = 14.613338537542258\n",
      "loss on batch = 15.460936695137967\n",
      "loss on batch = 14.914227248207855\n",
      "loss on batch = 15.435435527703847\n",
      "loss on batch = 14.937809192190326\n",
      "loss on batch = 16.288816513016734\n",
      "loss on batch = 16.082252257125297\n",
      "loss on batch = 15.01218784788273\n",
      "loss on batch = 15.518258624482915\n",
      "loss on batch = 15.11631921138676\n",
      "loss on batch = 15.555248970797638\n",
      "loss on batch = 14.89765501607844\n",
      "loss on batch = 14.264311992467139\n",
      "loss on batch = 15.645068363837705\n",
      "loss on batch = 14.56020368766084\n",
      "loss on batch = 14.72327255334868\n",
      "loss on batch = 15.291194296877347\n",
      "loss on batch = 15.01059728805198\n",
      "loss on batch = 15.006488793405302\n",
      "loss on batch = 15.244372821258477\n",
      "loss on batch = 15.42471548802325\n",
      "loss on batch = 14.533826473208137\n",
      "loss on batch = 14.646299454256193\n",
      "loss on batch = 14.718003970726386\n",
      "loss on batch = 15.723520890404854\n",
      "loss on batch = 14.57317738594184\n",
      "loss on batch = 15.067820677396702\n",
      "loss on batch = 15.791942802357052\n",
      "loss on batch = 14.55621491146459\n",
      "loss on batch = 14.325834661212562\n",
      "loss on batch = 15.133773866864242\n",
      "loss on batch = 15.33554421717362\n",
      "loss on batch = 17.268863330175506\n",
      "loss on batch = 14.714634909641367\n",
      "loss on batch = 15.505983612711471\n",
      "loss on batch = 15.075447254928918\n",
      "loss on batch = 15.552049483062495\n",
      "loss on batch = 14.452957450857161\n",
      "loss on batch = 15.033089250146872\n",
      "loss on batch = 17.4064336478368\n",
      "loss on batch = 15.498873736137869\n",
      "loss on batch = 16.365371660382486\n",
      "loss on batch = 14.748948476275075\n",
      "loss on batch = 14.56746136376355\n",
      "loss on batch = 14.511772978910702\n",
      "loss on batch = 15.995606950870208\n",
      "loss on batch = 15.333355668361522\n",
      "loss on batch = 14.851367371589841\n",
      "loss on batch = 16.124078897571096\n",
      "loss on batch = 14.714040258278077\n",
      "loss on batch = 16.708652975070503\n",
      "loss on batch = 16.89098016337072\n",
      "loss on batch = 14.381416425359394\n",
      "loss on batch = 15.3216274913263\n",
      "loss on batch = 15.563407132322551\n",
      "loss on batch = 14.90661073865947\n",
      "loss on batch = 15.451030152629286\n",
      "loss on batch = 14.678389315239379\n",
      "loss on batch = 15.997060945984318\n",
      "loss on batch = 14.860542485709512\n",
      "loss on batch = 15.221081383654262\n",
      "loss on batch = 14.8267925092937\n",
      "loss on batch = 15.405028983578308\n",
      "loss on batch = 14.969802016016033\n",
      "loss on batch = 14.583362322672075\n",
      "loss on batch = 15.548613161431359\n",
      "loss on batch = 14.68117427501115\n",
      "loss on batch = 15.746144671183728\n",
      "loss on batch = 14.55831497981068\n",
      "loss on batch = 15.319010834414414\n",
      "loss on batch = 14.650936294850187\n",
      "loss on batch = 15.510336901817997\n",
      "loss on batch = 16.3102079039259\n",
      "loss on batch = 14.760393954007784\n",
      "loss on batch = 14.3810156454355\n",
      "loss on batch = 15.574199317072312\n",
      "loss on batch = 15.441005860072499\n",
      "loss on batch = 14.409972943335525\n",
      "loss on batch = 15.352276857369805\n",
      "loss on batch = 14.284339588782203\n",
      "loss on batch = 14.642561107878889\n",
      "loss on batch = 14.61159186755118\n",
      "loss on batch = 14.724334296314833\n",
      "loss on batch = 14.62549661479676\n",
      "loss on batch = 14.881746250109378\n",
      "loss on batch = 14.335418582504488\n",
      "loss on batch = 15.182692776295568\n",
      "loss on batch = 16.03050374639531\n",
      "loss on batch = 17.552905274742027\n",
      "loss on batch = 15.257456662284842\n",
      "loss on batch = 14.567142469927214\n",
      "loss on batch = 14.533620766483912\n",
      "loss on batch = 14.974410391310544\n",
      "loss on batch = 15.311194413482134\n",
      "loss on batch = 14.14942242636776\n",
      "loss on batch = 16.490133795635494\n",
      "loss on batch = 15.677713566333916\n",
      "loss on batch = 14.662025674253243\n",
      "loss on batch = 14.23852003784786\n",
      "loss on batch = 14.699089986458546\n",
      "loss on batch = 15.143562704877777\n",
      "loss on batch = 14.582876202965817\n",
      "loss on batch = 14.722106044542109\n",
      "loss on batch = 15.8373500183138\n",
      "loss on batch = 14.555812169663021\n",
      "loss on batch = 14.457723565917473\n",
      "loss on batch = 15.511596513153874\n",
      "loss on batch = 14.618374130607904\n",
      "loss on batch = 14.364899014393428\n",
      "loss on batch = 14.633508406618487\n",
      "loss on batch = 16.38426339598465\n",
      "loss on batch = 15.49461787768598\n",
      "loss on batch = 14.570303256908343\n",
      "loss on batch = 14.836204216264486\n",
      "loss on batch = 14.620537323138397\n",
      "loss on batch = 14.494229946871373\n",
      "loss on batch = 16.082645884790875\n",
      "loss on batch = 16.359510850503128\n",
      "loss on batch = 14.834800129635445\n",
      "loss on batch = 14.278397376821717\n",
      "loss on batch = 14.242395362068793\n",
      "loss on batch = 14.995155698829551\n",
      "loss on batch = 15.412312277318495\n",
      "loss on batch = 14.913329883055717\n",
      "loss on batch = 15.687310669118876\n",
      "loss on batch = 14.622252877911833\n",
      "loss on batch = 15.718710905593483\n",
      "loss on batch = 14.975334745150516\n",
      "loss on batch = 14.71152316590731\n",
      "loss on batch = 15.697781926152516\n",
      "loss on batch = 14.581289257140877\n",
      "loss on batch = 14.568363780180976\n",
      "loss on batch = 14.393792509170634\n",
      "loss on batch = 14.744361565120323\n",
      "loss on batch = 14.453815396855628\n",
      "loss on batch = 16.24803001499154\n",
      "loss on batch = 15.245413936037227\n",
      "loss on batch = 15.469843243589569\n",
      "loss on batch = 14.93748401318997\n",
      "loss on batch = 15.030494162414952\n",
      "loss on batch = 14.61698020416518\n",
      "loss on batch = 15.531606250747712\n",
      "loss on batch = 14.177713568686672\n",
      "loss on batch = 15.284641300257537\n",
      "loss on batch = 15.071477046067566\n",
      "loss on batch = 14.70692573570778\n",
      "loss on batch = 15.945700251383972\n",
      "loss on batch = 15.16569388063157\n",
      "loss on batch = 14.812964197349437\n",
      "loss on batch = 14.901115349452347\n",
      "loss on batch = 14.781118048404892\n",
      "loss on batch = 15.470574433065007\n",
      "loss on batch = 14.306775484272507\n",
      "loss on batch = 14.426844993129677\n",
      "loss on batch = 14.919976152719865\n",
      "loss on batch = 14.549819988767226\n",
      "loss on batch = 14.619932846777775\n",
      "loss on batch = 15.330124809372451\n",
      "loss on batch = 15.322590574026027\n",
      "loss on batch = 15.352461917753121\n",
      "loss on batch = 15.211029481543164\n",
      "loss on batch = 14.713232900425778\n",
      "loss on batch = 14.941456917293024\n",
      "loss on batch = 14.756653741670299\n",
      "loss on batch = 14.337979782878296\n",
      "loss on batch = 15.5665819830335\n",
      "loss on batch = 15.4508360186755\n",
      "loss on batch = 14.72509816412407\n",
      "loss on batch = 14.540010132673975\n",
      "loss on batch = 14.649412759788735\n",
      "loss on batch = 14.75646676064218\n",
      "loss on batch = 16.444170124836887\n",
      "loss on batch = 14.702044602195697\n",
      "loss on batch = 14.607981205144412\n",
      "loss on batch = 14.666260512994587\n",
      "loss on batch = 14.255597161877223\n",
      "loss on batch = 16.474710175208177\n",
      "loss on batch = 15.333467078862164\n",
      "loss on batch = 16.237599597933603\n",
      "loss on batch = 15.378065720000423\n",
      "loss on batch = 14.536023065564354\n",
      "loss on batch = 14.284419657109282\n",
      "loss on batch = 15.488595227814159\n",
      "loss on batch = 14.837392776853111\n",
      "loss on batch = 15.658861008337254\n",
      "loss on batch = 14.617563501120749\n",
      "loss on batch = 14.316248869483843\n",
      "loss on batch = 15.513019899114287\n",
      "loss on batch = 15.386908651983342\n",
      "loss on batch = 15.693786535863769\n",
      "loss on batch = 14.819707242850784\n",
      "loss on batch = 15.343788742153224\n",
      "loss on batch = 14.760972958626098\n",
      "loss on batch = 15.673986657772677\n",
      "loss on batch = 14.687625288948848\n",
      "loss on batch = 15.950650466089018\n",
      "loss on batch = 15.515495815273152\n",
      "loss on batch = 15.708736165142195\n",
      "loss on batch = 15.556219141410255\n",
      "loss on batch = 14.508095206168065\n",
      "loss on batch = 14.723049793294155\n",
      "loss on batch = 15.281838933066602\n",
      "loss on batch = 16.182856344553926\n",
      "loss on batch = 14.44846350071488\n",
      "loss on batch = 15.104921265671582\n",
      "loss on batch = 14.428471056141394\n",
      "loss on batch = 17.083499711435113\n",
      "loss on batch = 14.990196798851864\n",
      "loss on batch = 15.566279764267058\n",
      "loss on batch = 14.374595772619102\n",
      "loss on batch = 14.437303329055489\n",
      "loss on batch = 14.791649995187974\n",
      "loss on batch = 14.519455435530045\n",
      "loss on batch = 14.65595013985818\n",
      "loss on batch = 14.459429082838835\n",
      "loss on batch = 14.822206360107302\n",
      "loss on batch = 14.45667696355591\n",
      "loss on batch = 14.857683516498913\n",
      "loss on batch = 15.463294595572398\n",
      "loss on batch = 14.616365843423893\n",
      "loss on batch = 14.437845117251905\n",
      "loss on batch = 15.019326308695366\n",
      "loss on batch = 16.273919971454823\n",
      "loss on batch = 15.682947784973308\n",
      "loss on batch = 15.327914947010786\n",
      "loss on batch = 15.33258580839639\n",
      "loss on batch = 15.03308879334182\n",
      "loss on batch = 14.605026082239458\n",
      "Acc: 0.9388\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=True,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=10, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9293\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=5, batch_size=5, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9164\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=5, batch_size=10, lr=.02, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9369\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.2)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9432\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=5, batch_size=12, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9416\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "#nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9461\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=5, batch_size=10, lr=.008, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.927\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "#nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=5, batch_size=10, lr=.01, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9441\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "#nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=5, batch_size=12, lr=.008, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.94\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=5, batch_size=12, lr=.008, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9476\n"
     ]
    }
   ],
   "source": [
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=1, batch_size=12, lr=.008, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9471\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=4, batch_size=12, lr=.008, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9465\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=5, batch_size=10, lr=.007, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.948\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=4, batch_size=13, lr=.008, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9498\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=4, batch_size=15, lr=.008, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9465\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=4, batch_size=20, lr=.008, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9418\n"
     ]
    }
   ],
   "source": [
    " nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=4, batch_size=15, lr=.007, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9524\n"
     ]
    }
   ],
   "source": [
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=4, batch_size=15, lr=.007, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9554\n"
     ]
    }
   ],
   "source": [
    " nn = NeuralNetwork(784, 10, loss_function='crossentropy', seed=123, verbose=False,  activation_function ='sigma')\n",
    "\n",
    "# Multiple layers\n",
    "nn.add_layer(250)\n",
    "nn.add_layer(100)\n",
    "nn.add_layer(49)\n",
    "nn.add_layer(10)\n",
    "\n",
    "nn.create_output_layer('softmax')\n",
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=10, batch_size=15, lr=.007, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acc: 0.9521\n"
     ]
    }
   ],
   "source": [
    "nn.errors = nn.train(train_X_scaled, train_y_onehot, epochs=1, batch_size=15, lr=.007, method='rmsprop', method_param=0.1)\n",
    "pred = nn.predict(test_X_scaled)\n",
    "\n",
    "print(\"Acc: \" + str(np.mean((np.argmax(test_y_onehot, axis=1).flatten() == np.argmax(pred, axis=1).flatten()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}